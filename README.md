We designed and evaluated a Bi-Directional Hybrid (BDH) architecture as a sparse, interpretable alternative to dense Transformer models. BDH activates only 2.5â€“5% of neurons per input, maintains a constant-size memory state 
ğ‘‚
(
1
)
O(1), and enables inference-time Hebbian synapse strengthening without backpropagation.

We built a complete experimental pipeline including sparse activation atlases, cross-lingual monosemantic probing, Hebbian learning visualization, long-context memory stress tests, and harmful prompt entropy analysis.
