{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873686b2-248b-4394-b92e-974328b17843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bdh_europarl_train_probe.py\n",
    "# --- START PASTE ---\n",
    "#!/usr/bin/env python3\n",
    "# bdh_europarl_train_probe.py\n",
    "# One-file: download -> train -> probe (monosemantic neuron IDs)\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# =========================\n",
    "#        BDH MODEL\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class BDHConfig:\n",
    "    n_layer: int = 6\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    n_head: int = 4\n",
    "    mlp_internal_dim_multiplier: int = 128\n",
    "    vocab_size: int = 256  # byte-level\n",
    "\n",
    "\n",
    "def get_freqs(n, theta, dtype):\n",
    "    def quantize(t, q=2):\n",
    "        return (t / q).floor() * q\n",
    "\n",
    "    return (\n",
    "        1.0\n",
    "        / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n))\n",
    "        / (2 * math.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def phases_cos_sin(phases):\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        return torch.cos(phases), torch.sin(phases)\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases, v):\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())\n",
    "        phases_cos, phases_sin = Attention.phases_cos_sin(phases)\n",
    "        return (v * phases_cos).to(v.dtype) + (v_rot * phases_sin).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        assert self.freqs.dtype == torch.float32\n",
    "        assert K is Q\n",
    "        _, _, T, _ = Q.size()\n",
    "\n",
    "        r_phases = (\n",
    "            torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype)\n",
    "            .view(1, 1, -1, 1)\n",
    "        ) * self.freqs\n",
    "\n",
    "        QR = self.rope(r_phases, Q)\n",
    "        KR = QR\n",
    "\n",
    "        # causal: only past (strictly lower triangle)\n",
    "        scores = (QR @ KR.mT).tril(diagonal=-1)\n",
    "\n",
    "        # NOTE: no softmax -> associative accumulation\n",
    "        return scores @ V\n",
    "\n",
    "\n",
    "class BDH(nn.Module):\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "\n",
    "        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))\n",
    "        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "\n",
    "        self.attn = Attention(config)\n",
    "        self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.lm_head = nn.Parameter(torch.zeros((D, config.vocab_size)).normal_(std=0.02))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_sparse: bool = False):\n",
    "        C = self.config\n",
    "        B, T = idx.size()\n",
    "        D = C.n_embd\n",
    "        nh = C.n_head\n",
    "        N = D * C.mlp_internal_dim_multiplier // nh\n",
    "\n",
    "        x = self.embed(idx).unsqueeze(1)  # (B,1,T,D)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        sparse_cache = []  # list of x_sparse per layer\n",
    "\n",
    "        for level in range(C.n_layer):\n",
    "            x_latent = x @ self.encoder                  # (B,nh,T,N)\n",
    "            x_sparse = F.relu(x_latent)                  # (B,nh,T,N)\n",
    "\n",
    "            if return_sparse:\n",
    "                sparse_cache.append(x_sparse.detach())\n",
    "\n",
    "            yKV = self.attn(Q=x_sparse, K=x_sparse, V=x) # (B,nh,T,T)@(B,1,T,D)->(B,nh,T,D) or broadcast-ish\n",
    "            yKV = self.ln(yKV)\n",
    "\n",
    "            y_latent = yKV @ self.encoder_v              # (B,nh,T,N)\n",
    "            y_sparse = F.relu(y_latent)                  # (B,nh,T,N)\n",
    "\n",
    "            xy_sparse = x_sparse * y_sparse              # (B,nh,T,N)\n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "\n",
    "            yMLP = xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder  # (B,1,T,D)\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)\n",
    "\n",
    "        logits = x.view(B, T, D) @ self.lm_head  # (B,T,256)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        if return_sparse:\n",
    "            return logits, loss, sparse_cache\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# =========================\n",
    "#     EUROPARL PIPELINE\n",
    "# =========================\n",
    "\n",
    "EUROPARL_URLS = {\n",
    "    # Europarl v7 pairs (English with EU languages)\n",
    "    \"de-en.tgz\": \"https://www.statmt.org/europarl/v7/de-en.tgz\",\n",
    "    \"fr-en.tgz\": \"https://www.statmt.org/europarl/v7/fr-en.tgz\",\n",
    "    # Add more if you want:\n",
    "    # \"es-en.tgz\": \"https://www.statmt.org/europarl/v7/es-en.tgz\",\n",
    "}\n",
    "\n",
    "def download(url: str, out_path: str):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"[download] exists: {out_path}\")\n",
    "        return\n",
    "    print(f\"[download] {url}\")\n",
    "    urllib.request.urlretrieve(url, out_path)\n",
    "    print(f\"[download] saved: {out_path}\")\n",
    "\n",
    "def extract_tgz(tgz_path: str, out_dir: str):\n",
    "    marker = os.path.join(out_dir, \".extracted_\" + os.path.basename(tgz_path))\n",
    "    if os.path.exists(marker):\n",
    "        print(f\"[extract] already extracted: {tgz_path}\")\n",
    "        return\n",
    "    print(f\"[extract] {tgz_path}\")\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(out_dir)\n",
    "    with open(marker, \"w\") as f:\n",
    "        f.write(\"ok\\n\")\n",
    "\n",
    "def iter_text_lines(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"<\"):  # skip tags if present\n",
    "                continue\n",
    "            yield line\n",
    "\n",
    "def build_train_txt(data_dir: str, out_txt: str, max_lines_per_file: int = 200_000, seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    files = []\n",
    "    for root, _, fnames in os.walk(data_dir):\n",
    "        for fn in fnames:\n",
    "            # Europarl extracted files usually: europarl-v7.de-en.en / .de etc.\n",
    "            if \"europarl\" in fn and fn.endswith((\".en\", \".de\", \".fr\")):\n",
    "                files.append(os.path.join(root, fn))\n",
    "\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No europarl text files found under {data_dir}. Did extraction work?\")\n",
    "\n",
    "    random.shuffle(files)\n",
    "    os.makedirs(os.path.dirname(out_txt), exist_ok=True)\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as out:\n",
    "        for fp in files:\n",
    "            c = 0\n",
    "            for ln in iter_text_lines(fp):\n",
    "                out.write(ln + \"\\n\")\n",
    "                c += 1\n",
    "                if c >= max_lines_per_file:\n",
    "                    break\n",
    "\n",
    "    print(f\"[dataset] wrote: {out_txt}\")\n",
    "    print(f\"[dataset] included files: {len(files)}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "#      BYTE DATASET\n",
    "# =========================\n",
    "\n",
    "class ByteDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_txt: str, block_size: int, max_bytes: int):\n",
    "        with open(train_txt, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "        b = text.encode(\"utf-8\", errors=\"ignore\")[:max_bytes]\n",
    "        self.data = torch.tensor(list(b), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.block_size - 1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.data[i : i + self.block_size]\n",
    "        y = self.data[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# =========================\n",
    "#      NEURON PROBING\n",
    "# =========================\n",
    "\n",
    "def neuron_id(layer: int, head: int, feat: int, nh: int, N: int) -> int:\n",
    "    # unique integer across all layers/heads/features\n",
    "    return layer * (nh * N) + head * N + feat\n",
    "\n",
    "def ids_for_text(text: str) -> torch.Tensor:\n",
    "    # byte-level ids in [0..255]\n",
    "    b = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "    if len(b) == 0:\n",
    "        b = b\" \"  # avoid empty\n",
    "    return torch.tensor([list(b)], dtype=torch.long)  # (1,T)\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_neurons_for_input(\n",
    "    model: BDH,\n",
    "    text: str,\n",
    "    topk: int = 200,\n",
    "    aggregate: str = \"mean\",   # \"mean\" over positions is better than just last byte\n",
    ") -> List[List[Tuple[int, float, int, int, int]]]:\n",
    "    \"\"\"\n",
    "    Returns per-layer list of hits:\n",
    "      hits[layer] = [(global_neuron_id, activation, layer, head, feat), ...] sorted by activation desc\n",
    "    \"\"\"\n",
    "    cfg = model.config\n",
    "    nh = cfg.n_head\n",
    "    D = cfg.n_embd\n",
    "    N = D * cfg.mlp_internal_dim_multiplier // nh\n",
    "\n",
    "    x = ids_for_text(text).to(next(model.parameters()).device)\n",
    "    _, _, sparse_cache = model(x, return_sparse=True)\n",
    "\n",
    "    hits_per_layer = []\n",
    "    for layer, x_sparse in enumerate(sparse_cache):\n",
    "        # x_sparse: (1, nh, T, N)\n",
    "        a = x_sparse[0]  # (nh, T, N)\n",
    "\n",
    "        if aggregate == \"last\":\n",
    "            vec = a[:, -1, :]              # (nh, N)\n",
    "        else:\n",
    "            vec = a.mean(dim=1)            # (nh, N) mean over T\n",
    "\n",
    "        flat = vec.reshape(-1)             # nh*N\n",
    "        k = min(topk, flat.numel())\n",
    "        vals, idxs = torch.topk(flat, k=k)\n",
    "\n",
    "        layer_hits = []\n",
    "        for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "            head = ix // N\n",
    "            feat = ix % N\n",
    "            gid = neuron_id(layer, head, feat, nh, N)\n",
    "            layer_hits.append((gid, float(v), layer, head, feat))\n",
    "\n",
    "        hits_per_layer.append(layer_hits)\n",
    "\n",
    "    return hits_per_layer\n",
    "\n",
    "def shared_neurons_across_texts(\n",
    "    all_hits: List[List[List[Tuple[int, float, int, int, int]]]],\n",
    "    topk_intersection: int,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    all_hits[word_i][layer] = list of top hits\n",
    "    Returns shared neuron IDs per layer (intersection across all words).\n",
    "    \"\"\"\n",
    "    n_layers = len(all_hits[0])\n",
    "    shared = []\n",
    "    for layer in range(n_layers):\n",
    "        sets = []\n",
    "        for wi in range(len(all_hits)):\n",
    "            ids = set(h[0] for h in all_hits[wi][layer][:topk_intersection])\n",
    "            sets.append(ids)\n",
    "        common = set.intersection(*sets) if sets else set()\n",
    "        shared.append(sorted(common))\n",
    "    return shared\n",
    "\n",
    "def decode_neuron(global_id: int, cfg: BDHConfig) -> Tuple[int, int, int]:\n",
    "    nh = cfg.n_head\n",
    "    D = cfg.n_embd\n",
    "    N = D * cfg.mlp_internal_dim_multiplier // nh\n",
    "    per_layer = nh * N\n",
    "    layer = global_id // per_layer\n",
    "    rem = global_id % per_layer\n",
    "    head = rem // N\n",
    "    feat = rem % N\n",
    "    return layer, head, feat\n",
    "\n",
    "\n",
    "# =========================\n",
    "#      TRAIN / PROBE\n",
    "# =========================\n",
    "\n",
    "def train(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
    "    print(f\"[train] device={device}\")\n",
    "\n",
    "    # 1) Download + extract Europarl (optional)\n",
    "    if args.download:\n",
    "        os.makedirs(args.data_dir, exist_ok=True)\n",
    "        for key in args.pairs:\n",
    "            if key not in EUROPARL_URLS:\n",
    "                raise ValueError(f\"Unknown pair key '{key}'. Available: {list(EUROPARL_URLS.keys())}\")\n",
    "            tgz_path = os.path.join(args.data_dir, key)\n",
    "            download(EUROPARL_URLS[key], tgz_path)\n",
    "            extract_tgz(tgz_path, args.data_dir)\n",
    "\n",
    "    # 2) Build train.txt\n",
    "    if not os.path.exists(args.train_txt) or args.rebuild_txt:\n",
    "        build_train_txt(\n",
    "            data_dir=args.data_dir,\n",
    "            out_txt=args.train_txt,\n",
    "            max_lines_per_file=args.max_lines_per_file,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "    # 3) Dataset\n",
    "    ds = ByteDataset(args.train_txt, block_size=args.block_size, max_bytes=args.max_bytes)\n",
    "    if len(ds) <= 0:\n",
    "        raise RuntimeError(\"Dataset too small. Increase max_bytes / ensure train.txt is non-empty.\")\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 4) Model\n",
    "    cfg = BDHConfig(\n",
    "        vocab_size=256,\n",
    "        n_layer=args.n_layer,\n",
    "        n_embd=args.n_embd,\n",
    "        n_head=args.n_head,\n",
    "        mlp_internal_dim_multiplier=args.mult,\n",
    "        dropout=args.dropout,\n",
    "    )\n",
    "    model = BDH(cfg).to(device)\n",
    "\n",
    "    if args.resume and os.path.exists(args.ckpt_path):\n",
    "        model.load_state_dict(torch.load(args.ckpt_path, map_location=device))\n",
    "        print(f\"[train] resumed from {args.ckpt_path}\")\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < args.steps:\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            if step % args.log_every == 0:\n",
    "                print(f\"[train] step={step} loss={float(loss):.4f}\")\n",
    "\n",
    "            step += 1\n",
    "            if step >= args.steps:\n",
    "                break\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.ckpt_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), args.ckpt_path)\n",
    "    print(f\"[train] saved checkpoint: {args.ckpt_path}\")\n",
    "\n",
    "def probe(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
    "    print(f\"[probe] device={device}\")\n",
    "\n",
    "    cfg = BDHConfig(\n",
    "        vocab_size=256,\n",
    "        n_layer=args.n_layer,\n",
    "        n_embd=args.n_embd,\n",
    "        n_head=args.n_head,\n",
    "        mlp_internal_dim_multiplier=args.mult,\n",
    "        dropout=0.0,\n",
    "    )\n",
    "    model = BDH(cfg).to(device)\n",
    "    if not os.path.exists(args.ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {args.ckpt_path}. Train first.\")\n",
    "    model.load_state_dict(torch.load(args.ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    texts = args.texts\n",
    "    if len(texts) < 2:\n",
    "        raise ValueError(\"Give at least 2 texts/words to find shared neurons.\")\n",
    "\n",
    "    all_hits = []\n",
    "    for t in texts:\n",
    "        hits = top_neurons_for_input(model, t, topk=args.topk, aggregate=args.aggregate)\n",
    "        all_hits.append(hits)\n",
    "\n",
    "    shared = shared_neurons_across_texts(all_hits, topk_intersection=args.topk_intersection)\n",
    "\n",
    "    print(\"\\nInputs:\")\n",
    "    for i, t in enumerate(texts):\n",
    "        print(f\"  [{i}] {t}\")\n",
    "\n",
    "    print(\"\\nShared neuron IDs per layer (intersection of top-k):\")\n",
    "    for layer, ids in enumerate(shared):\n",
    "        show = ids[:args.show]\n",
    "        print(f\"\\nLayer {layer}: {len(ids)} shared IDs\")\n",
    "        if not show:\n",
    "            print(\"  (none) -> try: train longer, increase --topk / --topk_intersection, or probe with short sentences)\")\n",
    "            continue\n",
    "        for gid in show:\n",
    "            L, H, Fidx = decode_neuron(gid, cfg)\n",
    "            print(f\"  neuron_id={gid}  (layer={L}, head={H}, feat={Fidx})\")\n",
    "\n",
    "    if args.print_top:\n",
    "        # Also print top neurons per input for layer 0..n\n",
    "        print(\"\\nTop neurons per input (first few layers):\")\n",
    "        for i, t in enumerate(texts):\n",
    "            print(f\"\\n=== Input [{i}] {t} ===\")\n",
    "            hits = all_hits[i]\n",
    "            for layer in range(min(args.layers_print, len(hits))):\n",
    "                top = hits[layer][:args.show]\n",
    "                print(f\"  Layer {layer}:\")\n",
    "                for gid, val, L, H, Fidx in top:\n",
    "                    print(f\"    neuron_id={gid} act={val:.4f} (head={H}, feat={Fidx})\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    sub = p.add_subparsers(dest=\"cmd\", required=True)\n",
    "\n",
    "    # TRAIN\n",
    "    pt = sub.add_parser(\"train\")\n",
    "    pt.add_argument(\"--data_dir\", type=str, default=\"data_europarl\")\n",
    "    pt.add_argument(\"--train_txt\", type=str, default=\"data_europarl/train.txt\")\n",
    "    pt.add_argument(\"--download\", action=\"store_true\", help=\"download+extract Europarl\")\n",
    "    pt.add_argument(\"--pairs\", nargs=\"+\", default=[\"de-en.tgz\", \"fr-en.tgz\"], help=\"which Europarl tgz keys\")\n",
    "    pt.add_argument(\"--rebuild_txt\", action=\"store_true\")\n",
    "    pt.add_argument(\"--max_lines_per_file\", type=int, default=200_000)\n",
    "    pt.add_argument(\"--max_bytes\", type=int, default=50_000_000)\n",
    "    pt.add_argument(\"--block_size\", type=int, default=256)\n",
    "    pt.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    pt.add_argument(\"--steps\", type=int, default=5000)\n",
    "    pt.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    pt.add_argument(\"--log_every\", type=int, default=100)\n",
    "    pt.add_argument(\"--seed\", type=int, default=0)\n",
    "    pt.add_argument(\"--ckpt_path\", type=str, default=\"checkpoints/bdh_europarl_bytes.pt\")\n",
    "    pt.add_argument(\"--resume\", action=\"store_true\")\n",
    "    pt.add_argument(\"--cpu\", action=\"store_true\")\n",
    "\n",
    "    # model hyperparams\n",
    "    pt.add_argument(\"--n_layer\", type=int, default=6)\n",
    "    pt.add_argument(\"--n_embd\", type=int, default=256)\n",
    "    pt.add_argument(\"--n_head\", type=int, default=4)\n",
    "    pt.add_argument(\"--mult\", type=int, default=128)\n",
    "    pt.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "\n",
    "    # PROBE\n",
    "    ppb = sub.add_parser(\"probe\")\n",
    "    ppb.add_argument(\"--texts\", nargs=\"+\", required=True, help=\"words/sentences you provide at test time\")\n",
    "    ppb.add_argument(\"--ckpt_path\", type=str, default=\"checkpoints/bdh_europarl_bytes.pt\")\n",
    "    ppb.add_argument(\"--topk\", type=int, default=300, help=\"top neurons to compute per layer per input\")\n",
    "    ppb.add_argument(\"--topk_intersection\", type=int, default=200, help=\"intersection over this many top neurons\")\n",
    "    ppb.add_argument(\"--show\", type=int, default=30, help=\"how many shared IDs to print per layer\")\n",
    "    ppb.add_argument(\"--aggregate\", choices=[\"mean\", \"last\"], default=\"mean\", help=\"aggregate over bytes\")\n",
    "    ppb.add_argument(\"--print_top\", action=\"store_true\", help=\"also print top neurons per input\")\n",
    "    ppb.add_argument(\"--layers_print\", type=int, default=2)\n",
    "    ppb.add_argument(\"--cpu\", action=\"store_true\")\n",
    "\n",
    "    # model hyperparams must match training\n",
    "    ppb.add_argument(\"--n_layer\", type=int, default=6)\n",
    "    ppb.add_argument(\"--n_embd\", type=int, default=256)\n",
    "    ppb.add_argument(\"--n_head\", type=int, default=4)\n",
    "    ppb.add_argument(\"--mult\", type=int, default=128)\n",
    "\n",
    "    args = p.parse_args()\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(1337)\n",
    "    random.seed(1337)\n",
    "\n",
    "    if args.cmd == \"train\":\n",
    "        train(args)\n",
    "    elif args.cmd == \"probe\":\n",
    "        probe(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# --- END PASTE ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c000364-aee0-40f3-8c72-27a82b9e846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l bdh_europarl_train_probe.py\n",
    "!ls -lh bdh_europarl_train_probe.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242384e-ce12-49c3-83d6-804fc64439a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12602a61-d392-4753-8ce2-a8b49ad5aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u bdh_europarl_train_probe.py train --download \\\n",
    "  --steps 200 --log_every 10 \\\n",
    "  --batch_size 2 --block_size 128 \\\n",
    "  --n_layer 4 --n_embd 128 --n_head 4 --mult 16 \\\n",
    "  --max_lines_per_file 20000 --max_bytes 5000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8aa5e-a9d1-4086-b8ad-f241e3b1e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u bdh_europarl_train_probe.py train \\\n",
    "  --steps 2000 --log_every 50 \\\n",
    "  --batch_size 4 --block_size 128 \\\n",
    "  --n_layer 6 --n_embd 128 --n_head 4 --mult 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df860ba9-f669-42b5-8701-9e159d7df00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u bdh_europarl_train_probe.py train --resume \\\n",
    "  --steps 12000 --log_every 200 \\\n",
    "  --batch_size 4 --block_size 128 \\\n",
    "  --n_layer 6 --n_embd 128 --n_head 4 --mult 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f7017-ff48-469f-b49a-1db813787d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # MONOSEMANTIC SENTENCE-LEVEL PROBE (LATE LAYERS, FIXED)\n",
    "# # ============================================\n",
    "\n",
    "# import torch\n",
    "# import importlib.util\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # ---------- Load BDH code ----------\n",
    "# spec = importlib.util.spec_from_file_location(\n",
    "#     \"bdhmod\", \"/content/bdh_europarl_train_probe.py\"\n",
    "# )\n",
    "# bdhmod = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(bdhmod)\n",
    "\n",
    "# BDHConfig = bdhmod.BDHConfig\n",
    "# BDH = bdhmod.BDH\n",
    "# ids_for_text = bdhmod.ids_for_text\n",
    "# neuron_id = bdhmod.neuron_id\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # ---------- Load checkpoint ----------\n",
    "# CKPT = \"checkpoints/bdh_europarl_bytes.pt\"\n",
    "# state = torch.load(CKPT, map_location=device)\n",
    "\n",
    "# nh, D, N_tensor = state[\"encoder\"].shape\n",
    "# mult = int((N_tensor * nh) // D)\n",
    "\n",
    "# cfg = BDHConfig(\n",
    "#     vocab_size=256,\n",
    "#     n_layer=6,\n",
    "#     n_embd=D,\n",
    "#     n_head=nh,\n",
    "#     mlp_internal_dim_multiplier=mult,\n",
    "#     dropout=0.0,\n",
    "# )\n",
    "\n",
    "# model = BDH(cfg).to(device)\n",
    "# model.load_state_dict(state, strict=True)\n",
    "# model.eval()\n",
    "\n",
    "# # Derived N per head for this config (must match model)\n",
    "# N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head\n",
    "\n",
    "# print(\"BDH loaded\")\n",
    "# print(\"Model config:\", {\"n_layer\": cfg.n_layer, \"n_embd\": cfg.n_embd, \"n_head\": cfg.n_head, \"mult\": cfg.mlp_internal_dim_multiplier, \"N_per_head\": N})\n",
    "\n",
    "# # ---------- Translation ----------\n",
    "# ALL_LANGS = {\n",
    "#     \"German\": \"Helsinki-NLP/opus-mt-en-de\",\n",
    "#     \"French\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "#     \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "#     \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "# }\n",
    "\n",
    "# _trans = {}\n",
    "\n",
    "# def translate(lang, text):\n",
    "#     if lang not in _trans:\n",
    "#         tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "#         mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "#         _trans[lang] = (tok, mdl)\n",
    "#     tok, mdl = _trans[lang]\n",
    "#     batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "#     out = mdl.generate(**batch, max_new_tokens=64)\n",
    "#     return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# # ---------- Neuron activation (FIXED: layer decoding) ----------\n",
    "# TOPK = 5\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_neurons_for_word(word, layer_idx, k=5):\n",
    "#     \"\"\"\n",
    "#     Returns TOP-K neurons for a given word at a specific layer.\n",
    "#     \"\"\"\n",
    "#     x = ids_for_text(word).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)   # (nh, N)\n",
    "#     flat = a.reshape(-1)                   # (nh*N)\n",
    "\n",
    "#     vals, idxs = torch.topk(flat, k)\n",
    "\n",
    "#     hits = []\n",
    "#     for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         gid = neuron_id(layer_idx, head, feat, cfg.n_head, N)\n",
    "#         hits.append((layer_idx, head, feat, gid, v))\n",
    "\n",
    "#     return hits\n",
    "\n",
    "\n",
    "# # ============================================\n",
    "# # PHASE 2: MONOSEMANTIC EXPERIMENT (BEST VERSION)\n",
    "# # - Demo: Top-K neuron IDs for EN vs translations\n",
    "# # - Dataset mode: enter many concept-words until END\n",
    "# # - Metrics: Jaccard overlap + Selectivity + Entropy\n",
    "# # - Plots: Jaccard distribution + Selectivity bars\n",
    "# # ============================================\n",
    "\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# # ---------- Settings ----------\n",
    "# LAYERS = [4, 5]                      # late layers\n",
    "# TOPK_PRINT = 5                       # show top-5 neuron IDs (demo)\n",
    "# TOPK_SET = 50                        # for overlap / intersection (more stable)\n",
    "# MAX_CAND = 400                       # cap number of candidate neurons to score (speed)\n",
    "# EPS = 1e-9\n",
    "\n",
    "# LAYERS = [L for L in LAYERS if 0 <= L < cfg.n_layer]\n",
    "\n",
    "# # ---------- Helpers ----------\n",
    "# def tokenize_words(s: str):\n",
    "#     # English-ish + accented letters (for translations)\n",
    "#     return re.findall(r\"[A-Za-zÀ-ÿ]+\", s.lower())\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_set_for_text(text: str, layer_idx: int, k: int):\n",
    "#     \"\"\"Top-K neuron IDs for FULL text at a layer.\"\"\"\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)       # (nh, N)\n",
    "#     flat = a.reshape(-1)                       # (nh*N,)\n",
    "#     k = min(k, flat.numel())\n",
    "#     _, idxs = torch.topk(flat, k)\n",
    "#     S = set()\n",
    "#     for ix in idxs.tolist():\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         S.add(neuron_id(layer_idx, head, feat, cfg.n_head, N))\n",
    "#     return S\n",
    "\n",
    "# def jaccard(a: set, b: set) -> float:\n",
    "#     return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "# def decode_gid(gid: int):\n",
    "#     per_layer = cfg.n_head * N\n",
    "#     layer = gid // per_layer\n",
    "#     rem = gid % per_layer\n",
    "#     head = rem // N\n",
    "#     feat = rem % N\n",
    "#     return layer, head, feat\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid(text: str, layer_idx: int, gid: int) -> float:\n",
    "#     \"\"\"Mean activation of ONE neuron on FULL text.\"\"\"\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return 0.0\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)       # (nh,N)\n",
    "#     return float(a[head, feat].item())\n",
    "\n",
    "# def normalized_entropy(vals):\n",
    "#     v = np.array(vals, dtype=np.float64)\n",
    "#     v = np.clip(v, 0, None)\n",
    "#     p = v / (v.sum() + EPS)\n",
    "#     H = -np.sum(p * np.log(p + 1e-12))\n",
    "#     return float(H / (np.log(len(p) + 1e-12)))\n",
    "\n",
    "# def read_list_until_end(prompt, min_n=1):\n",
    "#     print(prompt)\n",
    "#     out = []\n",
    "#     while True:\n",
    "#         s = input(\"> \").strip()\n",
    "#         if s.upper() == \"END\":\n",
    "#             break\n",
    "#         if s:\n",
    "#             out.append(s)\n",
    "#     if len(out) < min_n:\n",
    "#         raise ValueError(f\"Need at least {min_n} items.\")\n",
    "#     return out\n",
    "\n",
    "# # ============================================\n",
    "# # (1) DEMO: one input -> translation -> Top-K table\n",
    "# # ============================================\n",
    "\n",
    "# print(\"\\n=== DEMO MODE (one example) ===\")\n",
    "# sentence = input(\"Enter an English word/sentence for demo:\\n> \").strip()\n",
    "\n",
    "# print(\"\\nAvailable languages:\")\n",
    "# for k in ALL_LANGS:\n",
    "#     print(\" -\", k)\n",
    "\n",
    "# langs_raw = input(\"\\nChoose languages (comma-separated):\\n> \").strip()\n",
    "# LANGS = [l.strip() for l in langs_raw.split(\",\") if l.strip() in ALL_LANGS]\n",
    "# if len(LANGS) == 0:\n",
    "#     LANGS = [\"German\", \"French\", \"Spanish\", \"Italian\"]\n",
    "\n",
    "# translations = {lang: translate(lang, sentence) for lang in LANGS}\n",
    "\n",
    "# print(\"\\nTranslations:\")\n",
    "# print(\"EN:\", sentence)\n",
    "# for lang, txt in translations.items():\n",
    "#     print(f\"{lang[:2].upper()}: {txt}\")\n",
    "\n",
    "# # Print Top-K neurons for full text (not per word) — cleaner for proof\n",
    "# for LAYER in LAYERS:\n",
    "#     print(f\"\\nTOP-{TOPK_PRINT} NEURONS (FULL TEXT) — Layer {LAYER}\")\n",
    "#     print(f\"{'TEXT':<10} {'NEURON_ID':<10} {'(layer,head,feat)':<18} {'ACT'}\")\n",
    "#     print(\"-\" * 60)\n",
    "\n",
    "#     def topk_list(text, k=TOPK_PRINT):\n",
    "#         x = ids_for_text(text).to(device)\n",
    "#         _, _, sparse = model(x, return_sparse=True)\n",
    "#         a = sparse[LAYER][0].mean(dim=1)\n",
    "#         flat = a.reshape(-1)\n",
    "#         k = min(k, flat.numel())\n",
    "#         vals, idxs = torch.topk(flat, k)\n",
    "#         out = []\n",
    "#         for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "#             head = ix // N\n",
    "#             feat = ix % N\n",
    "#             gid = neuron_id(LAYER, head, feat, cfg.n_head, N)\n",
    "#             out.append((gid, (LAYER, head, feat), float(v)))\n",
    "#         return out\n",
    "\n",
    "#     for gid, trip, act in topk_list(sentence):\n",
    "#         print(f\"{'EN':<10} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "#     for lang, txt in translations.items():\n",
    "#         for gid, trip, act in topk_list(txt):\n",
    "#             print(f\"{lang[:2].upper():<10} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "\n",
    "# # ============================================\n",
    "# # (2) DATASET MODE: enter many concept words (no hardcoded list)\n",
    "# # ============================================\n",
    "\n",
    "# print(\"\\n\\n=== DATASET MODE (monosemantic test) ===\")\n",
    "# concepts = read_list_until_end(\n",
    "#     \"Enter concept WORDS one by one (e.g., dog, bicycle, oxygen). Type END to finish:\",\n",
    "#     min_n=5\n",
    "# )\n",
    "\n",
    "# # translate each concept into chosen languages\n",
    "# concept_trans = []\n",
    "# for w in concepts:\n",
    "#     trans = { \"EN\": w }\n",
    "#     for lang in LANGS:\n",
    "#         trans[lang] = translate(lang, w)\n",
    "#     concept_trans.append(trans)\n",
    "\n",
    "# # ============================================\n",
    "# # (3) OVERLAP METRIC: Jaccard EN vs translations for each concept\n",
    "# # ============================================\n",
    "\n",
    "# print(\"\\n=== Jaccard overlap per concept (EN vs each translation) ===\")\n",
    "# all_jaccards = {L: [] for L in LAYERS}\n",
    "\n",
    "# for LAYER in LAYERS:\n",
    "#     for trans in concept_trans:\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, LAYER, TOPK_SET)\n",
    "\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             tr = trans[lang]\n",
    "#             S_tr = topk_set_for_text(tr, LAYER, TOPK_SET)\n",
    "#             js.append(jaccard(S_en, S_tr))\n",
    "\n",
    "#         all_jaccards[LAYER].append(np.mean(js))\n",
    "\n",
    "#     print(f\"Layer {LAYER}: mean={np.mean(all_jaccards[LAYER]):.3f}  std={np.std(all_jaccards[LAYER]):.3f}\")\n",
    "\n",
    "# # ============================================\n",
    "# # PROPER VISUALS (readable proof plots)\n",
    "# # - No histograms\n",
    "# # - Show concept-by-concept results (readable)\n",
    "# # - For each concept: show activation bars with REAL text labels\n",
    "# # - Also show Jaccard actual vs baseline per concept\n",
    "# # ============================================\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def baseline_jaccard_for_concept(i, layer_idx):\n",
    "#     \"\"\"Baseline: compare EN concept i with translations of a different random concept.\"\"\"\n",
    "#     idxs = [j for j in range(len(concept_trans)) if j != i]\n",
    "#     j_idx = int(np.random.choice(idxs))\n",
    "#     en = concept_trans[i][\"EN\"]\n",
    "#     S_en = topk_set_for_text(en, layer_idx, TOPK_SET)\n",
    "\n",
    "#     base_js = []\n",
    "#     other = concept_trans[j_idx]\n",
    "#     for lang in LANGS:\n",
    "#         S_other = topk_set_for_text(other[lang], layer_idx, TOPK_SET)\n",
    "#         base_js.append(jaccard(S_en, S_other))\n",
    "#     return float(np.mean(base_js))\n",
    "\n",
    "# def best_shared_neuron_for_concept(i, layer_idx):\n",
    "#     \"\"\"Pick best shared neuron (intersection across EN+translations) with max (pos-neg).\"\"\"\n",
    "#     trans = concept_trans[i]\n",
    "#     POS = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "\n",
    "#     # Intersection of TopK neuron sets across POS texts\n",
    "#     inter = None\n",
    "#     for t in POS:\n",
    "#         S = topk_set_for_text(t, layer_idx, TOPK_SET)\n",
    "#         inter = S if inter is None else (inter & S)\n",
    "\n",
    "#     if not inter:\n",
    "#         return None\n",
    "\n",
    "#     inter = list(inter)[:MAX_CAND]\n",
    "\n",
    "#     best = None\n",
    "#     for gid in inter:\n",
    "#         pos_mean = float(np.mean([activation_gid(t, layer_idx, gid) for t in POS]))\n",
    "#         neg_mean = float(np.mean([activation_gid(t, layer_idx, gid) for t in NEG]))\n",
    "#         sel = pos_mean - neg_mean\n",
    "#         if (best is None) or (sel > best[1]):\n",
    "#             best = (gid, sel, pos_mean, neg_mean)\n",
    "\n",
    "#     return best  # (gid, sel, pos_mean, neg_mean)\n",
    "\n",
    "# # --------- 1) Show Jaccard table (actual vs baseline) ----------\n",
    "# np.random.seed(0)\n",
    "\n",
    "# for LAYER in LAYERS:\n",
    "#     print(f\"\\n==============================\")\n",
    "#     print(f\"Layer {LAYER}: Jaccard(EN, translations) per concept + baseline\")\n",
    "#     print(f\"==============================\")\n",
    "#     print(f\"{'CONCEPT':<14} {'ACTUAL':>8} {'BASE':>8} {'MARGIN':>8}\")\n",
    "\n",
    "#     actual_list, base_list = [], []\n",
    "\n",
    "#     for i, trans in enumerate(concept_trans):\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, LAYER, TOPK_SET)\n",
    "\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             S_tr = topk_set_for_text(trans[lang], LAYER, TOPK_SET)\n",
    "#             js.append(jaccard(S_en, S_tr))\n",
    "#         actual = float(np.mean(js))\n",
    "#         base = baseline_jaccard_for_concept(i, LAYER)\n",
    "#         margin = actual - base\n",
    "\n",
    "#         actual_list.append(actual)\n",
    "#         base_list.append(base)\n",
    "\n",
    "#         print(f\"{en:<14} {actual:>8.3f} {base:>8.3f} {margin:>8.3f}\")\n",
    "\n",
    "#     # Simple readable plot: actual vs baseline lines\n",
    "#     x = np.arange(len(concept_trans))\n",
    "#     names = [ct[\"EN\"] for ct in concept_trans]\n",
    "\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     x = np.arange(len(concept_names))\n",
    "#     plt.bar(x - 0.2, actual, width=0.4, label=\"Actual (EN vs its translations)\")\n",
    "#     plt.bar(x + 0.2, baseline, width=0.4, label=\"Baseline (EN vs other concept translations)\")\n",
    "#     plt.xticks(x, concept_names, rotation=45, ha=\"right\")\n",
    "#     plt.ylim(0,1)\n",
    "#     plt.title(f\"Layer {LAYER}: Actual vs Baseline Jaccard overlap (K={TOPK_SET})\")\n",
    "#     plt.ylabel(\"Jaccard\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # --------- 2) For each layer: show 2–3 best concepts proof plots ----------\n",
    "# TOP_SHOW = min(3, len(concept_trans))\n",
    "\n",
    "# for LAYER in LAYERS:\n",
    "#     # rank concepts by (actual-baseline)\n",
    "#     scored = []\n",
    "#     for i, trans in enumerate(concept_trans):\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, LAYER, TOPK_SET)\n",
    "\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             js.append(jaccard(S_en, topk_set_for_text(trans[lang], LAYER, TOPK_SET)))\n",
    "#         actual = float(np.mean(js))\n",
    "#         base = baseline_jaccard_for_concept(i, LAYER)\n",
    "#         scored.append((i, actual - base, actual, base))\n",
    "\n",
    "#     scored.sort(key=lambda x: x[1], reverse=True)\n",
    "#     picks = scored[:TOP_SHOW]\n",
    "\n",
    "#     print(f\"\\n==============================\")\n",
    "#     print(f\"Layer {LAYER}: Proof plots for top {TOP_SHOW} concepts\")\n",
    "#     print(f\"==============================\")\n",
    "\n",
    "#     for (i, margin, actual, base) in picks:\n",
    "#         trans = concept_trans[i]\n",
    "#         concept = trans[\"EN\"]\n",
    "#         best = best_shared_neuron_for_concept(i, LAYER)\n",
    "\n",
    "#         if best is None:\n",
    "#             print(f\"\\nConcept '{concept}': no shared neuron intersection.\")\n",
    "#             continue\n",
    "\n",
    "#         best_gid, sel, pos_m, neg_m = best\n",
    "#         POS_texts = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "#         POS_labels = [\"EN\"] + [lang[:2].upper() for lang in LANGS]\n",
    "\n",
    "#         # Prepare labels with short snippets so plot is readable\n",
    "#         def short(s, n=22):\n",
    "#             s = s.replace(\"\\n\", \" \")\n",
    "#             return s if len(s) <= n else s[:n-3] + \"...\"\n",
    "\n",
    "#         NEG_labels = [f\"NEG{i+1}\" for i in range(len(NEG))]\n",
    "#         all_labels = POS_labels + NEG_labels\n",
    "#         all_texts = POS_texts + NEG\n",
    "\n",
    "#         acts = [activation_gid(t, LAYER, best_gid) for t in all_texts]\n",
    "\n",
    "#         # Plot\n",
    "#         plt.figure(figsize=(11, 4))\n",
    "#         plt.bar(np.arange(len(all_labels)), acts)\n",
    "#         plt.xticks(np.arange(len(all_labels)),\n",
    "#                    [f\"{lab}\\n{short(tx)}\" for lab, tx in zip(all_labels, all_texts)],\n",
    "#                    rotation=0)\n",
    "#         plt.title(\n",
    "#             f\"Layer {LAYER} | concept='{concept}' | best neuron={best_gid} (layer,head,feat={decode_gid(best_gid)})\\n\"\n",
    "#             f\"Jaccard actual={actual:.3f} baseline={base:.3f} margin={margin:.3f} | selectivity(pos-neg)={sel:.3f}\"\n",
    "#         )\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         # Also print a clean table (so evaluator can read exact numbers)\n",
    "#         print(f\"\\nConcept: {concept}\")\n",
    "#         print(f\"  Jaccard actual={actual:.3f}, baseline={base:.3f}, margin={margin:.3f}\")\n",
    "#         print(f\"  Best neuron: {best_gid}  decode={decode_gid(best_gid)}\")\n",
    "#         print(f\"  Selectivity(pos-neg)={sel:.4f}  pos_mean={pos_m:.4f}  neg_mean={neg_m:.4f}\")\n",
    "#         print(\"  Activations:\")\n",
    "#         for lab, tx, av in zip(all_labels, all_texts, acts):\n",
    "#             print(f\"    {lab:<5} act={av:.4f} | {tx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3013a2-e47b-4af2-bb77-a752ffe36480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # PHASE 2 — BDH MONOSEMANTIC / POLYSEMANTIC PROBE (CLEAN, FINAL)\n",
    "# # What you get (clear + report-friendly):\n",
    "# #  (A) DEMO (single concept): translations + Top-5 neuron IDs table (Layer 4/5)\n",
    "# #  (B) DATASET MODE: user inputs concepts (>=5)\n",
    "# #      -> Jaccard Actual vs Baseline bar chart (per concept) for each layer\n",
    "# #      -> Clear table: Concept | Actual | Baseline | Margin\n",
    "# #  (C) PROOF PLOTS (top 3 concepts by margin):\n",
    "# #      -> For each chosen concept + each layer:\n",
    "# #         Find best shared neuron among POS texts and show:\n",
    "# #         EN/FR/.../NEG bars with proper labels (TEXT shown under tick)\n",
    "# #\n",
    "# # NEG texts are ALWAYS asked from user (no hardcoded negatives).\n",
    "# # Baseline is RANDOMLY selected (seeded) from \"other concepts\" so it is repeatable.\n",
    "# #\n",
    "# # How to \"clear memory\" / start again:\n",
    "# #  - Just re-run the cell (variables reset).\n",
    "# #  - In Colab: Runtime -> Restart runtime (clears model cache + variables).\n",
    "# #  - Translation models are cached in _trans dict; call clear_translation_cache()\n",
    "# # ============================================================\n",
    "\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import importlib.util\n",
    "# import matplotlib.pyplot as plt\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # -----------------------------\n",
    "# # 0) Load BDH code\n",
    "# # -----------------------------\n",
    "# spec = importlib.util.spec_from_file_location(\"bdhmod\", \"/content/bdh_europarl_train_probe.py\")\n",
    "# bdhmod = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(bdhmod)\n",
    "\n",
    "# BDHConfig = bdhmod.BDHConfig\n",
    "# BDH = bdhmod.BDH\n",
    "# ids_for_text = bdhmod.ids_for_text\n",
    "# neuron_id = bdhmod.neuron_id\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # -----------------------------\n",
    "# # 1) Load checkpoint\n",
    "# # -----------------------------\n",
    "# CKPT = \"checkpoints/bdh_europarl_bytes.pt\"\n",
    "# state = torch.load(CKPT, map_location=device)\n",
    "\n",
    "# nh, D, N_tensor = state[\"encoder\"].shape\n",
    "# mult = int((N_tensor * nh) // D)\n",
    "\n",
    "# cfg = BDHConfig(\n",
    "#     vocab_size=256,\n",
    "#     n_layer=6,\n",
    "#     n_embd=D,\n",
    "#     n_head=nh,\n",
    "#     mlp_internal_dim_multiplier=mult,\n",
    "#     dropout=0.0,\n",
    "# )\n",
    "\n",
    "# model = BDH(cfg).to(device)\n",
    "# model.load_state_dict(state, strict=True)\n",
    "# model.eval()\n",
    "\n",
    "# # Derived N per head (must match model)\n",
    "# N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head\n",
    "\n",
    "# print(\"BDH loaded\")\n",
    "# print(\"Model config:\", {\"n_layer\": cfg.n_layer, \"n_embd\": cfg.n_embd, \"n_head\": cfg.n_head,\n",
    "#                       \"mult\": cfg.mlp_internal_dim_multiplier, \"N_per_head\": N})\n",
    "\n",
    "# # -----------------------------\n",
    "# # 2) Translation\n",
    "# # -----------------------------\n",
    "# ALL_LANGS = {\n",
    "#     \"German\":  \"Helsinki-NLP/opus-mt-en-de\",\n",
    "#     \"French\":  \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "#     \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "#     \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "# }\n",
    "\n",
    "# _trans = {}  # caches tokenizers/models per language\n",
    "\n",
    "# def clear_translation_cache():\n",
    "#     \"\"\"Call this if you want to free memory from translator models.\"\"\"\n",
    "#     global _trans\n",
    "#     _trans = {}\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#     print(\"Translation cache cleared.\")\n",
    "\n",
    "# def translate(lang: str, text: str) -> str:\n",
    "#     if lang not in _trans:\n",
    "#         tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "#         mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "#         _trans[lang] = (tok, mdl)\n",
    "#     tok, mdl = _trans[lang]\n",
    "#     batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "#     out = mdl.generate(**batch, max_new_tokens=64)\n",
    "#     return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 3) Helpers (neurons / activations / sets)\n",
    "# # -----------------------------\n",
    "# EPS = 1e-9\n",
    "\n",
    "# def decode_gid(gid: int):\n",
    "#     \"\"\"gid -> (layer, head, feat)\"\"\"\n",
    "#     per_layer = cfg.n_head * N\n",
    "#     layer = gid // per_layer\n",
    "#     rem = gid % per_layer\n",
    "#     head = rem // N\n",
    "#     feat = rem % N\n",
    "#     return layer, head, feat\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_list_for_text(text: str, layer_idx: int, k: int):\n",
    "#     \"\"\"\n",
    "#     Returns list of (gid, (layer,head,feat), act) for top-k neurons for FULL text.\n",
    "#     \"\"\"\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)    # (nh, N)\n",
    "#     flat = a.reshape(-1)                   # (nh*N,)\n",
    "#     k = min(k, flat.numel())\n",
    "#     vals, idxs = torch.topk(flat, k)\n",
    "#     out = []\n",
    "#     for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         gid = neuron_id(layer_idx, head, feat, cfg.n_head, N)\n",
    "#         out.append((gid, (layer_idx, head, feat), float(v)))\n",
    "#     return out\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_set_for_text(text: str, layer_idx: int, k: int):\n",
    "#     \"\"\"\n",
    "#     Returns set of top-k neuron IDs for FULL text.\n",
    "#     \"\"\"\n",
    "#     hits = topk_list_for_text(text, layer_idx, k)\n",
    "#     return set([gid for (gid, _, _) in hits])\n",
    "\n",
    "# def jaccard(a: set, b: set) -> float:\n",
    "#     return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid(text: str, layer_idx: int, gid: int) -> float:\n",
    "#     \"\"\"\n",
    "#     Mean activation of ONE neuron gid on FULL text.\n",
    "#     \"\"\"\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return 0.0\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)    # (nh, N)\n",
    "#     return float(a[head, feat].item())\n",
    "\n",
    "# def read_list_until_end(prompt: str, min_n: int = 1):\n",
    "#     print(prompt)\n",
    "#     out = []\n",
    "#     while True:\n",
    "#         s = input(\"> \").strip()\n",
    "#         if s.upper() == \"END\":\n",
    "#             break\n",
    "#         if s:\n",
    "#             out.append(s)\n",
    "#     if len(out) < min_n:\n",
    "#         raise ValueError(f\"Need at least {min_n} items.\")\n",
    "#     return out\n",
    "\n",
    "# def shorten(s: str, maxlen: int = 18):\n",
    "#     s = s.replace(\"\\n\", \" \").strip()\n",
    "#     return s if len(s) <= maxlen else (s[:maxlen-1] + \"…\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4) Settings (choose what you want)\n",
    "# # -----------------------------\n",
    "# LAYERS = [4, 5]        # late layers\n",
    "# TOPK_PRINT = 5         # demo table: top 5 neurons\n",
    "# TOPK_SET = 50          # jaccard sets\n",
    "# MAX_CAND = 400         # candidate shared neurons to score (speed)\n",
    "# BASELINE_SEED = 0      # baseline randomness repeatable\n",
    "# N_PROOF_CONCEPTS = 3   # top-N concepts to show proof plots for\n",
    "\n",
    "# LAYERS = [L for L in LAYERS if 0 <= L < cfg.n_layer]\n",
    "\n",
    "# # ============================================================\n",
    "# # (A) DEMO MODE (single concept)\n",
    "# # ============================================================\n",
    "# print(\"\\n=== DEMO MODE (one example) ===\")\n",
    "# demo_text = input(\"Enter an English word/sentence for demo:\\n> \").strip()\n",
    "\n",
    "# print(\"\\nAvailable languages:\")\n",
    "# for k in ALL_LANGS:\n",
    "#     print(\" -\", k)\n",
    "\n",
    "# langs_raw = input(\"\\nChoose languages (comma-separated):\\n> \").strip()\n",
    "# LANGS = [l.strip() for l in langs_raw.split(\",\") if l.strip() in ALL_LANGS]\n",
    "# if len(LANGS) == 0:\n",
    "#     LANGS = [\"French\"]  # safe default\n",
    "\n",
    "# demo_trans = {lang: translate(lang, demo_text) for lang in LANGS}\n",
    "\n",
    "# print(\"\\nTranslations:\")\n",
    "# print(\"EN:\", demo_text)\n",
    "# for lang, txt in demo_trans.items():\n",
    "#     print(f\"{lang[:2].upper()}: {txt}\")\n",
    "\n",
    "# for LAYER in LAYERS:\n",
    "#     print(f\"\\nTOP-{TOPK_PRINT} NEURONS (FULL TEXT) — Layer {LAYER}\")\n",
    "#     print(f\"{'TEXT':<6} {'NEURON_ID':<8} {'(layer,head,feat)':<16} {'ACT'}\")\n",
    "#     print(\"-\" * 48)\n",
    "\n",
    "#     for gid, trip, act in topk_list_for_text(demo_text, LAYER, TOPK_PRINT):\n",
    "#         print(f\"{'EN':<6} {gid:<8} {str(trip):<16} {act:.3f}\")\n",
    "#     for lang, txt in demo_trans.items():\n",
    "#         for gid, trip, act in topk_list_for_text(txt, LAYER, TOPK_PRINT):\n",
    "#             print(f\"{lang[:2].upper():<6} {gid:<8} {str(trip):<16} {act:.3f}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # (B) DATASET MODE: concepts + negatives (USER INPUT)\n",
    "# # ============================================================\n",
    "# print(\"\\n\\n=== DATASET MODE (monosemantic test) ===\")\n",
    "# concepts = read_list_until_end(\n",
    "#     \"Enter CONCEPT words (>=5). Type END to finish:\\n\"\n",
    "#     \"Example: dog, cat, school, doctor, music\\n\"\n",
    "#     \"Type END when done.\",\n",
    "#     min_n=5\n",
    "# )\n",
    "\n",
    "# NEG = read_list_until_end(\n",
    "#     \"\\nEnter NEGATIVE texts (different meaning, >=3). Type END to finish:\\n\"\n",
    "#     \"Tip: use generic unrelated words: laptop, file, table, river, stone ...\",\n",
    "#     min_n=3\n",
    "# )\n",
    "\n",
    "# print(\"\\nConcepts:\", concepts)\n",
    "# print(\"Negatives:\", NEG)\n",
    "# print(\"Languages:\", LANGS)\n",
    "\n",
    "# # Build translations for each concept\n",
    "# concept_trans = []\n",
    "# for w in concepts:\n",
    "#     row = {\"EN\": w}\n",
    "#     for lang in LANGS:\n",
    "#         row[lang] = translate(lang, w)\n",
    "#     concept_trans.append(row)\n",
    "\n",
    "# # ============================================================\n",
    "# # (C) JACCARD ACTUAL vs BASELINE (CLEAR BAR CHART)\n",
    "# # ============================================================\n",
    "# rng = np.random.default_rng(BASELINE_SEED)\n",
    "# concept_names = [ct[\"EN\"] for ct in concept_trans]\n",
    "\n",
    "# def pick_other_index(i, n):\n",
    "#     choices = [j for j in range(n) if j != i]\n",
    "#     return int(rng.choice(choices))\n",
    "\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"Jaccard(EN, translations) per concept + baseline\")\n",
    "# print(\"Baseline = EN compared to translations of a DIFFERENT random concept (same language list)\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# for LAYER in LAYERS:\n",
    "#     actual = []\n",
    "#     baseline = []\n",
    "#     rows = []\n",
    "\n",
    "#     for i, trans in enumerate(concept_trans):\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, LAYER, TOPK_SET)\n",
    "\n",
    "#         # actual: EN vs its translations\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             js.append(jaccard(S_en, topk_set_for_text(trans[lang], LAYER, TOPK_SET)))\n",
    "#         a = float(np.mean(js))\n",
    "#         actual.append(a)\n",
    "\n",
    "#         # baseline: EN vs translations of another random concept\n",
    "#         j = pick_other_index(i, len(concept_trans))\n",
    "#         other = concept_trans[j]\n",
    "#         bjs = []\n",
    "#         for lang in LANGS:\n",
    "#             bjs.append(jaccard(S_en, topk_set_for_text(other[lang], LAYER, TOPK_SET)))\n",
    "#         b = float(np.mean(bjs))\n",
    "#         baseline.append(b)\n",
    "\n",
    "#         rows.append((en, a, b, a - b))\n",
    "\n",
    "#     # Print a clean table\n",
    "#     print(f\"\\n--- Layer {LAYER} ---\")\n",
    "#     print(f\"{'CONCEPT':<16} {'ACTUAL':>8} {'BASE':>8} {'MARGIN':>8}\")\n",
    "#     for (c, a, b, m) in rows:\n",
    "#         print(f\"{c:<16} {a:>8.3f} {b:>8.3f} {m:>8.3f}\")\n",
    "\n",
    "#     # Clear bar chart (Actual vs Baseline)\n",
    "#     x = np.arange(len(concept_names))\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     plt.bar(x - 0.2, actual, width=0.4, label=\"Actual (EN vs translations)\")\n",
    "#     plt.bar(x + 0.2, baseline, width=0.4, label=\"Baseline (EN vs other concept translations)\")\n",
    "#     plt.xticks(x, concept_names, rotation=35, ha=\"right\")\n",
    "#     plt.ylim(0, 1.0)\n",
    "#     plt.title(f\"Layer {LAYER}: Actual vs Baseline Jaccard overlap (TopK={TOPK_SET})\")\n",
    "#     plt.ylabel(\"Jaccard overlap\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # ============================================================\n",
    "# # (D) PROOF PLOTS (TOP N concepts by margin): best shared neuron\n",
    "# # ============================================================\n",
    "# print(\"\\n==============================\")\n",
    "# print(f\"PROOF PLOTS: top {N_PROOF_CONCEPTS} concepts by (Actual - Baseline)\")\n",
    "# print(\"We find a shared neuron among POS texts (EN + translations), then compare vs NEG texts.\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# for LAYER in LAYERS:\n",
    "#     # recompute margins for this layer to choose top concepts\n",
    "#     margins = []\n",
    "#     for i, trans in enumerate(concept_trans):\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, LAYER, TOPK_SET)\n",
    "\n",
    "#         # actual\n",
    "#         a = float(np.mean([\n",
    "#             jaccard(S_en, topk_set_for_text(trans[lang], LAYER, TOPK_SET))\n",
    "#             for lang in LANGS\n",
    "#         ]))\n",
    "\n",
    "#         # baseline\n",
    "#         j = pick_other_index(i, len(concept_trans))\n",
    "#         other = concept_trans[j]\n",
    "#         b = float(np.mean([\n",
    "#             jaccard(S_en, topk_set_for_text(other[lang], LAYER, TOPK_SET))\n",
    "#             for lang in LANGS\n",
    "#         ]))\n",
    "\n",
    "#         margins.append((i, a - b, a, b))\n",
    "\n",
    "#     margins.sort(key=lambda x: x[1], reverse=True)\n",
    "#     top_idxs = [margins[k][0] for k in range(min(N_PROOF_CONCEPTS, len(margins)))]\n",
    "\n",
    "#     print(f\"\\n--- Layer {LAYER}: top concepts by margin ---\")\n",
    "#     for k in range(min(N_PROOF_CONCEPTS, len(margins))):\n",
    "#         i, m, a, b = margins[k]\n",
    "#         print(f\"{k+1}. {concept_trans[i]['EN']:<16} margin={m:>7.3f}  actual={a:>6.3f}  base={b:>6.3f}\")\n",
    "\n",
    "#     # For each selected concept: find best shared neuron and plot activations\n",
    "#     for idx in top_idxs:\n",
    "#         trans = concept_trans[idx]\n",
    "#         concept = trans[\"EN\"]\n",
    "\n",
    "#         POS_texts = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "\n",
    "#         # candidate neurons = intersection of TopK sets across POS texts\n",
    "#         inter = None\n",
    "#         for t in POS_texts:\n",
    "#             S = topk_set_for_text(t, LAYER, TOPK_SET)\n",
    "#             inter = S if inter is None else (inter & S)\n",
    "\n",
    "#         if not inter:\n",
    "#             print(f\"\\nLayer {LAYER} | concept='{concept}': no shared neurons in TopK intersection. (Try bigger TOPK_SET)\")\n",
    "#             continue\n",
    "\n",
    "#         cand = list(inter)[:MAX_CAND]\n",
    "\n",
    "#         # pick best neuron by selectivity = pos_mean - neg_mean\n",
    "#         best = None\n",
    "#         for gid in cand:\n",
    "#             pos_mean = float(np.mean([activation_gid(t, LAYER, gid) for t in POS_texts]))\n",
    "#             neg_mean = float(np.mean([activation_gid(t, LAYER, gid) for t in NEG]))\n",
    "#             sel = pos_mean - neg_mean\n",
    "#             if (best is None) or (sel > best[0]):\n",
    "#                 best = (sel, gid, pos_mean, neg_mean)\n",
    "\n",
    "#         sel, best_gid, pos_mean, neg_mean = best\n",
    "#         lay, head, feat = decode_gid(best_gid)\n",
    "\n",
    "#         # Build a single clear bar plot\n",
    "#         labels = [\"EN\"] + [lang[:2].upper() for lang in LANGS] + [f\"NEG{i+1}\" for i in range(len(NEG))]\n",
    "#         texts = POS_texts + NEG\n",
    "#         acts = [activation_gid(t, LAYER, best_gid) for t in texts]\n",
    "\n",
    "#         xtick = []\n",
    "#         for lab, tx in zip(labels, texts):\n",
    "#             xtick.append(f\"{lab}\\n{shorten(tx, 16)}\")\n",
    "\n",
    "#         plt.figure(figsize=(14, 4))\n",
    "#         plt.bar(np.arange(len(labels)), acts)\n",
    "#         plt.xticks(np.arange(len(labels)), xtick, rotation=0)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(\n",
    "#             f\"Layer {LAYER} | concept='{concept}' | best neuron={best_gid} (head={head}, feat={feat})\\n\"\n",
    "#             f\"selectivity(pos-neg)={sel:.3f} | pos_mean={pos_mean:.3f} | neg_mean={neg_mean:.3f}\"\n",
    "#         )\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         # Print proof table (report-friendly)\n",
    "#         print(f\"\\n[PROOF TABLE] Layer {LAYER} concept='{concept}' best_gid={best_gid} decode={(lay,head,feat)}\")\n",
    "#         print(f\"  selectivity={sel:.4f}  pos_mean={pos_mean:.4f}  neg_mean={neg_mean:.4f}\")\n",
    "#         for lab, tx, av in zip(labels, texts, acts):\n",
    "#             print(f\"  {lab:<5} act={av:.4f} | {tx}\")\n",
    "\n",
    "# print(\"\\nDONE \")\n",
    "# print(\"Tip: If you want to restart fresh, re-run this cell. In Colab you can also Runtime -> Restart runtime.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614f597-f862-4c4e-9f15-b9499f1487ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbd033-5c3d-4a1e-8419-6d2a112b0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # BDH Phase-2 Probe (clear visuals + deterministic baseline + user NEG + optional GIF)\n",
    "# # ============================================\n",
    "\n",
    "# import os, re, math, random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import importlib.util\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # ----------------------------\n",
    "# # 0) Repro / clean start helpers\n",
    "# # ----------------------------\n",
    "# SEED = 1337\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "\n",
    "# def hard_reset_state():\n",
    "#     \"\"\"Notebook-level 'forget': clears translation cache + CUDA cache (does NOT restart runtime).\"\"\"\n",
    "#     global _trans\n",
    "#     _trans = {}\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 1) Load BDH module + checkpoint\n",
    "# # ----------------------------\n",
    "# spec = importlib.util.spec_from_file_location(\"bdhmod\", \"/content/bdh_europarl_train_probe.py\")\n",
    "# bdhmod = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(bdhmod)\n",
    "\n",
    "# BDHConfig   = bdhmod.BDHConfig\n",
    "# BDH         = bdhmod.BDH\n",
    "# ids_for_text= bdhmod.ids_for_text\n",
    "# neuron_id   = bdhmod.neuron_id\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# CKPT = \"checkpoints/bdh_europarl_bytes.pt\"\n",
    "# state = torch.load(CKPT, map_location=device)\n",
    "\n",
    "# nh, D, N_tensor = state[\"encoder\"].shape\n",
    "# mult = int((N_tensor * nh) // D)\n",
    "\n",
    "# cfg = BDHConfig(vocab_size=256, n_layer=6, n_embd=D, n_head=nh, mlp_internal_dim_multiplier=mult, dropout=0.0)\n",
    "# model = BDH(cfg).to(device)\n",
    "# model.load_state_dict(state, strict=True)\n",
    "# model.eval()\n",
    "\n",
    "# N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head  # per-head features\n",
    "\n",
    "# print(\"BDH loaded\")\n",
    "# print(\"Model config:\", {\"n_layer\": cfg.n_layer, \"n_embd\": cfg.n_embd, \"n_head\": cfg.n_head, \"mult\": cfg.mlp_internal_dim_multiplier, \"N_per_head\": N})\n",
    "\n",
    "# # ----------------------------\n",
    "# # 2) Translation (cached) + cache clear\n",
    "# # ----------------------------\n",
    "# ALL_LANGS = {\n",
    "#     \"German\":  \"Helsinki-NLP/opus-mt-en-de\",\n",
    "#     \"French\":  \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "#     \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "#     \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "# }\n",
    "# _trans = {}\n",
    "\n",
    "# def translate(lang, text):\n",
    "#     if lang not in _trans:\n",
    "#         tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "#         mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "#         _trans[lang] = (tok, mdl)\n",
    "#     tok, mdl = _trans[lang]\n",
    "#     batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "#     out = mdl.generate(**batch, max_new_tokens=64)\n",
    "#     return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 3) Core helpers\n",
    "# # ----------------------------\n",
    "# EPS = 1e-9\n",
    "# LAYERS = [4, 5]\n",
    "# LAYERS = [L for L in LAYERS if 0 <= L < cfg.n_layer]\n",
    "\n",
    "# TOPK_PRINT = 5\n",
    "# TOPK_SET   = 50\n",
    "# MAX_CAND   = 400\n",
    "\n",
    "# def tokenize_words(s: str):\n",
    "#     return re.findall(r\"[A-Za-zÀ-ÿ]+\", s.lower())\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_set_for_text(text: str, layer_idx: int, k: int):\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)  # (nh, N)\n",
    "#     flat = a.reshape(-1)\n",
    "#     k = min(k, flat.numel())\n",
    "#     _, idxs = torch.topk(flat, k)\n",
    "#     S = set()\n",
    "#     for ix in idxs.tolist():\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         S.add(neuron_id(layer_idx, head, feat, cfg.n_head, N))\n",
    "#     return S\n",
    "\n",
    "# def jaccard(a: set, b: set) -> float:\n",
    "#     return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "# def decode_gid(gid: int):\n",
    "#     per_layer = cfg.n_head * N\n",
    "#     layer = gid // per_layer\n",
    "#     rem = gid % per_layer\n",
    "#     head = rem // N\n",
    "#     feat = rem % N\n",
    "#     return layer, head, feat\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid_mean(text: str, layer_idx: int, gid: int) -> float:\n",
    "#     \"\"\"Mean activation of ONE neuron over positions.\"\"\"\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return 0.0\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0]  # (nh, T, N) since B=1\n",
    "#     return float(a[head, :, feat].mean().item())\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid_over_positions(text: str, layer_idx: int, gid: int):\n",
    "#     \"\"\"Per-position activations: (T,) for ONE neuron (good for GIF).\"\"\"\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return np.array([0.0])\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0]  # (nh, T, N)\n",
    "#     return a[head, :, feat].detach().float().cpu().numpy()\n",
    "\n",
    "# def read_list_until_end(prompt, min_n=1):\n",
    "#     print(prompt)\n",
    "#     out = []\n",
    "#     while True:\n",
    "#         s = input(\"> \").strip()\n",
    "#         if s.upper() == \"END\":\n",
    "#             break\n",
    "#         if s:\n",
    "#             out.append(s)\n",
    "#     if len(out) < min_n:\n",
    "#         raise ValueError(f\"Need at least {min_n} items.\")\n",
    "#     return out\n",
    "\n",
    "# # ----------------------------\n",
    "# # 4) DEMO MODE (Top-K neuron IDs, full-text)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== DEMO MODE ===\")\n",
    "# sentence = input(\"Enter an English word/sentence:\\n> \").strip()\n",
    "\n",
    "# print(\"\\nAvailable languages:\")\n",
    "# for k in ALL_LANGS: print(\" -\", k)\n",
    "\n",
    "# langs_raw = input(\"\\nChoose languages (comma-separated, blank=all):\\n> \").strip()\n",
    "# LANGS = [l.strip() for l in langs_raw.split(\",\") if l.strip() in ALL_LANGS]\n",
    "# if len(LANGS) == 0:\n",
    "#     LANGS = list(ALL_LANGS.keys())\n",
    "\n",
    "# translations = {lang: translate(lang, sentence) for lang in LANGS}\n",
    "\n",
    "# print(\"\\nTranslations:\")\n",
    "# print(\"EN:\", sentence)\n",
    "# for lang, txt in translations.items():\n",
    "#     print(f\"{lang[:2].upper()}: {txt}\")\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_list_fulltext(text, layer_idx, k=TOPK_PRINT):\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)  # (nh,N)\n",
    "#     flat = a.reshape(-1)\n",
    "#     k = min(k, flat.numel())\n",
    "#     vals, idxs = torch.topk(flat, k)\n",
    "#     out = []\n",
    "#     for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         gid = neuron_id(layer_idx, head, feat, cfg.n_head, N)\n",
    "#         out.append((gid, (layer_idx, head, feat), float(v)))\n",
    "#     return out\n",
    "\n",
    "# for L in LAYERS:\n",
    "#     print(f\"\\nTOP-{TOPK_PRINT} NEURONS (FULL TEXT) — Layer {L}\")\n",
    "#     print(f\"{'TEXT':<6} {'NEURON_ID':<10} {'(layer,head,feat)':<18} {'ACT'}\")\n",
    "#     print(\"-\"*60)\n",
    "#     for gid, trip, act in topk_list_fulltext(sentence, L):\n",
    "#         print(f\"{'EN':<6} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "#     for lang, txt in translations.items():\n",
    "#         for gid, trip, act in topk_list_fulltext(txt, L):\n",
    "#             print(f\"{lang[:2].upper():<6} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 5) DATASET MODE (concepts + NEG words)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== DATASET MODE (monosemantic test) ===\")\n",
    "# concepts = read_list_until_end(\n",
    "#     \"Enter CONCEPT words (>=5). Type END to finish:\",\n",
    "#     min_n=5\n",
    "# )\n",
    "\n",
    "# # translate each concept into chosen languages\n",
    "# concept_trans = []\n",
    "# for w in concepts:\n",
    "#     trans = {\"EN\": w}\n",
    "#     for lang in LANGS:\n",
    "#         trans[lang] = translate(lang, w)\n",
    "#     concept_trans.append(trans)\n",
    "\n",
    "# # NEG words: ALWAYS ask user (or AUTO)\n",
    "# print(\"\\nNEGATIVE SET:\")\n",
    "# print(\"Type negative words (different meaning).\")\n",
    "# print(\"Tip: use unrelated categories (tools, places, verbs, numbers).\")\n",
    "# neg_mode = input(\"Type 'AUTO' to use a default negative list, else press Enter to type manually:\\n> \").strip().upper()\n",
    "\n",
    "# DEFAULT_NEG = [\"table\",\"river\",\"engine\",\"laptop\",\"file\",\"money\",\"music\",\"doctor\",\"mountain\",\"battery\",\"cloud\",\"kitchen\",\"orange\",\"ten\",\"run\"]\n",
    "\n",
    "# if neg_mode == \"AUTO\":\n",
    "#     NEG = [w for w in DEFAULT_NEG if w.lower() not in set(c.lower() for c in concepts)]\n",
    "#     NEG = NEG[:8]  # keep small and readable\n",
    "#     print(\"Using AUTO NEG:\", NEG)\n",
    "# else:\n",
    "#     NEG = read_list_until_end(\"Enter NEGATIVE words (>=3). Type END to finish:\", min_n=3)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 6) Jaccard actual vs baseline (DETERMINISTIC baseline)\n",
    "# # ----------------------------\n",
    "# def baseline_pair_index(i, n):\n",
    "#     \"\"\"Deterministic baseline: compare concept i with translations of concept (i+1)%n.\"\"\"\n",
    "#     return (i + 1) % n\n",
    "\n",
    "# def compute_actual_and_baseline(layer_idx):\n",
    "#     names = [ct[\"EN\"] for ct in concept_trans]\n",
    "#     actual_list = []\n",
    "#     base_list = []\n",
    "\n",
    "#     for i, trans in enumerate(concept_trans):\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, layer_idx, TOPK_SET)\n",
    "\n",
    "#         # actual = mean jaccard (EN vs its translations)\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             S_tr = topk_set_for_text(trans[lang], layer_idx, TOPK_SET)\n",
    "#             js.append(jaccard(S_en, S_tr))\n",
    "#         actual = float(np.mean(js))\n",
    "\n",
    "#         # baseline = EN vs OTHER concept translations\n",
    "#         j = baseline_pair_index(i, len(concept_trans))\n",
    "#         other = concept_trans[j]\n",
    "#         base_js = []\n",
    "#         for lang in LANGS:\n",
    "#             S_other = topk_set_for_text(other[lang], layer_idx, TOPK_SET)\n",
    "#             base_js.append(jaccard(S_en, S_other))\n",
    "#         baseline = float(np.mean(base_js))\n",
    "\n",
    "#         actual_list.append(actual)\n",
    "#         base_list.append(baseline)\n",
    "\n",
    "#     return names, np.array(actual_list), np.array(base_list)\n",
    "\n",
    "# # Plot: clear actual vs baseline per layer\n",
    "# results = {}\n",
    "# for L in LAYERS:\n",
    "#     names, actual_arr, base_arr = compute_actual_and_baseline(L)\n",
    "#     results[L] = (names, actual_arr, base_arr)\n",
    "\n",
    "#     print(f\"\\nLayer {L}: actual_mean={actual_arr.mean():.3f}  baseline_mean={base_arr.mean():.3f}  (want actual > baseline)\")\n",
    "\n",
    "#     x = np.arange(len(names))\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     plt.bar(x - 0.2, actual_arr, width=0.4, label=\"Actual (EN vs its translations)\")\n",
    "#     plt.bar(x + 0.2, base_arr,   width=0.4, label=\"Baseline (EN vs other-concept translations)\")\n",
    "#     plt.xticks(x, names, rotation=45, ha=\"right\")\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.ylabel(\"Jaccard overlap\")\n",
    "#     plt.title(f\"Layer {L}: Actual vs Baseline Jaccard (TopK={TOPK_SET})\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 7) Best shared neuron per concept (with selectivity)\n",
    "# # ----------------------------\n",
    "# @torch.no_grad()\n",
    "# @torch.no_grad()\n",
    "# def best_shared_neuron_for_concept(i, layer_idx):\n",
    "#     trans = concept_trans[i]\n",
    "#     POS = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "\n",
    "#     inter = None\n",
    "#     for t in POS:\n",
    "#         S = topk_set_for_text(t, layer_idx, TOPK_SET)\n",
    "#         inter = S if inter is None else (inter & S)\n",
    "\n",
    "#     S_en = topk_set_for_text(POS[0], layer_idx, TOPK_SET)\n",
    "\n",
    "#     if inter is None:\n",
    "#         inter = set()\n",
    "\n",
    "#     cand_set = set(inter) | set(S_en)\n",
    "#     if len(cand_set) == 0:\n",
    "#         return None\n",
    "\n",
    "#     cand = list(cand_set)[:MAX_CAND]\n",
    "\n",
    "#     best = None\n",
    "#     for gid in cand:\n",
    "#         pos_mean = float(np.mean([activation_gid_mean(t, layer_idx, gid) for t in POS]))\n",
    "#         neg_mean = float(np.mean([activation_gid_mean(t, layer_idx, gid) for t in NEG]))\n",
    "#         sel = pos_mean - neg_mean\n",
    "#         if (best is None) or (sel > best[1]):\n",
    "#             best = (gid, sel, pos_mean, neg_mean)\n",
    "\n",
    "#     return best\n",
    "# def short(s, n=18):\n",
    "#     s = s.replace(\"\\n\",\" \")\n",
    "#     return s if len(s) <= n else s[:n-3] + \"...\"\n",
    "\n",
    "# TOP_SHOW = min(3, len(concept_trans))\n",
    "\n",
    "# for L in LAYERS:\n",
    "#     names, actual_arr, base_arr = results[L]\n",
    "#     margins = actual_arr - base_arr\n",
    "#     order = np.argsort(-margins)[:TOP_SHOW]\n",
    "\n",
    "#     print(f\"\\n=== Layer {L}: proof plots for top {TOP_SHOW} margin concepts ===\")\n",
    "\n",
    "#     for idx in order:\n",
    "#         concept = concept_trans[idx][\"EN\"]\n",
    "#         best = best_shared_neuron_for_concept(idx, L)\n",
    "#         if best is None:\n",
    "#             print(f\"Concept '{concept}': no shared neuron intersection.\")\n",
    "#             continue\n",
    "\n",
    "#         gid, sel, pos_m, neg_m = best\n",
    "\n",
    "#         POS_texts  = [concept_trans[idx][\"EN\"]] + [concept_trans[idx][lang] for lang in LANGS]\n",
    "#         POS_labels = [\"EN\"] + [lang[:2].upper() for lang in LANGS]\n",
    "\n",
    "#         NEG_texts  = NEG\n",
    "#         NEG_labels = [f\"NEG{i+1}\" for i in range(len(NEG_texts))]\n",
    "\n",
    "#         all_texts  = POS_texts + NEG_texts\n",
    "#         all_labels = POS_labels + NEG_labels\n",
    "\n",
    "#         acts = np.array([activation_gid_mean(t, L, gid) for t in all_texts], dtype=np.float64)\n",
    "\n",
    "#         # 7A) Activation BAR plot (clear labels)\n",
    "#         plt.figure(figsize=(12,4))\n",
    "#         plt.bar(np.arange(len(all_labels)), acts)\n",
    "#         plt.xticks(np.arange(len(all_labels)),\n",
    "#                    [f\"{lab}\\n{short(tx)}\" for lab, tx in zip(all_labels, all_texts)],\n",
    "#                    rotation=0)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(\n",
    "#             f\"Layer {L} | concept='{concept}' | best neuron={gid} decode={decode_gid(gid)}\\n\"\n",
    "#             f\"selectivity(pos-neg)={sel:.3f} (pos_mean={pos_m:.3f}, neg_mean={neg_m:.3f})\"\n",
    "#         )\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         # 7B) POS vs NEG boxplot (shows “fires for POS, not NEG” much clearer)\n",
    "#         pos_vals = acts[:len(POS_texts)]\n",
    "#         neg_vals = acts[len(POS_texts):]\n",
    "\n",
    "#         plt.figure(figsize=(6,4))\n",
    "#         plt.boxplot([pos_vals, neg_vals], labels=[\"POS (EN+translations)\", \"NEG\"], showmeans=True)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(f\"Layer {L} neuron {gid}: POS vs NEG activation distribution\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         print(f\"\\nConcept: {concept}\")\n",
    "#         print(f\"  Neuron: {gid} decode={decode_gid(gid)}\")\n",
    "#         print(f\"  Selectivity: {sel:.4f}  pos_mean={pos_m:.4f}  neg_mean={neg_m:.4f}\")\n",
    "#         for lab, tx, av in zip(all_labels, all_texts, acts.tolist()):\n",
    "#             print(f\"    {lab:<5} act={av:.4f} | {tx}\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 8) OPTIONAL: Make a GIF (activation over byte positions)\n",
    "# # ----------------------------\n",
    "# make_gif = input(\"\\nMake a GIF of per-byte activation for ONE concept? (y/n)\\n> \").strip().lower() == \"y\"\n",
    "# if make_gif:\n",
    "#     import imageio.v2 as imageio\n",
    "#     from IPython.display import Image, display\n",
    "\n",
    "#     # pick: best margin concept in first layer for GIF\n",
    "#     L = LAYERS[0]\n",
    "#     names, actual_arr, base_arr = results[L]\n",
    "#     best_idx = int(np.argmax(actual_arr - base_arr))\n",
    "#     concept = concept_trans[best_idx][\"EN\"]\n",
    "#     best = best_shared_neuron_for_concept(best_idx, L)\n",
    "\n",
    "#     if best is None:\n",
    "#         print(\"No shared neuron found for GIF.\")\n",
    "#     else:\n",
    "#         gid, sel, pos_m, neg_m = best\n",
    "\n",
    "#         en_text = concept_trans[best_idx][\"EN\"]\n",
    "#         other_lang = LANGS[0]  # first chosen language\n",
    "#         tr_text = concept_trans[best_idx][other_lang]\n",
    "\n",
    "#         seqs = [(\"EN\", en_text), (other_lang[:2].upper(), tr_text)]\n",
    "\n",
    "#         gif_path = \"bdh_activation.gif\"\n",
    "#         out_frames = []\n",
    "\n",
    "#         for tag, text in seqs:\n",
    "#             vals = activation_gid_over_positions(text, L, gid)  # (T,)\n",
    "\n",
    "#             max_frames = min(len(vals), 120)  # cap frames so it renders nicely\n",
    "#             for t in range(1, max_frames + 1):\n",
    "#                 fig = plt.figure(figsize=(8, 3))\n",
    "#                 plt.plot(np.arange(t), vals[:t])\n",
    "#                 plt.ylim(0, max(vals.max() * 1.1, 1e-3))\n",
    "#                 plt.xlabel(\"Byte position\")\n",
    "#                 plt.ylabel(\"Activation\")\n",
    "#                 plt.title(f\"Layer {L} neuron {gid} | {tag} '{short(text, 30)}' | t={t}/{len(vals)}\")\n",
    "#                 plt.tight_layout()\n",
    "\n",
    "#                 fig.canvas.draw()\n",
    "\n",
    "#                 # Matplotlib 3.9+ safe rendering\n",
    "#                 buf = np.asarray(fig.canvas.buffer_rgba())      # (H, W, 4)\n",
    "#                 out_frames.append(buf[:, :, :3].copy())         # (H, W, 3)\n",
    "\n",
    "#                 plt.close(fig)\n",
    "\n",
    "#         imageio.mimsave(gif_path, out_frames, fps=10)\n",
    "#         print(\"Saved GIF:\", gif_path)\n",
    "\n",
    "#         # show gif in output\n",
    "#         display(Image(filename=gif_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3036aaf-73f6-4a72-b8b2-8c5b40caf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # BDH Phase-2 Probe (PLOTS + GIF working reliably in Colab/Jupyter)\n",
    "# # Fixes:\n",
    "# # - Forces inline plotting\n",
    "# # - Fixes best_shared_neuron_for_concept (no dead code / no double decorator)\n",
    "# # - Uses Matplotlib 3.9+ safe frame capture for GIF (buffer_rgba)\n",
    "# # - Displays the GIF in notebook output\n",
    "# # ============================================\n",
    "\n",
    "# # --- (A) Notebook display setup: MUST be at top in Colab/Jupyter ---\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Force inline backend in notebooks (safe even if already inline)\n",
    "# try:\n",
    "#     get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# # Make sure interactive isn't blocking (Colab friendly)\n",
    "# plt.ioff()\n",
    "\n",
    "# import os, re, math, random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import importlib.util\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # ----------------------------\n",
    "# # 0) Repro / clean start helpers\n",
    "# # ----------------------------\n",
    "# SEED = 1337\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "\n",
    "# def hard_reset_state():\n",
    "#     \"\"\"Notebook-level 'forget': clears translation cache + CUDA cache (does NOT restart runtime).\"\"\"\n",
    "#     global _trans\n",
    "#     _trans = {}\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 1) Load BDH module + checkpoint\n",
    "# # ----------------------------\n",
    "# spec = importlib.util.spec_from_file_location(\"bdhmod\", \"/content/bdh_europarl_train_probe.py\")\n",
    "# bdhmod = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(bdhmod)\n",
    "\n",
    "# BDHConfig    = bdhmod.BDHConfig\n",
    "# BDH          = bdhmod.BDH\n",
    "# ids_for_text = bdhmod.ids_for_text\n",
    "# neuron_id    = bdhmod.neuron_id\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# CKPT = \"checkpoints/bdh_europarl_bytes.pt\"\n",
    "# state = torch.load(CKPT, map_location=device)\n",
    "\n",
    "# nh, D, N_tensor = state[\"encoder\"].shape\n",
    "# mult = int((N_tensor * nh) // D)\n",
    "\n",
    "# cfg = BDHConfig(vocab_size=256, n_layer=6, n_embd=D, n_head=nh,\n",
    "#                 mlp_internal_dim_multiplier=mult, dropout=0.0)\n",
    "\n",
    "# model = BDH(cfg).to(device)\n",
    "# model.load_state_dict(state, strict=True)\n",
    "# model.eval()\n",
    "\n",
    "# N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head  # per-head features\n",
    "\n",
    "# print(\"BDH loaded\")\n",
    "# print(\"Model config:\", {\n",
    "#     \"n_layer\": cfg.n_layer, \"n_embd\": cfg.n_embd,\n",
    "#     \"n_head\": cfg.n_head, \"mult\": cfg.mlp_internal_dim_multiplier,\n",
    "#     \"N_per_head\": N\n",
    "# })\n",
    "\n",
    "# # ----------------------------\n",
    "# # 2) Translation (cached)\n",
    "# # ----------------------------\n",
    "# ALL_LANGS = {\n",
    "#     \"German\":  \"Helsinki-NLP/opus-mt-en-de\",\n",
    "#     \"French\":  \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "#     \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "#     \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "# }\n",
    "# _trans = {}\n",
    "\n",
    "# def translate(lang, text):\n",
    "#     if lang not in _trans:\n",
    "#         tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "#         mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "#         _trans[lang] = (tok, mdl)\n",
    "#     tok, mdl = _trans[lang]\n",
    "#     batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "#     out = mdl.generate(**batch, max_new_tokens=64)\n",
    "#     return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 3) Core helpers\n",
    "# # ----------------------------\n",
    "# EPS = 1e-9\n",
    "# LAYERS = [4, 5]\n",
    "# LAYERS = [L for L in LAYERS if 0 <= L < cfg.n_layer]\n",
    "\n",
    "# TOPK_PRINT = 5\n",
    "# TOPK_SET   = 50\n",
    "# MAX_CAND   = 400\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_set_for_text(text: str, layer_idx: int, k: int):\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)  # (nh, N)\n",
    "#     flat = a.reshape(-1)\n",
    "#     k = min(k, flat.numel())\n",
    "#     _, idxs = torch.topk(flat, k)\n",
    "#     S = set()\n",
    "#     for ix in idxs.tolist():\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         S.add(neuron_id(layer_idx, head, feat, cfg.n_head, N))\n",
    "#     return S\n",
    "\n",
    "# def jaccard(a: set, b: set) -> float:\n",
    "#     return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "# def decode_gid(gid: int):\n",
    "#     per_layer = cfg.n_head * N\n",
    "#     layer = gid // per_layer\n",
    "#     rem = gid % per_layer\n",
    "#     head = rem // N\n",
    "#     feat = rem % N\n",
    "#     return layer, head, feat\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid_mean(text: str, layer_idx: int, gid: int) -> float:\n",
    "#     \"\"\"Mean activation of ONE neuron over positions.\"\"\"\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return 0.0\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0]  # (nh, T, N)\n",
    "#     return float(a[head, :, feat].mean().item())\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid_over_positions(text: str, layer_idx: int, gid: int):\n",
    "#     \"\"\"Per-position activations (T,) for ONE neuron.\"\"\"\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return np.array([0.0])\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0]\n",
    "#     return a[head, :, feat].detach().float().cpu().numpy()\n",
    "\n",
    "# def read_list_until_end(prompt, min_n=1):\n",
    "#     print(prompt)\n",
    "#     out = []\n",
    "#     while True:\n",
    "#         s = input(\"> \").strip()\n",
    "#         if s.upper() == \"END\":\n",
    "#             break\n",
    "#         if s:\n",
    "#             out.append(s)\n",
    "#     if len(out) < min_n:\n",
    "#         raise ValueError(f\"Need at least {min_n} items.\")\n",
    "#     return out\n",
    "\n",
    "# def short(s, n=18):\n",
    "#     s = s.replace(\"\\n\", \" \")\n",
    "#     return s if len(s) <= n else s[:n-3] + \"...\"\n",
    "\n",
    "# def showfig():\n",
    "#     \"\"\"Make sure figures actually render in notebooks.\"\"\"\n",
    "#     plt.show()\n",
    "#     plt.close(\"all\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 4) DEMO MODE (Top-K neurons)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== DEMO MODE ===\")\n",
    "# sentence = input(\"Enter an English word/sentence:\\n> \").strip()\n",
    "\n",
    "# print(\"\\nAvailable languages:\")\n",
    "# for k in ALL_LANGS:\n",
    "#     print(\" -\", k)\n",
    "\n",
    "# langs_raw = input(\"\\nChoose languages (comma-separated, blank=all):\\n> \").strip()\n",
    "# LANGS = [l.strip() for l in langs_raw.split(\",\") if l.strip() in ALL_LANGS]\n",
    "# if len(LANGS) == 0:\n",
    "#     LANGS = list(ALL_LANGS.keys())\n",
    "\n",
    "# translations = {lang: translate(lang, sentence) for lang in LANGS}\n",
    "\n",
    "# print(\"\\nTranslations:\")\n",
    "# print(\"EN:\", sentence)\n",
    "# for lang, txt in translations.items():\n",
    "#     print(f\"{lang[:2].upper()}: {txt}\")\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_list_fulltext(text, layer_idx, k=TOPK_PRINT):\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)  # (nh,N)\n",
    "#     flat = a.reshape(-1)\n",
    "#     k = min(k, flat.numel())\n",
    "#     vals, idxs = torch.topk(flat, k)\n",
    "#     out = []\n",
    "#     for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         gid = neuron_id(layer_idx, head, feat, cfg.n_head, N)\n",
    "#         out.append((gid, (layer_idx, head, feat), float(v)))\n",
    "#     return out\n",
    "\n",
    "# for L in LAYERS:\n",
    "#     print(f\"\\nTOP-{TOPK_PRINT} NEURONS (FULL TEXT) — Layer {L}\")\n",
    "#     print(f\"{'TEXT':<6} {'NEURON_ID':<10} {'(layer,head,feat)':<18} {'ACT'}\")\n",
    "#     print(\"-\"*60)\n",
    "#     for gid, trip, act in topk_list_fulltext(sentence, L):\n",
    "#         print(f\"{'EN':<6} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "#     for lang, txt in translations.items():\n",
    "#         for gid, trip, act in topk_list_fulltext(txt, L):\n",
    "#             print(f\"{lang[:2].upper():<6} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 5) DATASET MODE (concepts + NEG)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== DATASET MODE (monosemantic test) ===\")\n",
    "# concepts = read_list_until_end(\"Enter CONCEPT words (>=5). Type END to finish:\", min_n=5)\n",
    "\n",
    "# concept_trans = []\n",
    "# for w in concepts:\n",
    "#     trans = {\"EN\": w}\n",
    "#     for lang in LANGS:\n",
    "#         trans[lang] = translate(lang, w)\n",
    "#     concept_trans.append(trans)\n",
    "\n",
    "# print(\"\\nNEGATIVE SET:\")\n",
    "# print(\"Type negative words (different meaning).\")\n",
    "# print(\"Tip: use unrelated categories (tools, places, verbs, numbers).\")\n",
    "# neg_mode = input(\"Type 'AUTO' to use a default negative list, else press Enter to type manually:\\n> \").strip().upper()\n",
    "\n",
    "# DEFAULT_NEG = [\"table\",\"river\",\"engine\",\"laptop\",\"file\",\"money\",\"music\",\"doctor\",\"mountain\",\"battery\",\"cloud\",\"kitchen\",\"orange\",\"ten\",\"run\"]\n",
    "# if neg_mode == \"AUTO\":\n",
    "#     NEG = [w for w in DEFAULT_NEG if w.lower() not in set(c.lower() for c in concepts)]\n",
    "#     NEG = NEG[:8]\n",
    "#     print(\"Using AUTO NEG:\", NEG)\n",
    "# else:\n",
    "#     NEG = read_list_until_end(\"Enter NEGATIVE words (>=3). Type END to finish:\", min_n=3)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 6) Actual vs Baseline Jaccard\n",
    "# # ----------------------------\n",
    "# def baseline_pair_index(i, n):\n",
    "#     return (i + 1) % n\n",
    "\n",
    "# def compute_actual_and_baseline(layer_idx):\n",
    "#     names = [ct[\"EN\"] for ct in concept_trans]\n",
    "#     actual_list, base_list = [], []\n",
    "\n",
    "#     for i, trans in enumerate(concept_trans):\n",
    "#         en = trans[\"EN\"]\n",
    "#         S_en = topk_set_for_text(en, layer_idx, TOPK_SET)\n",
    "\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             S_tr = topk_set_for_text(trans[lang], layer_idx, TOPK_SET)\n",
    "#             js.append(jaccard(S_en, S_tr))\n",
    "#         actual = float(np.mean(js))\n",
    "\n",
    "#         j = baseline_pair_index(i, len(concept_trans))\n",
    "#         other = concept_trans[j]\n",
    "#         base_js = []\n",
    "#         for lang in LANGS:\n",
    "#             S_other = topk_set_for_text(other[lang], layer_idx, TOPK_SET)\n",
    "#             base_js.append(jaccard(S_en, S_other))\n",
    "#         baseline = float(np.mean(base_js))\n",
    "\n",
    "#         actual_list.append(actual)\n",
    "#         base_list.append(baseline)\n",
    "\n",
    "#     return names, np.array(actual_list), np.array(base_list)\n",
    "\n",
    "# results = {}\n",
    "# for L in LAYERS:\n",
    "#     names, actual_arr, base_arr = compute_actual_and_baseline(L)\n",
    "#     results[L] = (names, actual_arr, base_arr)\n",
    "\n",
    "#     print(f\"\\nLayer {L}: actual_mean={actual_arr.mean():.3f}  baseline_mean={base_arr.mean():.3f}  (want actual > baseline)\")\n",
    "\n",
    "#     x = np.arange(len(names))\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     plt.bar(x - 0.2, actual_arr, width=0.4, label=\"Actual (EN vs its translations)\")\n",
    "#     plt.bar(x + 0.2, base_arr,   width=0.4, label=\"Baseline (EN vs other-concept translations)\")\n",
    "#     plt.xticks(x, names, rotation=45, ha=\"right\")\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.ylabel(\"Jaccard overlap\")\n",
    "#     plt.title(f\"Layer {L}: Actual vs Baseline Jaccard (TopK={TOPK_SET})\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     showfig()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 7) Best shared neuron per concept (FIXED)\n",
    "# # ----------------------------\n",
    "# @torch.no_grad()\n",
    "# def best_shared_neuron_for_concept(i, layer_idx):\n",
    "#     trans = concept_trans[i]\n",
    "#     POS = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "\n",
    "#     inter = None\n",
    "#     for t in POS:\n",
    "#         S = topk_set_for_text(t, layer_idx, TOPK_SET)\n",
    "#         inter = S if inter is None else (inter & S)\n",
    "\n",
    "#     S_en = topk_set_for_text(POS[0], layer_idx, TOPK_SET)\n",
    "#     if inter is None:\n",
    "#         inter = set()\n",
    "\n",
    "#     cand_set = set(inter) | set(S_en)\n",
    "#     if len(cand_set) == 0:\n",
    "#         return None\n",
    "\n",
    "#     cand = list(cand_set)[:MAX_CAND]\n",
    "\n",
    "#     best = None\n",
    "#     for gid in cand:\n",
    "#         pos_mean = float(np.mean([activation_gid_mean(t, layer_idx, gid) for t in POS]))\n",
    "#         neg_mean = float(np.mean([activation_gid_mean(t, layer_idx, gid) for t in NEG]))\n",
    "#         sel = pos_mean - neg_mean\n",
    "#         if (best is None) or (sel > best[1]):\n",
    "#             best = (gid, sel, pos_mean, neg_mean)\n",
    "\n",
    "#     return best  # (gid, sel, pos_mean, neg_mean)\n",
    "\n",
    "# TOP_SHOW = min(3, len(concept_trans))\n",
    "\n",
    "# for L in LAYERS:\n",
    "#     names, actual_arr, base_arr = results[L]\n",
    "#     margins = actual_arr - base_arr\n",
    "#     order = np.argsort(-margins)[:TOP_SHOW]\n",
    "\n",
    "#     print(f\"\\n=== Layer {L}: proof plots for top {TOP_SHOW} margin concepts ===\")\n",
    "\n",
    "#     for idx in order:\n",
    "#         concept = concept_trans[idx][\"EN\"]\n",
    "#         best = best_shared_neuron_for_concept(idx, L)\n",
    "#         if best is None:\n",
    "#             print(f\"Concept '{concept}': no shared neuron candidate set.\")\n",
    "#             continue\n",
    "\n",
    "#         gid, sel, pos_m, neg_m = best\n",
    "\n",
    "#         POS_texts  = [concept_trans[idx][\"EN\"]] + [concept_trans[idx][lang] for lang in LANGS]\n",
    "#         POS_labels = [\"EN\"] + [lang[:2].upper() for lang in LANGS]\n",
    "\n",
    "#         NEG_texts  = NEG\n",
    "#         NEG_labels = [f\"NEG{i+1}\" for i in range(len(NEG_texts))]\n",
    "\n",
    "#         all_texts  = POS_texts + NEG_texts\n",
    "#         all_labels = POS_labels + NEG_labels\n",
    "\n",
    "#         acts = np.array([activation_gid_mean(t, L, gid) for t in all_texts], dtype=np.float64)\n",
    "\n",
    "#         # 7A) Bar plot\n",
    "#         plt.figure(figsize=(12,4))\n",
    "#         plt.bar(np.arange(len(all_labels)), acts)\n",
    "#         plt.xticks(np.arange(len(all_labels)),\n",
    "#                    [f\"{lab}\\n{short(tx)}\" for lab, tx in zip(all_labels, all_texts)],\n",
    "#                    rotation=0)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(\n",
    "#             f\"Layer {L} | concept='{concept}' | best neuron={gid} decode={decode_gid(gid)}\\n\"\n",
    "#             f\"selectivity(pos-neg)={sel:.3f} (pos_mean={pos_m:.3f}, neg_mean={neg_m:.3f})\"\n",
    "#         )\n",
    "#         plt.tight_layout()\n",
    "#         showfig()\n",
    "\n",
    "#         # 7B) Boxplot (Matplotlib 3.9+ prefers tick_labels, but labels still works; keep simple)\n",
    "#         pos_vals = acts[:len(POS_texts)]\n",
    "#         neg_vals = acts[len(POS_texts):]\n",
    "\n",
    "#         plt.figure(figsize=(6,4))\n",
    "#         plt.boxplot([pos_vals, neg_vals], tick_labels=[\"POS (EN+translations)\", \"NEG\"], showmeans=True)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(f\"Layer {L} neuron {gid}: POS vs NEG activation distribution\")\n",
    "#         plt.tight_layout()\n",
    "#         showfig()\n",
    "\n",
    "#         print(f\"\\nConcept: {concept}\")\n",
    "#         print(f\"  Neuron: {gid} decode={decode_gid(gid)}\")\n",
    "#         print(f\"  Selectivity: {sel:.4f}  pos_mean={pos_m:.4f}  neg_mean={neg_m:.4f}\")\n",
    "#         for lab, tx, av in zip(all_labels, all_texts, acts.tolist()):\n",
    "#             print(f\"    {lab:<5} act={av:.4f} | {tx}\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 8) OPTIONAL: GIF (Matplotlib 3.9+ safe)\n",
    "# # ----------------------------\n",
    "# make_gif = input(\"\\nMake a GIF of per-byte activation for ONE concept? (y/n)\\n> \").strip().lower() == \"y\"\n",
    "# if make_gif:\n",
    "#     import imageio.v2 as imageio\n",
    "#     from IPython.display import Image, display\n",
    "\n",
    "#     L = LAYERS[0]\n",
    "#     names, actual_arr, base_arr = results[L]\n",
    "#     best_idx = int(np.argmax(actual_arr - base_arr))\n",
    "#     concept = concept_trans[best_idx][\"EN\"]\n",
    "#     best = best_shared_neuron_for_concept(best_idx, L)\n",
    "\n",
    "#     if best is None:\n",
    "#         print(\"No shared neuron found for GIF.\")\n",
    "#     else:\n",
    "#         gid, sel, pos_m, neg_m = best\n",
    "\n",
    "#         en_text = concept_trans[best_idx][\"EN\"]\n",
    "#         other_lang = LANGS[0]\n",
    "#         tr_text = concept_trans[best_idx][other_lang]\n",
    "\n",
    "#         seqs = [(\"EN\", en_text), (other_lang[:2].upper(), tr_text)]\n",
    "\n",
    "#         gif_path = \"bdh_activation.gif\"\n",
    "#         out_frames = []\n",
    "\n",
    "#         for tag, text in seqs:\n",
    "#             vals = activation_gid_over_positions(text, L, gid)\n",
    "#             max_frames = min(len(vals), 120)\n",
    "\n",
    "#             for t in range(1, max_frames + 1):\n",
    "#                 fig = plt.figure(figsize=(8, 3))\n",
    "#                 plt.plot(np.arange(t), vals[:t])\n",
    "#                 plt.ylim(0, max(vals.max() * 1.1, 1e-3))\n",
    "#                 plt.xlabel(\"Byte position\")\n",
    "#                 plt.ylabel(\"Activation\")\n",
    "#                 plt.title(f\"Layer {L} neuron {gid} | {tag} '{short(text, 30)}' | t={t}/{len(vals)}\")\n",
    "#                 plt.tight_layout()\n",
    "\n",
    "#                 fig.canvas.draw()\n",
    "#                 buf = np.asarray(fig.canvas.buffer_rgba())      # (H, W, 4)\n",
    "#                 out_frames.append(buf[:, :, :3].copy())         # (H, W, 3)\n",
    "#                 plt.close(fig)\n",
    "\n",
    "#         imageio.mimsave(gif_path, out_frames, fps=10)\n",
    "#         print(\"Saved GIF:\", gif_path)\n",
    "\n",
    "#         # show gif in output (works in notebook)\n",
    "#         display(Image(filename=gif_path))\n",
    "\n",
    "# print(\"\\nDONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64799079-3107-46cb-a117-288018de8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # BDH Phase-2 Probe (STRONGER NEG + Strong baseline + EXPLAINABLE GIF)\n",
    "# # Colab/Jupyter ready (matplotlib inline) + Matplotlib 3.9+ safe capture\n",
    "# # ============================================\n",
    "\n",
    "# import os, re, random, html\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import importlib.util\n",
    "\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # ---------- IMPORTANT for notebooks ----------\n",
    "# try:\n",
    "#     get_ipython  # noqa\n",
    "#     get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# plt.ioff()  # non-blocking\n",
    "\n",
    "# # ----------------------------\n",
    "# # 0) Repro / clean reset helpers\n",
    "# # ----------------------------\n",
    "# SEED = 1337\n",
    "\n",
    "# def seed_all(seed=SEED):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "\n",
    "# seed_all(SEED)\n",
    "\n",
    "# _trans = {}  # translation cache\n",
    "\n",
    "# def hard_reset_state(also_seed=True):\n",
    "#     global _trans\n",
    "#     _trans = {}\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#     if also_seed:\n",
    "#         seed_all(SEED)\n",
    "#     print(\"[reset] cleared translation cache + cuda cache (if any) + reseeded.\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 1) Load BDH module + checkpoint\n",
    "# # ----------------------------\n",
    "# BDH_SCRIPT_PATH = \"/content/bdh_europarl_train_probe.py\"   # change if needed\n",
    "# CKPT_PATH = \"checkpoints/bdh_europarl_bytes.pt\"           # change if needed\n",
    "\n",
    "# spec = importlib.util.spec_from_file_location(\"bdhmod\", BDH_SCRIPT_PATH)\n",
    "# bdhmod = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(bdhmod)\n",
    "\n",
    "# BDHConfig    = bdhmod.BDHConfig\n",
    "# BDH          = bdhmod.BDH\n",
    "# ids_for_text = bdhmod.ids_for_text\n",
    "# neuron_id    = bdhmod.neuron_id\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# state = torch.load(CKPT_PATH, map_location=device)\n",
    "# nh, D, N_tensor = state[\"encoder\"].shape\n",
    "# mult = int((N_tensor * nh) // D)\n",
    "\n",
    "# cfg = BDHConfig(\n",
    "#     vocab_size=256, n_layer=6, n_embd=D, n_head=nh,\n",
    "#     mlp_internal_dim_multiplier=mult, dropout=0.0\n",
    "# )\n",
    "# model = BDH(cfg).to(device)\n",
    "# model.load_state_dict(state, strict=True)\n",
    "# model.eval()\n",
    "\n",
    "# N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head  # per-head features\n",
    "# print(\"BDH loaded\")\n",
    "# print(\"Model config:\", {\"n_layer\": cfg.n_layer, \"n_embd\": cfg.n_embd, \"n_head\": cfg.n_head,\n",
    "#                       \"mult\": cfg.mlp_internal_dim_multiplier, \"N_per_head\": N})\n",
    "\n",
    "# # ----------------------------\n",
    "# # 2) Translation (cached)\n",
    "# # ----------------------------\n",
    "# ALL_LANGS = {\n",
    "#     \"German\":  \"Helsinki-NLP/opus-mt-en-de\",\n",
    "#     \"French\":  \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "#     \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "#     \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "# }\n",
    "\n",
    "# def translate(lang, text):\n",
    "#     global _trans\n",
    "#     if lang not in _trans:\n",
    "#         tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "#         mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "#         _trans[lang] = (tok, mdl)\n",
    "#     tok, mdl = _trans[lang]\n",
    "#     batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "#     out = mdl.generate(**batch, max_new_tokens=64)\n",
    "#     return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 3) Core helpers\n",
    "# # ----------------------------\n",
    "# EPS = 1e-9\n",
    "# LAYERS = [4, 5]\n",
    "# LAYERS = [L for L in LAYERS if 0 <= L < cfg.n_layer]\n",
    "\n",
    "# TOPK_PRINT = 5\n",
    "# TOPK_SET   = 200   # IMPORTANT: higher = more stable; 50 was too small\n",
    "# MAX_CAND   = 800\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_set_for_text(text: str, layer_idx: int, k: int):\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)  # (nh, N)\n",
    "#     flat = a.reshape(-1)\n",
    "#     k = min(k, flat.numel())\n",
    "#     _, idxs = torch.topk(flat, k)\n",
    "#     S = set()\n",
    "#     for ix in idxs.tolist():\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         S.add(neuron_id(layer_idx, head, feat, cfg.n_head, N))\n",
    "#     return S\n",
    "\n",
    "# def jaccard(a: set, b: set) -> float:\n",
    "#     return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "# def decode_gid(gid: int):\n",
    "#     per_layer = cfg.n_head * N\n",
    "#     layer = gid // per_layer\n",
    "#     rem = gid % per_layer\n",
    "#     head = rem // N\n",
    "#     feat = rem % N\n",
    "#     return layer, head, feat\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid_mean(text: str, layer_idx: int, gid: int) -> float:\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return 0.0\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0]  # (nh, T, N)\n",
    "#     return float(a[head, :, feat].mean().item())\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def activation_gid_over_positions(text: str, layer_idx: int, gid: int):\n",
    "#     layer, head, feat = decode_gid(gid)\n",
    "#     if layer != layer_idx:\n",
    "#         return np.array([0.0], dtype=np.float32)\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0]  # (nh, T, N)\n",
    "#     return a[head, :, feat].detach().float().cpu().numpy()\n",
    "\n",
    "# def read_list_until_end(prompt, min_n=1):\n",
    "#     print(prompt)\n",
    "#     out = []\n",
    "#     while True:\n",
    "#         s = input(\"> \").strip()\n",
    "#         if s.upper() == \"END\":\n",
    "#             break\n",
    "#         if s:\n",
    "#             out.append(s)\n",
    "#     if len(out) < min_n:\n",
    "#         raise ValueError(f\"Need at least {min_n} items.\")\n",
    "#     return out\n",
    "\n",
    "# def short(s, n=28):\n",
    "#     s = s.replace(\"\\n\", \" \")\n",
    "#     return s if len(s) <= n else s[:n-3] + \"...\"\n",
    "\n",
    "# # ----------------------------\n",
    "# # 0.5) Optional reset prompt\n",
    "# # ----------------------------\n",
    "# do_reset = input(\"Reset caches before running? (y/n)\\n> \").strip().lower() == \"y\"\n",
    "# if do_reset:\n",
    "#     hard_reset_state()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 4) DEMO MODE (Top-K neuron IDs)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== DEMO MODE ===\")\n",
    "# sentence = input(\"Enter an English word/sentence:\\n> \").strip()\n",
    "\n",
    "# print(\"\\nAvailable languages:\")\n",
    "# for k in ALL_LANGS:\n",
    "#     print(\" -\", k)\n",
    "\n",
    "# langs_raw = input(\"\\nChoose languages (comma-separated, blank=all):\\n> \").strip()\n",
    "# LANGS = [l.strip() for l in langs_raw.split(\",\") if l.strip() in ALL_LANGS]\n",
    "# if len(LANGS) == 0:\n",
    "#     LANGS = list(ALL_LANGS.keys())\n",
    "\n",
    "# translations = {lang: translate(lang, sentence) for lang in LANGS}\n",
    "\n",
    "# print(\"\\nTranslations:\")\n",
    "# print(\"EN:\", sentence)\n",
    "# for lang, txt in translations.items():\n",
    "#     print(f\"{lang[:2].upper()}: {txt}\")\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def topk_list_fulltext(text, layer_idx, k=TOPK_PRINT):\n",
    "#     x = ids_for_text(text).to(device)\n",
    "#     _, _, sparse = model(x, return_sparse=True)\n",
    "#     a = sparse[layer_idx][0].mean(dim=1)  # (nh,N)\n",
    "#     flat = a.reshape(-1)\n",
    "#     k = min(k, flat.numel())\n",
    "#     vals, idxs = torch.topk(flat, k)\n",
    "#     out = []\n",
    "#     for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "#         head = ix // N\n",
    "#         feat = ix % N\n",
    "#         gid = neuron_id(layer_idx, head, feat, cfg.n_head, N)\n",
    "#         out.append((gid, (layer_idx, head, feat), float(v)))\n",
    "#     return out\n",
    "\n",
    "# for L in LAYERS:\n",
    "#     print(f\"\\nTOP-{TOPK_PRINT} NEURONS — Layer {L}\")\n",
    "#     print(f\"{'TEXT':<6} {'NEURON_ID':<10} {'(layer,head,feat)':<18} {'ACT'}\")\n",
    "#     print(\"-\"*60)\n",
    "#     for gid, trip, act in topk_list_fulltext(sentence, L):\n",
    "#         print(f\"{'EN':<6} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "#     for lang, txt in translations.items():\n",
    "#         for gid, trip, act in topk_list_fulltext(txt, L):\n",
    "#             print(f\"{lang[:2].upper():<6} {gid:<10} {str(trip):<18} {act:.3f}\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # 5) DATASET MODE (concepts)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== DATASET MODE (monosemantic test) ===\")\n",
    "# concepts = read_list_until_end(\"Enter CONCEPT words (>=5). Type END to finish:\", min_n=5)\n",
    "\n",
    "# concept_trans = []\n",
    "# for w in concepts:\n",
    "#     trans = {\"EN\": w}\n",
    "#     for lang in LANGS:\n",
    "#         trans[lang] = translate(lang, w)\n",
    "#     concept_trans.append(trans)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 5.5) STRONG NEG SETS (3 types)\n",
    "# # ----------------------------\n",
    "# print(\"\\n=== NEG SETS (3 groups) ===\")\n",
    "# print(\"NEG-A: Unrelated words (>=5)\")\n",
    "# NEG_A = read_list_until_end(\"Enter NEG-A (unrelated). Type END:\", min_n=5)\n",
    "\n",
    "# print(\"\\nNEG-B: Spelling/byte-similar controls (>=5)  [important for byte models]\")\n",
    "# NEG_B = read_list_until_end(\"Enter NEG-B (similar spelling). Type END:\", min_n=5)\n",
    "\n",
    "# print(\"\\nNEG-C: Same-domain-but-different concept (>=5)  [hardest test]\")\n",
    "# NEG_C = read_list_until_end(\"Enter NEG-C (same domain). Type END:\", min_n=5)\n",
    "\n",
    "# NEG_ALL = NEG_A + NEG_B + NEG_C\n",
    "# print(\"\\nNEG summary:\")\n",
    "# print(\"NEG-A:\", NEG_A)\n",
    "# print(\"NEG-B:\", NEG_B)\n",
    "# print(\"NEG-C:\", NEG_C)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 6) Actual vs Baseline Jaccard (STRONG baseline with shuffles)\n",
    "# # ----------------------------\n",
    "# def compute_actual(layer_idx):\n",
    "#     names = [ct[\"EN\"] for ct in concept_trans]\n",
    "#     actual = []\n",
    "#     for trans in concept_trans:\n",
    "#         S_en = topk_set_for_text(trans[\"EN\"], layer_idx, TOPK_SET)\n",
    "#         js = []\n",
    "#         for lang in LANGS:\n",
    "#             S_tr = topk_set_for_text(trans[lang], layer_idx, TOPK_SET)\n",
    "#             js.append(jaccard(S_en, S_tr))\n",
    "#         actual.append(float(np.mean(js)))\n",
    "#     return names, np.array(actual)\n",
    "\n",
    "# def compute_shuffle_baseline(layer_idx, n_shuffles=20):\n",
    "#     # for each shuffle, permute translations among concepts\n",
    "#     n = len(concept_trans)\n",
    "#     base = []\n",
    "#     for _ in range(n_shuffles):\n",
    "#         perm = np.random.permutation(n)\n",
    "#         vals = []\n",
    "#         for i in range(n):\n",
    "#             S_en = topk_set_for_text(concept_trans[i][\"EN\"], layer_idx, TOPK_SET)\n",
    "#             js = []\n",
    "#             for lang in LANGS:\n",
    "#                 S_wrong = topk_set_for_text(concept_trans[perm[i]][lang], layer_idx, TOPK_SET)\n",
    "#                 js.append(jaccard(S_en, S_wrong))\n",
    "#             vals.append(float(np.mean(js)))\n",
    "#         base.append(np.mean(vals))\n",
    "#     return np.array(base)\n",
    "\n",
    "# results = {}\n",
    "# for L in LAYERS:\n",
    "#     names, actual_arr = compute_actual(L)\n",
    "#     base_dist = compute_shuffle_baseline(L, n_shuffles=20)\n",
    "\n",
    "#     results[L] = (names, actual_arr, base_dist)\n",
    "\n",
    "#     print(f\"\\nLayer {L}: actual_mean={actual_arr.mean():.3f}  baseline_mean={base_dist.mean():.3f}  baseline_std={base_dist.std():.3f}\")\n",
    "\n",
    "#     # Plot: actual per concept + baseline mean line\n",
    "#     x = np.arange(len(names))\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     plt.bar(x, actual_arr, label=\"Actual (EN vs its translations)\")\n",
    "#     plt.axhline(base_dist.mean(), linestyle=\"--\", label=\"Baseline mean (shuffled)\")\n",
    "#     plt.axhspan(base_dist.mean()-base_dist.std(), base_dist.mean()+base_dist.std(), alpha=0.2, label=\"Baseline ±1σ\")\n",
    "#     plt.xticks(x, names, rotation=45, ha=\"right\")\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.ylabel(\"Jaccard overlap\")\n",
    "#     plt.title(f\"Layer {L}: Actual vs Shuffle Baseline (TopK={TOPK_SET})\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 7) Best shared neuron per concept + stronger selectivity metrics\n",
    "# # ----------------------------\n",
    "# @torch.no_grad()\n",
    "# def best_shared_neuron_for_concept(i, layer_idx):\n",
    "#     trans = concept_trans[i]\n",
    "#     POS = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "\n",
    "#     # candidates: intersection of topK across POS + EN topK fallback\n",
    "#     inter = None\n",
    "#     for t in POS:\n",
    "#         S = topk_set_for_text(t, layer_idx, TOPK_SET)\n",
    "#         inter = S if inter is None else (inter & S)\n",
    "#     if inter is None:\n",
    "#         inter = set()\n",
    "\n",
    "#     S_en = topk_set_for_text(trans[\"EN\"], layer_idx, TOPK_SET)\n",
    "#     cand = list((set(inter) | set(S_en)))[:MAX_CAND]\n",
    "#     if not cand:\n",
    "#         return None\n",
    "\n",
    "#     best = None\n",
    "#     for gid in cand:\n",
    "#         pos_vals = np.array([activation_gid_mean(t, layer_idx, gid) for t in POS], dtype=np.float64)\n",
    "#         neg_vals = np.array([activation_gid_mean(t, layer_idx, gid) for t in NEG_ALL], dtype=np.float64)\n",
    "\n",
    "#         pos_mean = float(pos_vals.mean())\n",
    "#         neg_mean = float(neg_vals.mean())\n",
    "#         neg_std  = float(neg_vals.std() + 1e-6)\n",
    "\n",
    "#         sel = pos_mean - neg_mean\n",
    "#         z   = sel / neg_std\n",
    "#         sep = float(pos_vals.min() - neg_vals.max())  # > 0 is strong separation\n",
    "\n",
    "#         score = (z, sep, sel)  # prioritize high z, then separation, then sel\n",
    "#         if (best is None) or (score > best[\"score\"]):\n",
    "#             best = {\n",
    "#                 \"gid\": gid,\n",
    "#                 \"pos_vals\": pos_vals,\n",
    "#                 \"neg_vals\": neg_vals,\n",
    "#                 \"pos_mean\": pos_mean,\n",
    "#                 \"neg_mean\": neg_mean,\n",
    "#                 \"neg_std\": neg_std,\n",
    "#                 \"sel\": sel,\n",
    "#                 \"z\": z,\n",
    "#                 \"sep\": sep,\n",
    "#                 \"score\": score,\n",
    "#             }\n",
    "#     return best\n",
    "\n",
    "# TOP_SHOW = min(3, len(concept_trans))\n",
    "\n",
    "# for L in LAYERS:\n",
    "#     names, actual_arr, base_dist = results[L]\n",
    "#     margins = actual_arr - base_dist.mean()\n",
    "#     order = np.argsort(-margins)[:TOP_SHOW]\n",
    "\n",
    "#     print(f\"\\n=== Layer {L}: top {TOP_SHOW} concepts by (actual - baseline_mean) ===\")\n",
    "\n",
    "#     for idx in order:\n",
    "#         concept = concept_trans[idx][\"EN\"]\n",
    "#         best = best_shared_neuron_for_concept(idx, L)\n",
    "#         if best is None:\n",
    "#             print(f\"Concept '{concept}': no candidates.\")\n",
    "#             continue\n",
    "\n",
    "#         gid = best[\"gid\"]\n",
    "#         pos_vals = best[\"pos_vals\"]\n",
    "#         neg_vals = best[\"neg_vals\"]\n",
    "\n",
    "#         # PASS rules (you can tune)\n",
    "#         pass_sep = best[\"sep\"] > 0.0\n",
    "#         pass_z   = best[\"z\"] >= 2.0\n",
    "#         verdict = \"PASS \" if (pass_sep and pass_z) else \"WEAK \"\n",
    "\n",
    "#         print(f\"\\nConcept: {concept} | neuron={gid} decode={decode_gid(gid)} | {verdict}\")\n",
    "#         print(f\"  pos_mean={best['pos_mean']:.3f}  neg_mean={best['neg_mean']:.3f}  sel={best['sel']:.3f}\")\n",
    "#         print(f\"  neg_std={best['neg_std']:.3f}  z={best['z']:.2f}  sep(minPOS-maxNEG)={best['sep']:.3f}\")\n",
    "\n",
    "#         # Bar plot (POS vs NEG groups)\n",
    "#         POS_texts  = [concept_trans[idx][\"EN\"]] + [concept_trans[idx][lang] for lang in LANGS]\n",
    "#         POS_labels = [\"EN\"] + [lang[:2].upper() for lang in LANGS]\n",
    "\n",
    "#         NEG_labels = (\n",
    "#             [f\"A{i+1}\" for i in range(len(NEG_A))] +\n",
    "#             [f\"B{i+1}\" for i in range(len(NEG_B))] +\n",
    "#             [f\"C{i+1}\" for i in range(len(NEG_C))]\n",
    "#         )\n",
    "\n",
    "#         all_labels = POS_labels + NEG_labels\n",
    "#         all_vals = np.concatenate([pos_vals, neg_vals], axis=0)\n",
    "\n",
    "#         plt.figure(figsize=(14,4))\n",
    "#         plt.bar(np.arange(len(all_labels)), all_vals)\n",
    "#         plt.xticks(np.arange(len(all_labels)), all_labels, rotation=0)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(f\"Layer {L} neuron {gid} | concept='{concept}' | {verdict}\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         # Boxplot (POS vs NEG)\n",
    "#         plt.figure(figsize=(6,4))\n",
    "#         plt.boxplot([pos_vals, neg_vals], tick_labels=[\"POS\", \"NEG(all)\"], showmeans=True)\n",
    "#         plt.ylabel(\"Mean activation\")\n",
    "#         plt.title(f\"Layer {L} neuron {gid}: POS vs NEG\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# # ----------------------------\n",
    "# # 8) EXPLAINABLE GIF (EN + translation + 2 NEG controls) with BYTES shown\n",
    "# # ----------------------------\n",
    "# make_gif = input(\"\\nMake an EXPLAINABLE GIF? (y/n)\\n> \").strip().lower() == \"y\"\n",
    "# if make_gif:\n",
    "#     import imageio.v2 as imageio\n",
    "#     from IPython.display import HTML, display\n",
    "\n",
    "#     print(\"Available layers:\", LAYERS)\n",
    "#     L_in = input(f\"Which layer? (default {LAYERS[0]})\\n> \").strip()\n",
    "#     L = int(L_in) if L_in.isdigit() else LAYERS[0]\n",
    "#     if L not in LAYERS:\n",
    "#         L = LAYERS[0]\n",
    "\n",
    "#     concept_names = [ct[\"EN\"] for ct in concept_trans]\n",
    "#     print(\"\\nConcepts:\", concept_names)\n",
    "#     chosen = input(\"Which EN concept for GIF? (type exactly)\\n> \").strip().lower()\n",
    "#     idx_map = {ct[\"EN\"].lower(): i for i, ct in enumerate(concept_trans)}\n",
    "#     if chosen not in idx_map:\n",
    "#         print(\"Not matched; using first concept:\", concept_names[0])\n",
    "#         best_idx = 0\n",
    "#     else:\n",
    "#         best_idx = idx_map[chosen]\n",
    "\n",
    "#     print(\"Chosen LANGS:\", LANGS)\n",
    "#     other_lang = input(f\"Which translation language for GIF? (default {LANGS[0]})\\n> \").strip()\n",
    "#     if other_lang not in LANGS:\n",
    "#         other_lang = LANGS[0]\n",
    "\n",
    "#     best = best_shared_neuron_for_concept(best_idx, L)\n",
    "#     if best is None:\n",
    "#         print(\"No neuron found.\")\n",
    "#     else:\n",
    "#         gid = best[\"gid\"]\n",
    "#         concept = concept_trans[best_idx][\"EN\"]\n",
    "#         en_text = concept_trans[best_idx][\"EN\"]\n",
    "#         tr_text = concept_trans[best_idx][other_lang]\n",
    "\n",
    "#         # choose 2 NEG controls: 1 spelling-control + 1 unrelated\n",
    "#         neg1 = NEG_B[0] if len(NEG_B) else NEG_ALL[0]\n",
    "#         neg2 = NEG_A[0] if len(NEG_A) else (NEG_ALL[1] if len(NEG_ALL) > 1 else NEG_ALL[0])\n",
    "\n",
    "#         seqs = [\n",
    "#             (\"EN\", en_text),\n",
    "#             (other_lang[:2].upper(), tr_text),\n",
    "#             (\"NEG-B\", neg1),\n",
    "#             (\"NEG-A\", neg2),\n",
    "#         ]\n",
    "\n",
    "#         print(\"\\n[gif] Will animate:\")\n",
    "#         print(f\"  concept: {concept}\")\n",
    "#         print(f\"  layer:   {L}\")\n",
    "#         print(f\"  neuron:  {gid} decode={decode_gid(gid)}\")\n",
    "#         for tag, t in seqs:\n",
    "#             print(f\"   - {tag}: {t}\")\n",
    "\n",
    "#         gif_path = \"bdh_activation_explain.gif\"\n",
    "#         out_frames = []\n",
    "\n",
    "#         def to_bytes_preview(text, max_len=48):\n",
    "#             b = text.encode(\"utf-8\", errors=\"replace\")\n",
    "#             # show bytes as printable-ish: hex\n",
    "#             hexs = [f\"{x:02x}\" for x in b[:max_len]]\n",
    "#             if len(b) > max_len:\n",
    "#                 hexs.append(\"..\")\n",
    "#             return b, \" \".join(hexs)\n",
    "\n",
    "#         def fig_to_rgb(fig):\n",
    "#             fig.canvas.draw()\n",
    "#             buf = np.asarray(fig.canvas.buffer_rgba())  # (H,W,4)\n",
    "#             return buf[:, :, :3].copy()\n",
    "\n",
    "#         # build per-seq curves first so we can share y-scale\n",
    "#         curves = []\n",
    "#         byte_views = []\n",
    "#         for tag, text in seqs:\n",
    "#             vals = activation_gid_over_positions(text, L, gid)\n",
    "#             curves.append(vals)\n",
    "#             b, hx = to_bytes_preview(text, max_len=48)\n",
    "#             byte_views.append((b, hx))\n",
    "\n",
    "#         global_max = max([float(v.max()) for v in curves]) if curves else 1.0\n",
    "#         y_max = max(global_max * 1.15, 1e-3)\n",
    "\n",
    "#         # GIF parameters\n",
    "#         fps = 12\n",
    "#         max_frames = 140  # keep manageable\n",
    "#         step = 1\n",
    "\n",
    "#         for (tag, text), vals, (b, hx) in zip(seqs, curves, byte_views):\n",
    "#             T = len(vals)\n",
    "#             if T <= 2:\n",
    "#                 continue\n",
    "\n",
    "#             # frame sampling so long texts still animate\n",
    "#             if T > max_frames:\n",
    "#                 step = int(np.ceil(T / max_frames))\n",
    "#             else:\n",
    "#                 step = 1\n",
    "\n",
    "#             for t in range(2, T, step):\n",
    "#                 fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "#                 # activation plot\n",
    "#                 ax1 = plt.subplot(2, 1, 1)\n",
    "#                 ax1.plot(np.arange(T), vals, alpha=0.25, linewidth=1)  # faint full curve\n",
    "#                 ax1.plot(np.arange(t), vals[:t], linewidth=2)          # growing curve\n",
    "#                 ax1.axvline(t-1, linestyle=\"--\", linewidth=1)\n",
    "#                 ax1.set_xlim(0, T-1)\n",
    "#                 ax1.set_ylim(0, y_max)\n",
    "#                 ax1.set_ylabel(\"Activation\")\n",
    "#                 ax1.set_title(f\"Layer {L} neuron {gid} | {tag}: '{short(text, 40)}' | byte index {t-1}/{T-1}\")\n",
    "\n",
    "#                 # bytes display (hex + highlight current byte)\n",
    "#                 ax2 = plt.subplot(2, 1, 2)\n",
    "#                 ax2.axis(\"off\")\n",
    "#                 cur = min(t-1, len(b)-1)\n",
    "#                 # show a small window around cur byte\n",
    "#                 left = max(0, cur-16)\n",
    "#                 right = min(len(b), cur+16)\n",
    "#                 window = b[left:right]\n",
    "#                 window_hex = [f\"{x:02x}\" for x in window]\n",
    "\n",
    "#                 # highlight current byte in the window using brackets\n",
    "#                 hi = cur - left\n",
    "#                 if 0 <= hi < len(window_hex):\n",
    "#                     window_hex[hi] = \"[\" + window_hex[hi] + \"]\"\n",
    "\n",
    "#                 ax2.text(\n",
    "#                     0.01, 0.55,\n",
    "#                     f\"bytes(hex) around current:\\n\" + \" \".join(window_hex),\n",
    "#                     fontsize=10, family=\"monospace\"\n",
    "#                 )\n",
    "#                 ax2.text(\n",
    "#                     0.01, 0.05,\n",
    "#                     f\"full bytes preview:\\n{hx}\",\n",
    "#                     fontsize=9, family=\"monospace\", alpha=0.85\n",
    "#                 )\n",
    "\n",
    "#                 plt.tight_layout()\n",
    "#                 out_frames.append(fig_to_rgb(fig))\n",
    "#                 plt.close(fig)\n",
    "\n",
    "#         imageio.mimsave(gif_path, out_frames, fps=fps)\n",
    "#         print(\"Saved GIF:\", gif_path)\n",
    "\n",
    "#         # More reliable looping display than IPython.display.Image in some setups:\n",
    "#         # embed as HTML <img> so browser loops it smoothly\n",
    "#         with open(gif_path, \"rb\") as f:\n",
    "#             data = f.read()\n",
    "#         import base64\n",
    "#         b64 = base64.b64encode(data).decode(\"utf-8\")\n",
    "#         display(HTML(f\"<img src='data:image/gif;base64,{b64}' loop='infinite' />\"))\n",
    "\n",
    "# print(\"\\nDONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328a2dc-e6cb-41b4-bbeb-d44550f14eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "# ============================================\n",
    "# Streamlit BDH Phase-2 Probe UI\n",
    "# ============================================\n",
    "\n",
    "import os, re, random, base64\n",
    "import numpy as np\n",
    "import torch\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib.util\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# ----------------------------\n",
    "# App Config\n",
    "# ----------------------------\n",
    "st.set_page_config(page_title=\"BDH Phase-2 Probe\", layout=\"wide\")\n",
    "\n",
    "# ----------------------------\n",
    "# Constants / Defaults\n",
    "# ----------------------------\n",
    "SEED = 1337\n",
    "\n",
    "ALL_LANGS = {\n",
    "    \"German\":  \"Helsinki-NLP/opus-mt-en-de\",\n",
    "    \"French\":  \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "    \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "}\n",
    "\n",
    "DEFAULT_BDH_SCRIPT_PATH = \"/content/bdh_europarl_train_probe.py\"\n",
    "DEFAULT_CKPT_PATH = \"checkpoints/bdh_europarl_bytes.pt\"\n",
    "\n",
    "# ----------------------------\n",
    "# Utility\n",
    "# ----------------------------\n",
    "def short(s, n=28):\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    return s if len(s) <= n else s[:n-3] + \"...\"\n",
    "\n",
    "def seed_all(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def split_lines(s: str):\n",
    "    return [x.strip() for x in s.splitlines() if x.strip()]\n",
    "\n",
    "# ----------------------------\n",
    "# Session \"Memory\"\n",
    "# ----------------------------\n",
    "if \"trans_cache\" not in st.session_state:\n",
    "    st.session_state.trans_cache = {}\n",
    "if \"seed\" not in st.session_state:\n",
    "    st.session_state.seed = SEED\n",
    "\n",
    "def hard_reset_state(also_seed=True, clear_gpu=True):\n",
    "    st.session_state.trans_cache = {}\n",
    "    if clear_gpu and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    if also_seed:\n",
    "        seed_all(st.session_state.seed)\n",
    "\n",
    "# ----------------------------\n",
    "# Cached loaders (fast)\n",
    "# ----------------------------\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_bdh_module(bdh_script_path: str):\n",
    "    spec = importlib.util.spec_from_file_location(\"bdhmod\", bdh_script_path)\n",
    "    bdhmod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(bdhmod)\n",
    "    return bdhmod\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_bdh_model(bdh_script_path: str, ckpt_path: str):\n",
    "    bdhmod = load_bdh_module(bdh_script_path)\n",
    "\n",
    "    BDHConfig    = bdhmod.BDHConfig\n",
    "    BDH          = bdhmod.BDH\n",
    "    ids_for_text = bdhmod.ids_for_text\n",
    "    neuron_id    = bdhmod.neuron_id\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    nh, D, N_tensor = state[\"encoder\"].shape\n",
    "    mult = int((N_tensor * nh) // D)\n",
    "\n",
    "    cfg = BDHConfig(\n",
    "        vocab_size=256, n_layer=6, n_embd=D, n_head=nh,\n",
    "        mlp_internal_dim_multiplier=mult, dropout=0.0\n",
    "    )\n",
    "    model = BDH(cfg).to(device)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head\n",
    "    return dict(\n",
    "        bdhmod=bdhmod, cfg=cfg, model=model, device=device, N=N,\n",
    "        BDHConfig=BDHConfig, BDH=BDH, ids_for_text=ids_for_text, neuron_id=neuron_id\n",
    "    )\n",
    "\n",
    "def translate(lang, text, device):\n",
    "    cache = st.session_state.trans_cache\n",
    "    if lang not in cache:\n",
    "        tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "        mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "        cache[lang] = (tok, mdl)\n",
    "    tok, mdl = cache[lang]\n",
    "    batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    out = mdl.generate(**batch, max_new_tokens=64)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# ----------------------------\n",
    "# BDH math helpers\n",
    "# ----------------------------\n",
    "EPS = 1e-9\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "def decode_gid(gid: int, cfg, N):\n",
    "    per_layer = cfg.n_head * N\n",
    "    layer = gid // per_layer\n",
    "    rem = gid % per_layer\n",
    "    head = rem // N\n",
    "    feat = rem % N\n",
    "    return layer, head, feat\n",
    "\n",
    "@torch.no_grad()\n",
    "def topk_set_for_text(text: str, layer_idx: int, k: int, ids_for_text, model, device, cfg, N, neuron_id):\n",
    "    x = ids_for_text(text).to(device)\n",
    "    _, _, sparse = model(x, return_sparse=True)\n",
    "    a = sparse[layer_idx][0].mean(dim=1)  # (nh, N)\n",
    "    flat = a.reshape(-1)\n",
    "    k = min(k, flat.numel())\n",
    "    _, idxs = torch.topk(flat, k)\n",
    "    S = set()\n",
    "    for ix in idxs.tolist():\n",
    "        head = ix // N\n",
    "        feat = ix % N\n",
    "        S.add(neuron_id(layer_idx, head, feat, cfg.n_head, N))\n",
    "    return S\n",
    "\n",
    "@torch.no_grad()\n",
    "def activation_gid_mean(text: str, layer_idx: int, gid: int, ids_for_text, model, device, cfg, N):\n",
    "    layer, head, feat = decode_gid(gid, cfg, N)\n",
    "    if layer != layer_idx:\n",
    "        return 0.0\n",
    "    x = ids_for_text(text).to(device)\n",
    "    _, _, sparse = model(x, return_sparse=True)\n",
    "    a = sparse[layer_idx][0]  # (nh, T, N)\n",
    "    return float(a[head, :, feat].mean().item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def activation_gid_over_positions(text: str, layer_idx: int, gid: int, ids_for_text, model, device, cfg, N):\n",
    "    layer, head, feat = decode_gid(gid, cfg, N)\n",
    "    if layer != layer_idx:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    x = ids_for_text(text).to(device)\n",
    "    _, _, sparse = model(x, return_sparse=True)\n",
    "    a = sparse[layer_idx][0]\n",
    "    return a[head, :, feat].detach().float().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def topk_list_fulltext(text, layer_idx, k, ids_for_text, model, device, cfg, N, neuron_id):\n",
    "    x = ids_for_text(text).to(device)\n",
    "    _, _, sparse = model(x, return_sparse=True)\n",
    "    a = sparse[layer_idx][0].mean(dim=1)  # (nh,N)\n",
    "    flat = a.reshape(-1)\n",
    "    k = min(k, flat.numel())\n",
    "    vals, idxs = torch.topk(flat, k)\n",
    "    out = []\n",
    "    for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "        head = ix // N\n",
    "        feat = ix % N\n",
    "        gid = neuron_id(layer_idx, head, feat, cfg.n_head, N)\n",
    "        out.append((gid, (layer_idx, head, feat), float(v)))\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def best_shared_neuron_for_concept(concept_trans, idx, layer_idx, LANGS, TOPK_SET, MAX_CAND,\n",
    "                                  NEG_ALL, ids_for_text, model, device, cfg, N, neuron_id):\n",
    "    trans = concept_trans[idx]\n",
    "    POS = [trans[\"EN\"]] + [trans[lang] for lang in LANGS]\n",
    "\n",
    "    # candidates: inter(POS topK) + EN topK\n",
    "    inter = None\n",
    "    for t in POS:\n",
    "        S = topk_set_for_text(t, layer_idx, TOPK_SET, ids_for_text, model, device, cfg, N, neuron_id)\n",
    "        inter = S if inter is None else (inter & S)\n",
    "    if inter is None:\n",
    "        inter = set()\n",
    "\n",
    "    S_en = topk_set_for_text(trans[\"EN\"], layer_idx, TOPK_SET, ids_for_text, model, device, cfg, N, neuron_id)\n",
    "    cand = list((set(inter) | set(S_en)))[:MAX_CAND]\n",
    "    if not cand:\n",
    "        return None\n",
    "\n",
    "    best = None\n",
    "    for gid in cand:\n",
    "        pos_vals = np.array([activation_gid_mean(t, layer_idx, gid, ids_for_text, model, device, cfg, N) for t in POS], dtype=np.float64)\n",
    "        neg_vals = np.array([activation_gid_mean(t, layer_idx, gid, ids_for_text, model, device, cfg, N) for t in NEG_ALL], dtype=np.float64)\n",
    "\n",
    "        pos_mean = float(pos_vals.mean())\n",
    "        neg_mean = float(neg_vals.mean())\n",
    "        neg_std  = float(neg_vals.std() + 1e-6)\n",
    "\n",
    "        sel = pos_mean - neg_mean\n",
    "        z   = sel / neg_std\n",
    "        sep = float(pos_vals.min() - neg_vals.max())  # > 0 strong\n",
    "\n",
    "        score = (z, sep, sel)\n",
    "        if (best is None) or (score > best[\"score\"]):\n",
    "            best = dict(gid=gid, pos_vals=pos_vals, neg_vals=neg_vals,\n",
    "                        pos_mean=pos_mean, neg_mean=neg_mean, neg_std=neg_std,\n",
    "                        sel=sel, z=z, sep=sep, score=score)\n",
    "    return best\n",
    "\n",
    "# ----------------------------\n",
    "# UI\n",
    "# ----------------------------\n",
    "st.title(\"BDH Phase-2 Probe — Streamlit UI\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Paths\")\n",
    "    bdh_script_path = st.text_input(\"BDH script path\", DEFAULT_BDH_SCRIPT_PATH)\n",
    "    ckpt_path = st.text_input(\"Checkpoint path\", DEFAULT_CKPT_PATH)\n",
    "\n",
    "    st.header(\"Run settings\")\n",
    "    st.session_state.seed = st.number_input(\"Seed\", value=SEED, step=1)\n",
    "    seed_all(st.session_state.seed)\n",
    "\n",
    "    TOPK_PRINT = st.slider(\"Top-K print (demo)\", 1, 20, 5)\n",
    "    TOPK_SET = st.slider(\"Top-K set (probe stability)\", 50, 500, 200, step=50)\n",
    "    MAX_CAND = st.slider(\"Max candidates\", 100, 2000, 800, step=100)\n",
    "\n",
    "    st.header(\"Reset\")\n",
    "    colR1, colR2 = st.columns(2)\n",
    "    if colR1.button(\"Reset cache\"):\n",
    "        hard_reset_state(also_seed=False, clear_gpu=False)\n",
    "        st.success(\"Cleared translation cache.\")\n",
    "    if colR2.button(\"Reset + reseed\"):\n",
    "        hard_reset_state(also_seed=True, clear_gpu=True)\n",
    "        st.success(\"Cleared cache + reseeded (+ cleared GPU cache if available).\")\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    pack = load_bdh_model(bdh_script_path, ckpt_path)\n",
    "except Exception as e:\n",
    "    st.error(f\"Could not load BDH model/module. Check paths.\\n\\n{e}\")\n",
    "    st.stop()\n",
    "\n",
    "bdhmod = pack[\"bdhmod\"]\n",
    "cfg = pack[\"cfg\"]\n",
    "model = pack[\"model\"]\n",
    "device = pack[\"device\"]\n",
    "N = pack[\"N\"]\n",
    "ids_for_text = pack[\"ids_for_text\"]\n",
    "neuron_id = pack[\"neuron_id\"]\n",
    "\n",
    "st.caption(f\"Device: {device} | Layers: {cfg.n_layer} | Embd: {cfg.n_embd} | Heads: {cfg.n_head} | N/head: {N}\")\n",
    "\n",
    "tabs = st.tabs([\"Demo\", \"Dataset probe\", \"GIF builder\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Demo tab\n",
    "# ----------------------------\n",
    "with tabs[0]:\n",
    "    st.subheader(\"Demo: Top neurons for a text and its translations\")\n",
    "\n",
    "    sentence = st.text_input(\"English word/sentence\", value=\"lion\")\n",
    "    chosen_langs = st.multiselect(\"Languages\", options=list(ALL_LANGS.keys()), default=[\"German\"])\n",
    "\n",
    "    if st.button(\"Run demo\"):\n",
    "        if not chosen_langs:\n",
    "            chosen_langs = list(ALL_LANGS.keys())\n",
    "\n",
    "        translations = {}\n",
    "        with st.spinner(\"Translating...\"):\n",
    "            for lang in chosen_langs:\n",
    "                translations[lang] = translate(lang, sentence, device)\n",
    "\n",
    "        st.write(\"**Translations**\")\n",
    "        st.write(\"EN:\", sentence)\n",
    "        for lang, txt in translations.items():\n",
    "            st.write(f\"{lang[:2].upper()}: {txt}\")\n",
    "\n",
    "        layers = [4, 5]\n",
    "        layers = [L for L in layers if 0 <= L < cfg.n_layer]\n",
    "\n",
    "        for L in layers:\n",
    "            st.markdown(f\"### Layer {L}\")\n",
    "            rows = []\n",
    "            for gid, trip, act in topk_list_fulltext(sentence, L, TOPK_PRINT, ids_for_text, model, device, cfg, N, neuron_id):\n",
    "                rows.append((\"EN\", gid, trip, act))\n",
    "            for lang, txt in translations.items():\n",
    "                for gid, trip, act in topk_list_fulltext(txt, L, TOPK_PRINT, ids_for_text, model, device, cfg, N, neuron_id):\n",
    "                    rows.append((lang[:2].upper(), gid, trip, act))\n",
    "\n",
    "            st.dataframe(\n",
    "                [{\"TEXT\": r[0], \"NEURON_ID\": r[1], \"(layer,head,feat)\": str(r[2]), \"ACT\": round(r[3], 4)} for r in rows],\n",
    "                use_container_width=True\n",
    "            )\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Probe tab\n",
    "# ----------------------------\n",
    "with tabs[1]:\n",
    "    st.subheader(\"Dataset probe: Actual vs shuffle baseline + best neuron proof\")\n",
    "\n",
    "    st.markdown(\"Enter one word per line.\")\n",
    "    concept_text = st.text_area(\"CONCEPT words (>=5)\", value=\"dog\\nlion\\nplant\\nrain\\nfire\", height=130)\n",
    "\n",
    "    chosen_langs = st.multiselect(\"Languages (probe)\", options=list(ALL_LANGS.keys()), default=[\"German\"])\n",
    "\n",
    "    st.markdown(\"### NEG sets (fresh each run)\")\n",
    "    neg_a_text = st.text_area(\"NEG-A (unrelated, >=5)\", value=\"laptop\\npune\\ndance\\nmountain\\nriver\", height=120)\n",
    "    neg_b_text = st.text_area(\"NEG-B (spelling/byte-similar, >=5)\", value=\"li0n\\nl1on\\nlioness\\nlions\\nlioning\", height=120)\n",
    "    neg_c_text = st.text_area(\"NEG-C (same domain but different, >=5)\", value=\"tiger\\ncheetah\\nleopard\\npanther\\nhyena\", height=120)\n",
    "\n",
    "    n_shuffles = st.slider(\"Baseline shuffles\", 5, 60, 20, step=5)\n",
    "    top_show = st.slider(\"Show top concepts\", 1, 6, 3)\n",
    "\n",
    "    if st.button(\"Run dataset probe\"):\n",
    "        concepts = split_lines(concept_text)\n",
    "        NEG_A = split_lines(neg_a_text)\n",
    "        NEG_B = split_lines(neg_b_text)\n",
    "        NEG_C = split_lines(neg_c_text)\n",
    "\n",
    "        if len(concepts) < 5:\n",
    "            st.error(\"Need at least 5 concept words.\")\n",
    "            st.stop()\n",
    "        if len(NEG_A) < 5 or len(NEG_B) < 5 or len(NEG_C) < 5:\n",
    "            st.error(\"Each NEG group must have at least 5 words.\")\n",
    "            st.stop()\n",
    "\n",
    "        if not chosen_langs:\n",
    "            chosen_langs = list(ALL_LANGS.keys())\n",
    "        LANGS = chosen_langs\n",
    "\n",
    "        with st.spinner(\"Translating concepts...\"):\n",
    "            concept_trans = []\n",
    "            for w in concepts:\n",
    "                trans = {\"EN\": w}\n",
    "                for lang in LANGS:\n",
    "                    trans[lang] = translate(lang, w, device)\n",
    "                concept_trans.append(trans)\n",
    "\n",
    "        NEG_ALL = NEG_A + NEG_B + NEG_C\n",
    "\n",
    "        def compute_actual(layer_idx):\n",
    "            names = [ct[\"EN\"] for ct in concept_trans]\n",
    "            actual = []\n",
    "            for trans in concept_trans:\n",
    "                S_en = topk_set_for_text(trans[\"EN\"], layer_idx, TOPK_SET, ids_for_text, model, device, cfg, N, neuron_id)\n",
    "                js = []\n",
    "                for lang in LANGS:\n",
    "                    S_tr = topk_set_for_text(trans[lang], layer_idx, TOPK_SET, ids_for_text, model, device, cfg, N, neuron_id)\n",
    "                    js.append(jaccard(S_en, S_tr))\n",
    "                actual.append(float(np.mean(js)))\n",
    "            return names, np.array(actual)\n",
    "\n",
    "        def compute_shuffle_baseline(layer_idx, n_shuffles=20):\n",
    "            n = len(concept_trans)\n",
    "            base = []\n",
    "            for _ in range(n_shuffles):\n",
    "                perm = np.random.permutation(n)\n",
    "                vals = []\n",
    "                for i in range(n):\n",
    "                    S_en = topk_set_for_text(concept_trans[i][\"EN\"], layer_idx, TOPK_SET, ids_for_text, model, device, cfg, N, neuron_id)\n",
    "                    js = []\n",
    "                    for lang in LANGS:\n",
    "                        S_wrong = topk_set_for_text(concept_trans[perm[i]][lang], layer_idx, TOPK_SET, ids_for_text, model, device, cfg, N, neuron_id)\n",
    "                        js.append(jaccard(S_en, S_wrong))\n",
    "                    vals.append(float(np.mean(js)))\n",
    "                base.append(np.mean(vals))\n",
    "            return np.array(base)\n",
    "\n",
    "        layers = [4, 5]\n",
    "        layers = [L for L in layers if 0 <= L < cfg.n_layer]\n",
    "\n",
    "        for L in layers:\n",
    "            names, actual_arr = compute_actual(L)\n",
    "            base_dist = compute_shuffle_baseline(L, n_shuffles=n_shuffles)\n",
    "\n",
    "            st.markdown(f\"## Layer {L}\")\n",
    "            st.write(\n",
    "                f\"actual_mean={actual_arr.mean():.3f} | baseline_mean={base_dist.mean():.3f} | baseline_std={base_dist.std():.3f}\"\n",
    "            )\n",
    "\n",
    "            # Plot\n",
    "            fig = plt.figure(figsize=(10, 3.2))\n",
    "            x = np.arange(len(names))\n",
    "            plt.bar(x, actual_arr, label=\"Actual (EN vs translations)\")\n",
    "            plt.axhline(base_dist.mean(), linestyle=\"--\", label=\"Baseline mean (shuffled)\")\n",
    "            plt.axhspan(base_dist.mean()-base_dist.std(), base_dist.mean()+base_dist.std(), alpha=0.2, label=\"Baseline ±1σ\")\n",
    "            plt.xticks(x, names, rotation=45, ha=\"right\")\n",
    "            plt.ylim(0, 1)\n",
    "            plt.ylabel(\"Jaccard overlap\")\n",
    "            plt.title(f\"Layer {L}: Actual vs Shuffle Baseline (TopK={TOPK_SET})\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            st.pyplot(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "            # Best neuron proof for top margins\n",
    "            margins = actual_arr - base_dist.mean()\n",
    "            order = np.argsort(-margins)[:min(top_show, len(names))]\n",
    "\n",
    "            for idx in order:\n",
    "                concept = concept_trans[idx][\"EN\"]\n",
    "                best = best_shared_neuron_for_concept(\n",
    "                    concept_trans, idx, L, LANGS, TOPK_SET, MAX_CAND, NEG_ALL,\n",
    "                    ids_for_text, model, device, cfg, N, neuron_id\n",
    "                )\n",
    "                if best is None:\n",
    "                    st.write(f\"Concept '{concept}': no candidates.\")\n",
    "                    continue\n",
    "\n",
    "                gid = best[\"gid\"]\n",
    "                verdict = \"PASS\" if (best[\"sep\"] > 0.0 and best[\"z\"] >= 2.0) else \"WEAK\"\n",
    "\n",
    "                st.markdown(f\"### Concept: `{concept}` | neuron `{gid}` decode={decode_gid(gid, cfg, N)} | **{verdict}**\")\n",
    "                st.code(\n",
    "                    f\"pos_mean={best['pos_mean']:.3f}  neg_mean={best['neg_mean']:.3f}  sel={best['sel']:.3f}\\n\"\n",
    "                    f\"neg_std={best['neg_std']:.3f}  z={best['z']:.2f}  sep(minPOS-maxNEG)={best['sep']:.3f}\"\n",
    "                )\n",
    "\n",
    "                POS_labels = [\"EN\"] + [lang[:2].upper() for lang in LANGS]\n",
    "                NEG_labels = (\n",
    "                    [f\"NEG-A:{w}\" for w in NEG_A] +\n",
    "                    [f\"NEG-B:{w}\" for w in NEG_B] +\n",
    "                    [f\"NEG-C:{w}\" for w in NEG_C]\n",
    "                )\n",
    "                all_labels = POS_labels + NEG_labels\n",
    "                all_vals = np.concatenate([best[\"pos_vals\"], best[\"neg_vals\"]], axis=0)\n",
    "\n",
    "                fig2 = plt.figure(figsize=(12, 3.2))\n",
    "                plt.bar(np.arange(len(all_labels)), all_vals)\n",
    "                plt.xticks(np.arange(len(all_labels)), [short(x, 18) for x in all_labels], rotation=45, ha=\"right\")\n",
    "                plt.ylabel(\"Mean activation\")\n",
    "                plt.title(f\"Layer {L} neuron {gid} | concept='{concept}' | {verdict}\")\n",
    "                plt.tight_layout()\n",
    "                st.pyplot(fig2)\n",
    "                plt.close(fig2)\n",
    "\n",
    "# ----------------------------\n",
    "# GIF Builder tab\n",
    "# ----------------------------\n",
    "with tabs[2]:\n",
    "    st.subheader(\"GIF builder (explainable): EN + translation + 2 NEG controls\")\n",
    "\n",
    "    st.markdown(\"This makes a GIF that animates activation over byte positions and shows the current byte window in hex.\")\n",
    "\n",
    "    layers = [4, 5]\n",
    "    layers = [L for L in layers if 0 <= L < cfg.n_layer]\n",
    "    L = st.selectbox(\"Layer\", layers, index=0)\n",
    "\n",
    "    concept_text = st.text_area(\"Concepts (one per line, used for selection)\", value=\"dog\\nlion\\nplant\\nrain\\nfire\", height=120)\n",
    "    concepts = split_lines(concept_text)\n",
    "    if not concepts:\n",
    "        concepts = [\"lion\"]\n",
    "\n",
    "    concept_choice = st.selectbox(\"Choose EN concept\", concepts, index=0)\n",
    "\n",
    "    chosen_lang = st.selectbox(\"Translation language\", list(ALL_LANGS.keys()), index=0)\n",
    "\n",
    "    # NEG selectors (user-controlled)\n",
    "    neg_a = st.text_input(\"NEG-A word (unrelated)\", value=\"laptop\")\n",
    "    neg_b = st.text_input(\"NEG-B word (spelling/byte control)\", value=\"li0n\")\n",
    "\n",
    "    TOPK_SET_gif = st.slider(\"TopK for selecting best neuron (GIF)\", 50, 500, 200, step=50)\n",
    "    MAX_FRAMES = st.slider(\"Max frames per sequence\", 60, 250, 140, step=10)\n",
    "    FPS = st.slider(\"FPS\", 5, 20, 12)\n",
    "\n",
    "    if st.button(\"Make GIF\"):\n",
    "        # Build minimal concept_trans (translate only what we need)\n",
    "        LANGS = [chosen_lang]\n",
    "        concept_trans = []\n",
    "        for w in concepts:\n",
    "            trans = {\"EN\": w, chosen_lang: translate(chosen_lang, w, device)}\n",
    "            concept_trans.append(trans)\n",
    "\n",
    "        idx_map = {ct[\"EN\"].lower(): i for i, ct in enumerate(concept_trans)}\n",
    "        idx = idx_map.get(concept_choice.lower(), 0)\n",
    "\n",
    "        NEG_ALL = [neg_a, neg_b]  # only for scoring selection in GIF builder\n",
    "\n",
    "        best = best_shared_neuron_for_concept(\n",
    "            concept_trans, idx, L, LANGS, TOPK_SET_gif, 800, NEG_ALL,\n",
    "            ids_for_text, model, device, cfg, N, neuron_id\n",
    "        )\n",
    "\n",
    "        if best is None:\n",
    "            st.error(\"No neuron found for this concept/layer.\")\n",
    "            st.stop()\n",
    "\n",
    "        gid = best[\"gid\"]\n",
    "        en_text = concept_trans[idx][\"EN\"]\n",
    "        tr_text = concept_trans[idx][chosen_lang]\n",
    "\n",
    "        seqs = [\n",
    "            (\"EN\", en_text),\n",
    "            (chosen_lang[:2].upper(), tr_text),\n",
    "            (\"NEG-A\", neg_a),\n",
    "            (\"NEG-B\", neg_b),\n",
    "        ]\n",
    "\n",
    "        st.write(\"Neuron:\", gid, \"decode:\", decode_gid(gid, cfg, N))\n",
    "        st.write(\"Animating:\", seqs)\n",
    "\n",
    "        curves = [activation_gid_over_positions(text, L, gid, ids_for_text, model, device, cfg, N) for _, text in seqs]\n",
    "        global_max = max(float(v.max()) for v in curves) if curves else 1.0\n",
    "        y_max = max(global_max * 1.15, 1e-3)\n",
    "\n",
    "        out_frames = []\n",
    "\n",
    "        def fig_to_rgb(fig):\n",
    "            fig.canvas.draw()\n",
    "            buf = np.asarray(fig.canvas.buffer_rgba())\n",
    "            return buf[:, :, :3].copy()\n",
    "\n",
    "        for (tag, text), vals in zip(seqs, curves):\n",
    "            T = len(vals)\n",
    "            if T <= 2:\n",
    "                continue\n",
    "            step = int(np.ceil(T / MAX_FRAMES)) if T > MAX_FRAMES else 1\n",
    "\n",
    "            b = text.encode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "            for t in range(2, T, step):\n",
    "                fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "                ax1 = plt.subplot(2, 1, 1)\n",
    "                ax1.plot(np.arange(T), vals, alpha=0.25, linewidth=1)\n",
    "                ax1.plot(np.arange(t), vals[:t], linewidth=2)\n",
    "                ax1.axvline(t-1, linestyle=\"--\", linewidth=1)\n",
    "                ax1.set_xlim(0, T-1)\n",
    "                ax1.set_ylim(0, y_max)\n",
    "                ax1.set_ylabel(\"Activation\")\n",
    "                ax1.set_title(f\"Layer {L} neuron {gid} | {tag}: '{short(text, 40)}' | byte index {t-1}/{T-1}\")\n",
    "\n",
    "                ax2 = plt.subplot(2, 1, 2)\n",
    "                ax2.axis(\"off\")\n",
    "\n",
    "                cur = min(t-1, len(b)-1)\n",
    "                left = max(0, cur-16)\n",
    "                right = min(len(b), cur+16)\n",
    "                window = b[left:right]\n",
    "                window_hex = [f\"{x:02x}\" for x in window]\n",
    "                hi = cur - left\n",
    "                if 0 <= hi < len(window_hex):\n",
    "                    window_hex[hi] = \"[\" + window_hex[hi] + \"]\"\n",
    "\n",
    "                ax2.text(0.01, 0.55, \"bytes(hex) around current:\\n\" + \" \".join(window_hex),\n",
    "                         fontsize=10, family=\"monospace\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                out_frames.append(fig_to_rgb(fig))\n",
    "                plt.close(fig)\n",
    "\n",
    "        gif_path = \"bdh_activation_explain.gif\"\n",
    "        imageio.mimsave(gif_path, out_frames, fps=FPS)\n",
    "\n",
    "        st.success(f\"Saved GIF: {gif_path}\")\n",
    "\n",
    "        # Display looping GIF reliably\n",
    "        with open(gif_path, \"rb\") as f:\n",
    "            b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        st.markdown(f\"<img src='data:image/gif;base64,{b64}' loop='infinite' />\", unsafe_allow_html=True)\n",
    "\n",
    "st.caption(\"Tip: If translations feel 'stuck', hit Reset cache in the sidebar. NEG words are never stored—only what you type is used.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44dab7-4267-4293-9479-acd371730779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(39F7ybX9kasKCYRs2VZhiT419kA_3sxai6e28Ya5Xix2aKFxr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814819d-189b-4714-b3a6-f9bfe9381536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time\n",
    "from pyngrok import ngrok\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"]\n",
    ")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "public_url = ngrok.connect(8501).public_url\n",
    "print(\" Streamlit running at:\", public_url)\n",
    "\n",
    "display(IFrame(public_url, width=1200, height=750))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
