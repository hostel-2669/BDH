{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1e288-dbf6-4f57-97ac-4249cbde594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bdh_europarl_train_probe.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) BDH MODEL (byte-level vocab)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class BDHConfig:\n",
    "    n_layer: int = 6\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    n_head: int = 4\n",
    "    mlp_internal_dim_multiplier: int = 128\n",
    "    vocab_size: int = 256  # byte-level IDs in [0..255]\n",
    "\n",
    "\n",
    "def get_freqs(n: int, theta: float, dtype: torch.dtype) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rope-like frequency schedule used inside BDH attention.\n",
    "\n",
    "    Implementation note:\n",
    "    - quantize() keeps frequency steps coarse (as in the original BDH-style code).\n",
    "    - returned tensor has shape (n,) and dtype=float-like.\n",
    "    \"\"\"\n",
    "    def quantize(t: torch.Tensor, q: int = 2) -> torch.Tensor:\n",
    "        return (t / q).floor() * q\n",
    "\n",
    "    return (\n",
    "        1.0\n",
    "        / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n))\n",
    "        / (2 * math.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    BDH attention block (non-softmax, associative accumulation).\n",
    "\n",
    "    Key difference vs Transformer:\n",
    "    - No softmax normalization.\n",
    "    - Uses a causal triangular mask via tril().\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "\n",
    "        # Buffer so it moves with the module and is saved in state_dict.\n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def phases_cos_sin(phases: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        return torch.cos(phases), torch.sin(phases)\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        # rotate pairs (even, odd) -> (-odd, even)\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())\n",
    "        phases_cos, phases_sin = Attention.phases_cos_sin(phases)\n",
    "        return (v * phases_cos).to(v.dtype) + (v_rot * phases_sin).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          Q, K: (B, nh, T, N)\n",
    "          V:    (B, 1,  T, D)  (as in your BDH implementation)\n",
    "\n",
    "        Returns:\n",
    "          (B, nh, T, D)\n",
    "        \"\"\"\n",
    "        assert self.freqs.dtype == torch.float32\n",
    "        assert K is Q, \"This implementation expects K=Q (same sparse activations).\"\n",
    "\n",
    "        _, _, T, _ = Q.size()\n",
    "\n",
    "        # Here we build per-position phases for RoPE. Shape becomes (1,1,T,1) * (1,1,1,N).\n",
    "        r_phases = (\n",
    "            torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype)\n",
    "            .view(1, 1, -1, 1)\n",
    "        ) * self.freqs\n",
    "\n",
    "        # Apply RoPE to Q; then K is set equal to Q (as in your code).\n",
    "        QR = self.rope(r_phases, Q)\n",
    "        KR = QR\n",
    "\n",
    "        # Causal mask: only attend to strictly previous tokens.\n",
    "        # tril(diagonal=-1) removes diagonal (self-attend).\n",
    "        scores = (QR @ KR.mT).tril(diagonal=-1)\n",
    "\n",
    "        # No softmax: directly multiply scores with V (associative accumulation).\n",
    "        return scores @ V\n",
    "\n",
    "\n",
    "class BDH(nn.Module):\n",
    "    \"\"\"\n",
    "    BDH language model with sparse, ReLU-gated intermediate features.\n",
    "\n",
    "    During probing, we return x_sparse (post-ReLU) at each layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "\n",
    "        # Encoder/decoder weights are Parameters (as in your original).\n",
    "        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))\n",
    "        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "\n",
    "        self.attn = Attention(config)\n",
    "        self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.lm_head = nn.Parameter(torch.zeros((D, config.vocab_size)).normal_(std=0.02))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        # Standard init for any Linear/Embedding if they exist.\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None, return_sparse: bool = False):\n",
    "        \"\"\"\n",
    "        idx: (B,T) integer byte IDs\n",
    "        targets: (B,T) next-byte targets (shifted)\n",
    "        return_sparse: if True, also return per-layer sparse activations (x_sparse)\n",
    "\n",
    "        Returns:\n",
    "          logits: (B,T,256)\n",
    "          loss: scalar or None\n",
    "          sparse_cache: list of tensors, one per layer, each (B,nh,T,N)  (only if return_sparse)\n",
    "        \"\"\"\n",
    "        C = self.config\n",
    "        B, T = idx.size()\n",
    "        D = C.n_embd\n",
    "        nh = C.n_head\n",
    "        N = D * C.mlp_internal_dim_multiplier // nh\n",
    "\n",
    "        # Here we embed bytes to (B,1,T,D). The singleton dim matches the original BDH shapes.\n",
    "        x = self.embed(idx).unsqueeze(1)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        sparse_cache = []  # store x_sparse per layer for probing\n",
    "\n",
    "        for _layer in range(C.n_layer):\n",
    "            # Here we project to latent features per head, then apply ReLU to get sparse activations.\n",
    "            x_latent = x @ self.encoder                  # (B,nh,T,N)\n",
    "            x_sparse = F.relu(x_latent)                  # (B,nh,T,N)\n",
    "\n",
    "            if return_sparse:\n",
    "                sparse_cache.append(x_sparse.detach())\n",
    "\n",
    "            # Here attention mixes information over time using sparse Q/K and dense V (=x).\n",
    "            yKV = self.attn(Q=x_sparse, K=x_sparse, V=x)  # (B,nh,T,D)\n",
    "            yKV = self.ln(yKV)\n",
    "\n",
    "            # Second sparse gating path (encoder_v), then elementwise product for “Hebbian-like” interaction.\n",
    "            y_latent = yKV @ self.encoder_v              # (B,nh,T,N)\n",
    "            y_sparse = F.relu(y_latent)                  # (B,nh,T,N)\n",
    "\n",
    "            xy_sparse = x_sparse * y_sparse              # (B,nh,T,N)\n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "\n",
    "            # Decode back to model width and apply residual.\n",
    "            yMLP = xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder  # (B,1,T,D)\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)\n",
    "\n",
    "        logits = x.view(B, T, D) @ self.lm_head          # (B,T,256)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        if return_sparse:\n",
    "            return logits, loss, sparse_cache\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) EUROPARL PIPELINE (download -> extract -> build train.txt)\n",
    "# ============================================================\n",
    "\n",
    "EUROPARL_URLS = {\n",
    "    # Europarl v7 pairs (English with EU languages)\n",
    "    \"de-en.tgz\": \"https://www.statmt.org/europarl/v7/de-en.tgz\",\n",
    "    \"fr-en.tgz\": \"https://www.statmt.org/europarl/v7/fr-en.tgz\",\n",
    "    # Add more if needed:\n",
    "    # \"es-en.tgz\": \"https://www.statmt.org/europarl/v7/es-en.tgz\",\n",
    "}\n",
    "\n",
    "\n",
    "def download(url: str, out_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Here we download the dataset archive if it is not already present locally.\n",
    "    \"\"\"\n",
    "    out_dir = os.path.dirname(out_path)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"[download] exists: {out_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[download] {url}\")\n",
    "    urllib.request.urlretrieve(url, out_path)\n",
    "    print(f\"[download] saved: {out_path}\")\n",
    "\n",
    "\n",
    "def _safe_extract_tgz(tgz_path: str, out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Safer tar extraction: prevents path traversal (files writing outside out_dir).\n",
    "    \"\"\"\n",
    "    def is_within_directory(directory: str, target: str) -> bool:\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonpath([abs_directory]) == os.path.commonpath([abs_directory, abs_target])\n",
    "\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(out_dir, member.name)\n",
    "            if not is_within_directory(out_dir, member_path):\n",
    "                raise RuntimeError(f\"Unsafe tar member path detected: {member.name}\")\n",
    "        tar.extractall(out_dir)\n",
    "\n",
    "\n",
    "def extract_tgz(tgz_path: str, out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Here we extract the downloaded .tgz once and drop a marker file to skip re-extraction.\n",
    "    \"\"\"\n",
    "    marker = os.path.join(out_dir, \".extracted_\" + os.path.basename(tgz_path))\n",
    "    if os.path.exists(marker):\n",
    "        print(f\"[extract] already extracted: {tgz_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[extract] {tgz_path}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    _safe_extract_tgz(tgz_path, out_dir)\n",
    "\n",
    "    with open(marker, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ok\\n\")\n",
    "\n",
    "\n",
    "def iter_text_lines(path: str):\n",
    "    \"\"\"\n",
    "    Generator over clean text lines:\n",
    "    - strips whitespace\n",
    "    - skips empty lines\n",
    "    - skips Europarl tag-like lines (starting with '<')\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"<\"):\n",
    "                continue\n",
    "            yield line\n",
    "\n",
    "\n",
    "def build_train_txt(\n",
    "    data_dir: str,\n",
    "    out_txt: str,\n",
    "    max_lines_per_file: int = 200_000,\n",
    "    seed: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Here we create a single training file by concatenating up to max_lines_per_file\n",
    "    from each Europarl extracted file.\n",
    "\n",
    "    Why:\n",
    "    - BDH training here is byte-level and language-agnostic, so we can mix languages.\n",
    "    - Mixing EN/DE/FR can encourage reuse of sparse features across languages.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    files: List[str] = []\n",
    "    for root, _, fnames in os.walk(data_dir):\n",
    "        for fn in fnames:\n",
    "            # Europarl extracted files typically: europarl-v7.de-en.en / .de etc.\n",
    "            if \"europarl\" in fn and fn.endswith((\".en\", \".de\", \".fr\")):\n",
    "                files.append(os.path.join(root, fn))\n",
    "\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No europarl text files found under {data_dir}. Did extraction work?\")\n",
    "\n",
    "    random.shuffle(files)\n",
    "    out_dir = os.path.dirname(out_txt)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as out:\n",
    "        for fp in files:\n",
    "            c = 0\n",
    "            for ln in iter_text_lines(fp):\n",
    "                out.write(ln + \"\\n\")\n",
    "                c += 1\n",
    "                if c >= max_lines_per_file:\n",
    "                    break\n",
    "\n",
    "    print(f\"[dataset] wrote: {out_txt}\")\n",
    "    print(f\"[dataset] included files: {len(files)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) BYTE DATASET (next-byte prediction)\n",
    "# ============================================================\n",
    "\n",
    "class ByteDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Byte-level language modeling dataset.\n",
    "\n",
    "    Given a long byte array b[0..M-1], we sample windows:\n",
    "      x = b[i : i+block_size]\n",
    "      y = b[i+1 : i+block_size+1]\n",
    "\n",
    "    This trains next-byte prediction: p(b[t+1] | b[:t]).\n",
    "    \"\"\"\n",
    "    def __init__(self, train_txt: str, block_size: int, max_bytes: int):\n",
    "        with open(train_txt, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "        b = text.encode(\"utf-8\", errors=\"ignore\")[:max_bytes]\n",
    "        self.data = torch.tensor(list(b), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return max(0, len(self.data) - self.block_size - 1)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        x = self.data[i : i + self.block_size]\n",
    "        y = self.data[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) NEURON PROBING (sparse features -> top-k -> intersection)\n",
    "# ============================================================\n",
    "\n",
    "def neuron_id(layer: int, head: int, feat: int, nh: int, N: int) -> int:\n",
    "    \"\"\"\n",
    "    Here we map (layer, head, feat) -> a single global integer ID.\n",
    "\n",
    "    This makes it easy to compare neurons across layers and print them.\n",
    "    \"\"\"\n",
    "    return layer * (nh * N) + head * N + feat\n",
    "\n",
    "\n",
    "def ids_for_text(text: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert input text to byte IDs in [0..255] with shape (1,T).\n",
    "    \"\"\"\n",
    "    b = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "    if len(b) == 0:\n",
    "        b = b\" \"  # avoid empty inputs\n",
    "    return torch.tensor([list(b)], dtype=torch.long)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_neurons_for_input(\n",
    "    model: BDH,\n",
    "    text: str,\n",
    "    topk: int = 200,\n",
    "    aggregate: str = \"mean\",  # \"mean\" over positions is more stable than \"last\"\n",
    ") -> List[List[Tuple[int, float, int, int, int]]]:\n",
    "    \"\"\"\n",
    "    Returns per-layer list of top sparse features.\n",
    "\n",
    "    Output format:\n",
    "      hits[layer] = [(global_neuron_id, activation, layer, head, feat), ...]\n",
    "    sorted by activation descending.\n",
    "\n",
    "    We aggregate across positions because byte-level tokenization spreads a word\n",
    "    across multiple bytes, and we want a word-level-ish signal.\n",
    "    \"\"\"\n",
    "    cfg = model.config\n",
    "    nh = cfg.n_head\n",
    "    D = cfg.n_embd\n",
    "    N = D * cfg.mlp_internal_dim_multiplier // nh\n",
    "\n",
    "    x = ids_for_text(text).to(next(model.parameters()).device)\n",
    "    _, _, sparse_cache = model(x, return_sparse=True)\n",
    "\n",
    "    hits_per_layer = []\n",
    "    for layer, x_sparse in enumerate(sparse_cache):\n",
    "        # x_sparse: (1, nh, T, N)\n",
    "        a = x_sparse[0]  # (nh, T, N)\n",
    "\n",
    "        if aggregate == \"last\":\n",
    "            vec = a[:, -1, :]          # (nh, N)\n",
    "        else:\n",
    "            vec = a.mean(dim=1)        # (nh, N) mean over T\n",
    "\n",
    "        flat = vec.reshape(-1)         # (nh*N,)\n",
    "        k = min(topk, flat.numel())\n",
    "        vals, idxs = torch.topk(flat, k=k)\n",
    "\n",
    "        layer_hits = []\n",
    "        for v, ix in zip(vals.tolist(), idxs.tolist()):\n",
    "            head = ix // N\n",
    "            feat = ix % N\n",
    "            gid = neuron_id(layer, head, feat, nh, N)\n",
    "            layer_hits.append((gid, float(v), layer, head, feat))\n",
    "\n",
    "        hits_per_layer.append(layer_hits)\n",
    "\n",
    "    return hits_per_layer\n",
    "\n",
    "\n",
    "def shared_neurons_across_texts(\n",
    "    all_hits: List[List[List[Tuple[int, float, int, int, int]]]],\n",
    "    topk_intersection: int,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    all_hits[word_i][layer] = list of top hits for that word/text and that layer.\n",
    "\n",
    "    Here we compute an intersection:\n",
    "      For each layer:\n",
    "        Take the top topk_intersection neurons per input,\n",
    "        intersect across all inputs,\n",
    "        return the shared neuron IDs.\n",
    "\n",
    "    Output:\n",
    "      shared[layer] = sorted list of global neuron IDs.\n",
    "    \"\"\"\n",
    "    n_layers = len(all_hits[0])\n",
    "    shared: List[List[int]] = []\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        sets = []\n",
    "        for wi in range(len(all_hits)):\n",
    "            ids = set(h[0] for h in all_hits[wi][layer][:topk_intersection])\n",
    "            sets.append(ids)\n",
    "        common = set.intersection(*sets) if sets else set()\n",
    "        shared.append(sorted(common))\n",
    "\n",
    "    return shared\n",
    "\n",
    "\n",
    "def decode_neuron(global_id: int, cfg: BDHConfig) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Inverse of neuron_id(): global integer -> (layer, head, feat).\n",
    "    \"\"\"\n",
    "    nh = cfg.n_head\n",
    "    D = cfg.n_embd\n",
    "    N = D * cfg.mlp_internal_dim_multiplier // nh\n",
    "\n",
    "    per_layer = nh * N\n",
    "    layer = global_id // per_layer\n",
    "    rem = global_id % per_layer\n",
    "    head = rem // N\n",
    "    feat = rem % N\n",
    "    return layer, head, feat\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) TRAIN / PROBE ENTRYPOINTS\n",
    "# ============================================================\n",
    "\n",
    "def train(args) -> None:\n",
    "    \"\"\"\n",
    "    Train BDH as a byte-level LM on Europarl-mixed text.\n",
    "\n",
    "    What we do here:\n",
    "      - build ByteDataset over train.txt\n",
    "      - optimize cross-entropy for next-byte prediction\n",
    "      - save checkpoint for later probing\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
    "    print(f\"[train] device={device}\")\n",
    "\n",
    "    # (Optional) Download + extract Europarl archives\n",
    "    if args.download:\n",
    "        os.makedirs(args.data_dir, exist_ok=True)\n",
    "        for key in args.pairs:\n",
    "            if key not in EUROPARL_URLS:\n",
    "                raise ValueError(f\"Unknown pair key '{key}'. Available: {list(EUROPARL_URLS.keys())}\")\n",
    "            tgz_path = os.path.join(args.data_dir, key)\n",
    "            download(EUROPARL_URLS[key], tgz_path)\n",
    "            extract_tgz(tgz_path, args.data_dir)\n",
    "\n",
    "    # Build train.txt if missing or forced\n",
    "    if not os.path.exists(args.train_txt) or args.rebuild_txt:\n",
    "        build_train_txt(\n",
    "            data_dir=args.data_dir,\n",
    "            out_txt=args.train_txt,\n",
    "            max_lines_per_file=args.max_lines_per_file,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "    # Dataset / loader\n",
    "    ds = ByteDataset(args.train_txt, block_size=args.block_size, max_bytes=args.max_bytes)\n",
    "    if len(ds) <= 0:\n",
    "        raise RuntimeError(\"Dataset too small. Increase --max_bytes or ensure train.txt is non-empty.\")\n",
    "\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        ds, batch_size=args.batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    # Model config\n",
    "    cfg = BDHConfig(\n",
    "        vocab_size=256,\n",
    "        n_layer=args.n_layer,\n",
    "        n_embd=args.n_embd,\n",
    "        n_head=args.n_head,\n",
    "        mlp_internal_dim_multiplier=args.mult,\n",
    "        dropout=args.dropout,\n",
    "    )\n",
    "    model = BDH(cfg).to(device)\n",
    "\n",
    "    # Resume if requested\n",
    "    if args.resume and os.path.exists(args.ckpt_path):\n",
    "        model.load_state_dict(torch.load(args.ckpt_path, map_location=device))\n",
    "        print(f\"[train] resumed from {args.ckpt_path}\")\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < args.steps:\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            _, loss = model(x, y)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            if step % args.log_every == 0:\n",
    "                print(f\"[train] step={step} loss={float(loss):.4f}\")\n",
    "\n",
    "            step += 1\n",
    "            if step >= args.steps:\n",
    "                break\n",
    "\n",
    "    ckpt_dir = os.path.dirname(args.ckpt_path)\n",
    "    if ckpt_dir:\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), args.ckpt_path)\n",
    "    print(f\"[train] saved checkpoint: {args.ckpt_path}\")\n",
    "\n",
    "\n",
    "def probe(args) -> None:\n",
    "    \"\"\"\n",
    "    Probe a trained model to find shared sparse neurons across multiple inputs.\n",
    "\n",
    "    What we do here:\n",
    "      - load checkpoint\n",
    "      - compute top-k sparse features per layer for each input text\n",
    "      - intersect the top-k sets across inputs\n",
    "      - print shared neuron IDs (candidate reusable features)\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
    "    print(f\"[probe] device={device}\")\n",
    "\n",
    "    cfg = BDHConfig(\n",
    "        vocab_size=256,\n",
    "        n_layer=args.n_layer,\n",
    "        n_embd=args.n_embd,\n",
    "        n_head=args.n_head,\n",
    "        mlp_internal_dim_multiplier=args.mult,\n",
    "        dropout=0.0,  # important: disable dropout at probe time\n",
    "    )\n",
    "    model = BDH(cfg).to(device)\n",
    "\n",
    "    if not os.path.exists(args.ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {args.ckpt_path}. Train first.\")\n",
    "    model.load_state_dict(torch.load(args.ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    texts = args.texts\n",
    "    if len(texts) < 2:\n",
    "        raise ValueError(\"Provide at least 2 texts/words to find shared neurons.\")\n",
    "\n",
    "    all_hits = []\n",
    "    for t in texts:\n",
    "        hits = top_neurons_for_input(\n",
    "            model, t, topk=args.topk, aggregate=args.aggregate\n",
    "        )\n",
    "        all_hits.append(hits)\n",
    "\n",
    "    shared = shared_neurons_across_texts(all_hits, topk_intersection=args.topk_intersection)\n",
    "\n",
    "    print(\"\\nInputs:\")\n",
    "    for i, t in enumerate(texts):\n",
    "        print(f\"  [{i}] {t}\")\n",
    "\n",
    "    print(\"\\nShared neuron IDs per layer (intersection of top-k):\")\n",
    "    for layer, ids in enumerate(shared):\n",
    "        show = ids[:args.show]\n",
    "        print(f\"\\nLayer {layer}: {len(ids)} shared IDs\")\n",
    "        if not show:\n",
    "            print(\"  (none) -> try: train longer, increase --topk/--topk_intersection, or probe with short sentences.\")\n",
    "            continue\n",
    "        for gid in show:\n",
    "            L, H, Fidx = decode_neuron(gid, cfg)\n",
    "            print(f\"  neuron_id={gid}  (layer={L}, head={H}, feat={Fidx})\")\n",
    "\n",
    "    if args.print_top:\n",
    "        print(\"\\nTop neurons per input (first few layers):\")\n",
    "        for i, t in enumerate(texts):\n",
    "            print(f\"\\n=== Input [{i}] {t} ===\")\n",
    "            hits = all_hits[i]\n",
    "            for layer in range(min(args.layers_print, len(hits))):\n",
    "                top = hits[layer][:args.show]\n",
    "                print(f\"  Layer {layer}:\")\n",
    "                for gid, val, _L, H, Fidx in top:\n",
    "                    print(f\"    neuron_id={gid} act={val:.4f} (head={H}, feat={Fidx})\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    p = argparse.ArgumentParser()\n",
    "    sub = p.add_subparsers(dest=\"cmd\", required=True)\n",
    "\n",
    "    # TRAIN\n",
    "    pt = sub.add_parser(\"train\")\n",
    "    pt.add_argument(\"--data_dir\", type=str, default=\"data_europarl\")\n",
    "    pt.add_argument(\"--train_txt\", type=str, default=\"data_europarl/train.txt\")\n",
    "    pt.add_argument(\"--download\", action=\"store_true\", help=\"download+extract Europarl\")\n",
    "    pt.add_argument(\"--pairs\", nargs=\"+\", default=[\"de-en.tgz\", \"fr-en.tgz\"], help=\"which Europarl tgz keys\")\n",
    "    pt.add_argument(\"--rebuild_txt\", action=\"store_true\")\n",
    "    pt.add_argument(\"--max_lines_per_file\", type=int, default=200_000)\n",
    "    pt.add_argument(\"--max_bytes\", type=int, default=50_000_000)\n",
    "    pt.add_argument(\"--block_size\", type=int, default=256)\n",
    "    pt.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    pt.add_argument(\"--steps\", type=int, default=5000)\n",
    "    pt.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    pt.add_argument(\"--log_every\", type=int, default=100)\n",
    "    pt.add_argument(\"--seed\", type=int, default=0)\n",
    "    pt.add_argument(\"--ckpt_path\", type=str, default=\"checkpoints/bdh_europarl_bytes.pt\")\n",
    "    pt.add_argument(\"--resume\", action=\"store_true\")\n",
    "    pt.add_argument(\"--cpu\", action=\"store_true\")\n",
    "\n",
    "    # Model hyperparams (must match probe time)\n",
    "    pt.add_argument(\"--n_layer\", type=int, default=6)\n",
    "    pt.add_argument(\"--n_embd\", type=int, default=256)\n",
    "    pt.add_argument(\"--n_head\", type=int, default=4)\n",
    "    pt.add_argument(\"--mult\", type=int, default=128)\n",
    "    pt.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "\n",
    "    # PROBE\n",
    "    ppb = sub.add_parser(\"probe\")\n",
    "    ppb.add_argument(\"--texts\", nargs=\"+\", required=True, help=\"words/sentences you provide at test time\")\n",
    "    ppb.add_argument(\"--ckpt_path\", type=str, default=\"checkpoints/bdh_europarl_bytes.pt\")\n",
    "    ppb.add_argument(\"--topk\", type=int, default=300, help=\"top neurons to compute per layer per input\")\n",
    "    ppb.add_argument(\"--topk_intersection\", type=int, default=200, help=\"intersection over this many top neurons\")\n",
    "    ppb.add_argument(\"--show\", type=int, default=30, help=\"how many shared IDs to print per layer\")\n",
    "    ppb.add_argument(\"--aggregate\", choices=[\"mean\", \"last\"], default=\"mean\", help=\"aggregate over bytes\")\n",
    "    ppb.add_argument(\"--print_top\", action=\"store_true\", help=\"also print top neurons per input\")\n",
    "    ppb.add_argument(\"--layers_print\", type=int, default=2)\n",
    "    ppb.add_argument(\"--cpu\", action=\"store_true\")\n",
    "\n",
    "    # Model hyperparams (must match training)\n",
    "    ppb.add_argument(\"--n_layer\", type=int, default=6)\n",
    "    ppb.add_argument(\"--n_embd\", type=int, default=256)\n",
    "    ppb.add_argument(\"--n_head\", type=int, default=4)\n",
    "    ppb.add_argument(\"--mult\", type=int, default=128)\n",
    "\n",
    "    args = p.parse_args()\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(1337)\n",
    "    random.seed(1337)\n",
    "\n",
    "    if args.cmd == \"train\":\n",
    "        train(args)\n",
    "    elif args.cmd == \"probe\":\n",
    "        probe(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f0ac6-92d0-42a1-a355-aec1bb1d0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l bdh_europarl_train_probe.py\n",
    "!ls -lh bdh_europarl_train_probe.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf50c2-6b93-4604-9c8b-784345fe7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a84112-d382-4f9f-a58b-fa5bb93ea215",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u bdh_europarl_train_probe.py train --download \\\n",
    "  --steps 200 --log_every 10 \\\n",
    "  --batch_size 2 --block_size 128 \\\n",
    "  --n_layer 4 --n_embd 128 --n_head 4 --mult 16 \\\n",
    "  --max_lines_per_file 20000 --max_bytes 5000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5664e4-bc84-4d12-92be-c825d2e4f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u bdh_europarl_train_probe.py train \\\n",
    "  --steps 2000 --log_every 50 \\\n",
    "  --batch_size 4 --block_size 128 \\\n",
    "  --n_layer 6 --n_embd 128 --n_head 4 --mult 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829cbb41-72c1-4991-bb1a-ca9f773870e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u bdh_europarl_train_probe.py train --resume \\\n",
    "  --steps 12000 --log_every 200 \\\n",
    "  --batch_size 4 --block_size 128 \\\n",
    "  --n_layer 6 --n_embd 128 --n_head 4 --mult 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd753d-a52e-434b-b3dc-11b11b2e4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2ca55-bf3a-4f6c-9263-34e4e2841fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_CODE = r'''\n",
    "\n",
    "import os, re, importlib.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import streamlit as st\n",
    "from transformers import (MarianMTModel, MarianTokenizer,\n",
    "                          AutoModelForCausalLM, AutoTokenizer)\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"BDH Monosemanticity Probe | KRITI 2026\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# CSS  —  dark space theme\n",
    "# --------------------------------------------------------------\n",
    "st.markdown(r\"\"\"\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;600;700&display=swap');\n",
    "html,body,[class*=\"css\"]{ font-family:'Space Grotesk',sans-serif; }\n",
    ".stApp{\n",
    "  background:#04070f;\n",
    "  background-image:\n",
    "    radial-gradient(ellipse at 12% 4%,rgba(18,55,180,.22) 0%,transparent 52%),\n",
    "    radial-gradient(ellipse at 88% 96%,rgba(80,18,160,.18) 0%,transparent 52%);\n",
    "}\n",
    "[data-testid=\"stSidebar\"]{background:#060c1a!important;border-right:1px solid rgba(60,110,255,.20);}\n",
    "[data-testid=\"stSidebar\"] label{color:#c8dcff!important;font-size:.88rem;}\n",
    "[data-testid=\"stSidebar\"] h3{\n",
    "  font-family:'JetBrains Mono',monospace!important;color:#7aa8ff!important;\n",
    "  font-size:.72rem!important;letter-spacing:.16em;text-transform:uppercase;\n",
    "  margin-top:1.8rem!important;padding-bottom:5px;border-bottom:1px solid rgba(70,120,255,.22);\n",
    "}\n",
    "h1{font-family:'JetBrains Mono',monospace!important;font-size:2.0rem!important;\n",
    "   font-weight:700!important;color:#f0f6ff!important;}\n",
    "h2{font-family:'JetBrains Mono',monospace!important;color:#c4dcff!important;\n",
    "   font-size:1.18rem!important;margin-top:2rem!important;\n",
    "   border-left:4px solid #4a78f0;padding-left:.9rem;}\n",
    "h3{font-family:'JetBrains Mono',monospace!important;color:#9ec4ff!important;font-size:.98rem!important;}\n",
    "p,li{color:#c8dcff;font-size:.97rem;line-height:1.72;}\n",
    ".stMarkdown p{color:#c8dcff!important;}\n",
    "[data-testid=\"stMetric\"]{background:rgba(10,18,48,.95)!important;\n",
    "  border:1px solid rgba(65,115,255,.30)!important;border-radius:10px;padding:.9rem 1.1rem!important;}\n",
    "[data-testid=\"stMetric\"] label{font-family:'JetBrains Mono',monospace!important;\n",
    "  color:#7aa8ff!important;font-size:.70rem!important;letter-spacing:.14em;text-transform:uppercase;}\n",
    "[data-testid=\"stMetric\"] [data-testid=\"stMetricValue\"]{font-family:'JetBrains Mono',monospace!important;\n",
    "  color:#f0f6ff!important;font-size:1.60rem!important;font-weight:700!important;}\n",
    ".stButton>button{background:linear-gradient(135deg,#162870 0%,#261668 100%)!important;\n",
    "  border:1px solid rgba(110,165,255,.55)!important;color:#ddeeff!important;\n",
    "  font-family:'JetBrains Mono',monospace!important;font-size:.78rem!important;\n",
    "  letter-spacing:.10em;border-radius:7px!important;padding:.70rem 1.6rem!important;\n",
    "  transition:all .18s ease!important;}\n",
    ".stButton>button:hover{background:linear-gradient(135deg,#1f3aa8 0%,#321f9e 100%)!important;\n",
    "  box-shadow:0 0 20px rgba(80,130,255,.4)!important;}\n",
    ".stTextInput>div>div>input,.stTextArea>div>div>textarea,.stNumberInput>div>div>input{\n",
    "  background:rgba(6,12,32,.98)!important;border:1px solid rgba(80,130,255,.42)!important;\n",
    "  color:#f0f6ff!important;border-radius:7px!important;\n",
    "  font-family:'JetBrains Mono',monospace!important;font-size:.88rem!important;}\n",
    ".stTabs [data-baseweb=\"tab-list\"]{background:rgba(6,10,28,.94);border-bottom:1px solid rgba(60,100,230,.22);}\n",
    ".stTabs [data-baseweb=\"tab\"]{font-family:'JetBrains Mono',monospace!important;\n",
    "  font-size:.76rem!important;letter-spacing:.09em;color:#6882b8!important;\n",
    "  padding:.76rem 1.30rem!important;text-transform:uppercase;}\n",
    ".stTabs [aria-selected=\"true\"]{color:#a8caff!important;\n",
    "  border-bottom:2px solid #4a78f0!important;background:rgba(22,42,110,.30)!important;}\n",
    ".info-strip{background:rgba(10,20,52,.90);border:1px solid rgba(60,100,230,.24);\n",
    "  border-radius:11px;padding:1.1rem 1.4rem;margin:.8rem 0 1.3rem 0;\n",
    "  font-size:.95rem;color:#c8dcff;line-height:1.78;}\n",
    ".token-hdr{font-family:'JetBrains Mono',monospace;font-size:.84rem;color:#5290ff;\n",
    "  letter-spacing:.11em;text-transform:uppercase;margin:1.5rem 0 .55rem 0;\n",
    "  padding:.55rem 1rem;border-left:4px solid #5290ff;\n",
    "  background:rgba(18,38,100,.32);border-radius:0 7px 7px 0;}\n",
    ".badge-pass{display:inline-block;background:rgba(18,155,75,.20);\n",
    "  border:1px solid rgba(35,210,100,.50);color:#6dffa8;\n",
    "  font-family:'JetBrains Mono',monospace;font-size:.70rem;padding:3px 11px;\n",
    "  border-radius:7px;letter-spacing:.12em;text-transform:uppercase;font-weight:700;}\n",
    ".badge-weak{display:inline-block;background:rgba(200,120,18,.20);\n",
    "  border:1px solid rgba(240,160,40,.50);color:#ffd060;\n",
    "  font-family:'JetBrains Mono',monospace;font-size:.70rem;padding:3px 11px;\n",
    "  border-radius:7px;letter-spacing:.12em;text-transform:uppercase;font-weight:700;}\n",
    ".concept-row{font-family:'JetBrains Mono',monospace;font-size:1.0rem;\n",
    "  color:#f0f6ff;font-weight:700;}\n",
    ".neuron-tag{display:inline-block;background:rgba(38,82,215,.24);\n",
    "  border:1px solid rgba(100,165,255,.52);color:#c4dcff;\n",
    "  font-family:'JetBrains Mono',monospace;font-size:.70rem;padding:3px 11px;\n",
    "  border-radius:7px;margin-left:8px;font-weight:700;}\n",
    ".subtitle{font-family:'JetBrains Mono',monospace;font-size:.78rem;color:#6882b8;\n",
    "  letter-spacing:.16em;text-transform:uppercase;margin-bottom:1.3rem;}\n",
    ".compare-box{background:rgba(10,20,55,.88);border:1px solid rgba(65,110,255,.28);\n",
    "  border-radius:12px;padding:1.4rem 1.6rem;margin:1rem 0;}\n",
    ".bdh-label{color:#38e090;font-family:'JetBrains Mono',monospace;font-weight:700;font-size:1.0rem;}\n",
    ".tfm-label{color:#ff5a72;font-family:'JetBrains Mono',monospace;font-weight:700;font-size:1.0rem;}\n",
    "hr{border-color:rgba(60,100,230,.14)!important;margin:1.8rem 0!important;}\n",
    "\n",
    "/* Summary scorecard styles */\n",
    ".score-card{background:linear-gradient(135deg,rgba(8,16,48,.98) 0%,rgba(12,8,40,.98) 100%);\n",
    "  border:1px solid rgba(65,110,255,.35);border-radius:16px;\n",
    "  padding:2rem 2.2rem;margin:1.2rem 0;overflow:hidden;}\n",
    ".score-metric-row{display:grid;grid-template-columns:30% 30% 30% 10%;\n",
    "  align-items:center;padding:.85rem 1rem;margin:.4rem 0;\n",
    "  background:rgba(16,28,72,.60);border-radius:10px;\n",
    "  border:1px solid rgba(55,90,200,.20);}\n",
    ".score-metric-row:hover{background:rgba(20,38,95,.80);\n",
    "  border-color:rgba(80,130,255,.35);}\n",
    ".score-metric-name{font-family:'JetBrains Mono',monospace;\n",
    "  font-size:.82rem;color:#8aaedf;line-height:1.5;}\n",
    ".score-metric-sub{font-size:.70rem;color:#445878;margin-top:2px;}\n",
    ".score-val-win{font-family:'JetBrains Mono',monospace;font-size:1.05rem;\n",
    "  font-weight:700;color:#38e090;}\n",
    ".score-val-lose{font-family:'JetBrains Mono',monospace;font-size:1.05rem;\n",
    "  font-weight:700;color:#ff5a72;}\n",
    ".score-val-neutral{font-family:'JetBrains Mono',monospace;font-size:1.05rem;\n",
    "  font-weight:700;color:#c8dcff;}\n",
    ".score-val-sub{font-size:.72rem;margin-top:2px;}\n",
    ".score-badge-win{display:inline-flex;align-items:center;justify-content:center;\n",
    "  background:rgba(25,180,90,.18);border:1px solid rgba(56,224,144,.55);\n",
    "  color:#38e090;font-family:'JetBrains Mono',monospace;font-size:.66rem;\n",
    "  padding:4px 10px;border-radius:20px;letter-spacing:.10em;font-weight:700;}\n",
    ".score-badge-lose{display:inline-flex;align-items:center;justify-content:center;\n",
    "  background:rgba(180,25,55,.18);border:1px solid rgba(255,90,114,.55);\n",
    "  color:#ff5a72;font-family:'JetBrains Mono',monospace;font-size:.66rem;\n",
    "  padding:4px 10px;border-radius:20px;letter-spacing:.10em;font-weight:700;}\n",
    ".score-badge-tie{display:inline-flex;align-items:center;justify-content:center;\n",
    "  background:rgba(120,120,50,.18);border:1px solid rgba(255,200,60,.45);\n",
    "  color:#ffc040;font-family:'JetBrains Mono',monospace;font-size:.66rem;\n",
    "  padding:4px 10px;border-radius:20px;letter-spacing:.10em;font-weight:700;}\n",
    ".score-bar-wrap{height:8px;background:rgba(30,50,110,.50);\n",
    "  border-radius:4px;margin-top:6px;overflow:hidden;}\n",
    ".score-bar-fill{height:100%;border-radius:4px;}\n",
    ".score-hdr{font-family:'JetBrains Mono',monospace;font-size:.68rem;\n",
    "  color:#445878;letter-spacing:.14em;text-transform:uppercase;\n",
    "  padding:.5rem 1rem;margin-bottom:.5rem;}\n",
    ".verdict-strip{text-align:center;padding:1.4rem;border-radius:12px;\n",
    "  margin-top:1.4rem;font-family:'JetBrains Mono',monospace;\n",
    "  font-size:1.0rem;font-weight:700;letter-spacing:.04em;}\n",
    ".expected-box{background:rgba(10,18,50,.85);border:1px solid rgba(55,90,200,.30);\n",
    "  border-radius:10px;padding:1.1rem 1.4rem;margin:.6rem 0;\n",
    "  font-family:'JetBrains Mono',monospace;font-size:.82rem;color:#8aaedf;}\n",
    ".expected-box strong{color:#c4dcff;}\n",
    ".expected-box span.win{color:#38e090;}\n",
    ".expected-box span.lose{color:#ff5a72;}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# MATPLOTLIB THEME\n",
    "# ==============================================================\n",
    "PAL        = [\"#5a9cff\",\"#ff5a72\",\"#38e090\",\"#ffc040\",\"#c878ff\",\"#38d4f8\",\"#ff9040\"]\n",
    "C_POS      = \"#5a9cff\"\n",
    "C_NEG      = \"#ff5060\"\n",
    "C_BDH      = \"#38e090\"\n",
    "C_TFM      = \"#ff5a72\"\n",
    "C_AX       = \"#04070f\"\n",
    "C_EDGE     = \"#243666\"\n",
    "C_GRID     = \"#151e3a\"\n",
    "C_TITLE    = \"#f0f6ff\"\n",
    "C_LABEL    = \"#e0eeff\"\n",
    "C_TICK     = \"#c4dcff\"\n",
    "C_LEG      = \"#f0f6ff\"\n",
    "\n",
    "_TS  = 28\n",
    "_LS  = 21\n",
    "_KS  = 18\n",
    "_AS  = 16\n",
    "_LEG = 17\n",
    "_LTI = 15\n",
    "\n",
    "_BBOX = dict(facecolor=\"#ffffff\", alpha=0.95, edgecolor=\"#90aadd\",\n",
    "             pad=4, boxstyle=\"round,pad=0.30\")\n",
    "_TC   = \"#060d22\"\n",
    "_TW   = \"bold\"\n",
    "\n",
    "EPS = 1e-9\n",
    "\n",
    "# Sparsity threshold for Transformer (|GELU| below this is counted as inactive).\n",
    "# GELU produces negative values for negative pre-activations; these are\n",
    "# functionally suppressed and should count toward \"inactive\" neurons.\n",
    "TFM_SPARSE_THRESH = 0.02\n",
    "\n",
    "\n",
    "def _apply_theme():\n",
    "    plt.rcParams.update({\n",
    "        \"figure.facecolor\": C_AX,   \"axes.facecolor\":  C_AX,\n",
    "        \"axes.edgecolor\":   C_EDGE, \"axes.labelcolor\": C_LABEL,\n",
    "        \"axes.titlecolor\":  C_TITLE,\"axes.titlesize\":  _TS,\n",
    "        \"axes.titlepad\":    24,     \"axes.labelsize\":  _LS,\n",
    "        \"xtick.labelsize\":  _KS,    \"ytick.labelsize\": _KS,\n",
    "        \"legend.fontsize\":  _LEG,   \"legend.title_fontsize\": _LTI,\n",
    "        \"xtick.color\":      C_TICK, \"ytick.color\":     C_TICK,\n",
    "        \"text.color\":       C_LABEL,\"grid.color\":      C_GRID,\n",
    "        \"grid.alpha\":       .9,     \"axes.grid\":       True,\n",
    "        \"axes.grid.axis\":   \"y\",    \"legend.facecolor\":\"#0a1428\",\n",
    "        \"legend.edgecolor\": \"#2e4880\",\n",
    "        \"figure.dpi\":       160,    \"savefig.dpi\":     240,\n",
    "        \"font.family\":      \"monospace\",\n",
    "        \"axes.spines.top\":  False,  \"axes.spines.right\": False,\n",
    "        \"xtick.major.pad\":  10,     \"ytick.major.pad\":    8,\n",
    "        \"axes.linewidth\":   1.5,\n",
    "    })\n",
    "\n",
    "_apply_theme()\n",
    "\n",
    "\n",
    "def _tick_boxes(ax):\n",
    "    ax.figure.canvas.draw()\n",
    "    for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        lab.set_color(_TC)\n",
    "        lab.set_fontweight(_TW)\n",
    "        lab.set_bbox(dict(facecolor=\"#ffffff\", alpha=0.92,\n",
    "                          edgecolor=\"#9ab0e0\", pad=3, boxstyle=\"round,pad=0.22\"))\n",
    "        lab.set_fontsize(_KS)\n",
    "\n",
    "\n",
    "def _style(ax):\n",
    "    ax.title.set_color(C_TITLE)\n",
    "    ax.title.set_fontsize(_TS)\n",
    "    ax.title.set_bbox(dict(facecolor=\"#04070f\", alpha=.60, edgecolor=\"none\", pad=10))\n",
    "    for attr in (ax.xaxis.label, ax.yaxis.label):\n",
    "        attr.set_color(C_LABEL)\n",
    "        attr.set_fontsize(_LS)\n",
    "        attr.set_bbox(dict(facecolor=\"#04070f\", alpha=.40, edgecolor=\"none\", pad=6))\n",
    "\n",
    "\n",
    "def _legend(leg):\n",
    "    if not leg:\n",
    "        return\n",
    "    for t in leg.get_texts():\n",
    "        t.set_color(C_LEG)\n",
    "        t.set_fontsize(_LEG)\n",
    "    if leg.get_title():\n",
    "        leg.get_title().set_color(C_LEG)\n",
    "        leg.get_title().set_fontsize(_LTI)\n",
    "\n",
    "\n",
    "def _tight(fig, ax):\n",
    "    fig.canvas.draw()\n",
    "    _tick_boxes(ax)\n",
    "    fig.tight_layout(pad=3.2)\n",
    "\n",
    "\n",
    "def _fig(w=30, h=12):\n",
    "    _apply_theme()\n",
    "    return plt.figure(figsize=(w, h))\n",
    "\n",
    "\n",
    "def _ann_bars(ax, bars, vals, vmax=None):\n",
    "    ref = vmax if vmax else (max(vals) if len(vals) else 1)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        if v > 5e-4:\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                bar.get_height() + ref * .014,\n",
    "                f\"{v:.3f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=_AS,\n",
    "                fontfamily=\"monospace\", color=_TC, fontweight=\"bold\",\n",
    "                bbox=dict(facecolor=\"#ffffff\", alpha=.93,\n",
    "                          edgecolor=\"#90aadd\", pad=3, boxstyle=\"round,pad=0.22\")\n",
    "            )\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# TRANSLATION UTILITIES\n",
    "# ==============================================================\n",
    "ALL_LANGS = {\n",
    "    \"German\":  \"Helsinki-NLP/opus-mt-en-de\",\n",
    "    \"French\":  \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "    \"Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "}\n",
    "LANG_TAGS = {\"German\": \"DE\", \"French\": \"FR\", \"Spanish\": \"ES\", \"Italian\": \"IT\"}\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def get_translator(lang, device):\n",
    "    tok = MarianTokenizer.from_pretrained(ALL_LANGS[lang])\n",
    "    mdl = MarianMTModel.from_pretrained(ALL_LANGS[lang]).to(device)\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "\n",
    "def translate(lang, text, device):\n",
    "    tok, mdl = get_translator(lang, device)\n",
    "    batch = tok([text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = mdl.generate(**batch, max_new_tokens=96)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# BDH MODEL LOADER\n",
    "# ==============================================================\n",
    "@st.cache_resource(show_spinner=True)\n",
    "def load_bdh(script_path, ckpt_path):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    spec   = importlib.util.spec_from_file_location(\"bdhmod\", script_path)\n",
    "    bdhmod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(bdhmod)\n",
    "\n",
    "    ids_for_text = bdhmod.ids_for_text\n",
    "    neuron_id    = bdhmod.neuron_id\n",
    "\n",
    "    state      = torch.load(ckpt_path, map_location=device)\n",
    "    nh, D, N_t = state[\"encoder\"].shape\n",
    "    mult       = int((N_t * nh) // D)\n",
    "\n",
    "    cfg = bdhmod.BDHConfig(\n",
    "        vocab_size=256, n_layer=6, n_embd=D,\n",
    "        n_head=nh, mlp_internal_dim_multiplier=mult, dropout=0.0\n",
    "    )\n",
    "    mdl = bdhmod.BDH(cfg).to(device)\n",
    "    mdl.load_state_dict(state, strict=True)\n",
    "    mdl.eval()\n",
    "\n",
    "    N = (cfg.n_embd * cfg.mlp_internal_dim_multiplier) // cfg.n_head\n",
    "    return device, cfg, mdl, N, ids_for_text, neuron_id\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# TRANSFORMER (distilgpt2) LOADER\n",
    "# ==============================================================\n",
    "@st.cache_resource(show_spinner=True)\n",
    "def load_transformer(device):\n",
    "    tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "    mdl.eval()\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return mdl, tok\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# ACTIVATION EXTRACTION\n",
    "# ==============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def bdh_token_acts(text, layer_idx, device, model, ids_for_text):\n",
    "    \"\"\"\n",
    "    Return the per-token BDH activation matrix for one layer.\n",
    "    Shape: (T, total_neurons)  — ReLU activations, exact zeros preserved.\n",
    "    \"\"\"\n",
    "    text = text or \" \"\n",
    "    x    = ids_for_text(text).to(device)\n",
    "    _, _, sparse = model(x, return_sparse=True)\n",
    "    acts = sparse[layer_idx][0]          # (n_head, T, N_per_head)\n",
    "    T    = acts.shape[1]\n",
    "    return acts.permute(1, 0, 2).reshape(T, -1).detach().float().cpu().numpy()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def tfm_token_acts(tfm_mdl, tfm_tok, text, layer_idx, device):\n",
    "    \"\"\"\n",
    "    Return per-token MLP activations (post-GELU) for one distilgpt2 layer.\n",
    "    Shape: (T, 3072).\n",
    "\n",
    "    IMPORTANT: GELU(x) can be negative for negative x (GELU(-x) ≈ -x * Phi(-x)).\n",
    "    For sparsity analysis, we treat |activation| < TFM_SPARSE_THRESH as inactive,\n",
    "    since those neurons contribute negligible magnitude regardless of sign.\n",
    "    \"\"\"\n",
    "    text = text or \" \"\n",
    "    enc  = tfm_tok(text, return_tensors=\"pt\",\n",
    "                   truncation=True, max_length=128).to(device)\n",
    "    buf  = {}\n",
    "\n",
    "    def _hook(module, inp, out):\n",
    "        buf[\"h\"] = out.detach()\n",
    "\n",
    "    handle = tfm_mdl.transformer.h[layer_idx].mlp.c_fc.register_forward_hook(_hook)\n",
    "    tfm_mdl(**enc)\n",
    "    handle.remove()\n",
    "\n",
    "    raw = buf[\"h\"].squeeze(0)             # (T, 3072)\n",
    "    return F.gelu(raw).cpu().numpy()      # (T, 3072)\n",
    "\n",
    "\n",
    "def bdh_sparsity_pct(token_acts_matrix):\n",
    "    \"\"\"\n",
    "    Fraction of (token, neuron) pairs with ReLU activation > 0.\n",
    "    Uses exact-zero threshold appropriate for ReLU.\n",
    "    Returns percentage active (non-zero).\n",
    "    \"\"\"\n",
    "    return float(np.mean(token_acts_matrix > 0.0)) * 100.0\n",
    "\n",
    "\n",
    "def tfm_sparsity_pct(token_acts_matrix, thresh=TFM_SPARSE_THRESH):\n",
    "    \"\"\"\n",
    "    Fraction of (token, neuron) pairs where |GELU activation| > thresh.\n",
    "    Uses absolute value because GELU produces negative values for negative\n",
    "    pre-activations, which are functionally near-zero but count as 'active'\n",
    "    if not handled with abs().\n",
    "    Returns percentage active (above threshold in magnitude).\n",
    "    \"\"\"\n",
    "    return float(np.mean(np.abs(token_acts_matrix) > thresh)) * 100.0\n",
    "\n",
    "\n",
    "def activation_entropy(acts_1d):\n",
    "    \"\"\"\n",
    "    Shannon entropy (bits) of the activation magnitude distribution.\n",
    "    Lower entropy => energy concentrated in few neurons => monosemantic.\n",
    "    Higher entropy => energy spread across all neurons => polysemantic.\n",
    "    \"\"\"\n",
    "    a = np.abs(acts_1d)\n",
    "    s = a.sum() + EPS\n",
    "    p = a / s\n",
    "    p = p[p > 1e-12]\n",
    "    return float(-np.sum(p * np.log2(p)))\n",
    "\n",
    "\n",
    "def gini_coefficient(acts_1d):\n",
    "    \"\"\"\n",
    "    Gini coefficient of the absolute activation distribution.\n",
    "    0 = perfectly equal (dense / polysemantic).\n",
    "    1 = perfectly concentrated (sparse / monosemantic).\n",
    "    Complements entropy as a second sparsity measure.\n",
    "    \"\"\"\n",
    "    a = np.abs(acts_1d)\n",
    "    a = np.sort(a)\n",
    "    n = len(a)\n",
    "    if n == 0 or a.sum() < EPS:\n",
    "        return 0.0\n",
    "    idx = np.arange(1, n + 1)\n",
    "    return float((2 * (idx * a).sum()) / (n * a.sum() + EPS) - (n + 1) / n)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# BDH HELPER FUNCTIONS\n",
    "# ==============================================================\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(a & b) / (len(a | b) + EPS)\n",
    "\n",
    "\n",
    "def decode_gid(gid, cfg, N):\n",
    "    pl    = cfg.n_head * N\n",
    "    layer = gid // pl\n",
    "    rem   = gid % pl\n",
    "    return layer, rem // N, rem % N\n",
    "\n",
    "\n",
    "def wsplit(text):\n",
    "    return [w.strip() for w in re.findall(r\"\\S+\", text) if w.strip()]\n",
    "\n",
    "\n",
    "def clean_token(tok_str):\n",
    "    return re.sub(r\"^\\W+|\\W+$\", \"\", tok_str, flags=re.UNICODE).strip()\n",
    "\n",
    "\n",
    "def pos_wrap(c):\n",
    "    return [\n",
    "        f\"The text is about {c}.\",\n",
    "        f\"This sentence mentions {c}.\",\n",
    "        f\"I saw a {c} yesterday.\",\n",
    "    ]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _flat_mean(text, layer_idx, device, model, ids_for_text):\n",
    "    return bdh_token_acts(text, layer_idx, device, model, ids_for_text).mean(0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def topk_list(text, layer_idx, k, device, cfg, model, N, ids_for_text, neuron_id):\n",
    "    fa   = _flat_mean(text, layer_idx, device, model, ids_for_text)\n",
    "    k    = min(int(k), fa.shape[0])\n",
    "    idxs = np.argsort(-fa)[:k]\n",
    "    return [neuron_id(layer_idx, int(ix) // N, int(ix) % N, cfg.n_head, N) for ix in idxs]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def topk_set(text, layer_idx, k, device, cfg, model, N, ids_for_text, neuron_id):\n",
    "    return set(topk_list(text, layer_idx, k, device, cfg, model, N, ids_for_text, neuron_id))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def act_mean(text, layer_idx, gid, device, cfg, model, N, ids_for_text):\n",
    "    layer, head, feat = decode_gid(gid, cfg, N)\n",
    "    if layer != layer_idx:\n",
    "        return 0.0\n",
    "    text = text or \" \"\n",
    "    x    = ids_for_text(text).to(device)\n",
    "    _, _, sparse = model(x, return_sparse=True)\n",
    "    return float(sparse[layer_idx][0][head, :, feat].mean().item())\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# PLOT FUNCTIONS  —  Tabs 1, 2, 3\n",
    "# ==============================================================\n",
    "\n",
    "def plot_line(df, tags, title):\n",
    "    fig = _fig()\n",
    "    ax  = fig.add_subplot(111)\n",
    "    x   = np.arange(len(df))\n",
    "    for i, t in enumerate(tags):\n",
    "        c = PAL[i % len(PAL)]\n",
    "        ax.plot(x, df[f\"{t}_overlap\"], marker=\"o\", lw=4.0, color=c,\n",
    "                label=t, ms=12, markerfacecolor=C_AX,\n",
    "                markeredgewidth=3.5, markeredgecolor=c)\n",
    "        ax.fill_between(x, df[f\"{t}_overlap\"], alpha=.12, color=c)\n",
    "        for xi, yi in zip(x, df[f\"{t}_overlap\"]):\n",
    "            if yi > 0:\n",
    "                ax.annotate(str(int(yi)), (xi, yi),\n",
    "                            textcoords=\"offset points\", xytext=(0, 14),\n",
    "                            ha=\"center\", fontsize=_AS + 1, color=_TC,\n",
    "                            fontfamily=\"monospace\", fontweight=\"bold\",\n",
    "                            bbox=dict(facecolor=\"#fff\", alpha=.94,\n",
    "                                      edgecolor=\"#90aadd\", pad=3,\n",
    "                                      boxstyle=\"round,pad=0.26\"))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df[\"EN_word\"], rotation=22, ha=\"right\")\n",
    "    ax.set_ylabel(\"Shared features with EN (best-match)\", labelpad=14)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _legend(ax.legend(framealpha=.97, title=\"Language\", title_fontsize=_LTI))\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_heat(df, tags, title):\n",
    "    heat = np.vstack([df[f\"{t}_overlap\"].values for t in tags])\n",
    "    fig  = _fig(30, 4 + 2.2 * len(tags))\n",
    "    ax   = fig.add_subplot(111)\n",
    "    im   = ax.imshow(heat, aspect=\"auto\", cmap=\"plasma\",\n",
    "                     interpolation=\"nearest\", vmin=0, vmax=max(heat.max(), 1))\n",
    "    ax.set_yticks(np.arange(len(tags)))\n",
    "    ax.set_yticklabels(tags)\n",
    "    ax.set_xticks(np.arange(len(df)))\n",
    "    ax.set_xticklabels(df[\"EN_word\"], rotation=22, ha=\"right\")\n",
    "    for yi in range(len(tags)):\n",
    "        for xi in range(len(df)):\n",
    "            ax.text(xi, yi, str(int(heat[yi, xi])),\n",
    "                    ha=\"center\", va=\"center\", fontsize=_AS + 2,\n",
    "                    color=_TC, fontfamily=\"monospace\", fontweight=\"bold\",\n",
    "                    bbox=dict(facecolor=\"#fff\", alpha=.85,\n",
    "                              edgecolor=\"none\", pad=2.5,\n",
    "                              boxstyle=\"round,pad=0.22\"))\n",
    "    cb = fig.colorbar(im, ax=ax, fraction=.025, pad=.02)\n",
    "    cb.set_label(\"Overlap count\", color=C_LABEL, fontsize=_LS - 1)\n",
    "    for t in cb.ax.get_yticklabels():\n",
    "        t.set_color(_TC)\n",
    "        t.set_fontweight(_TW)\n",
    "        t.set_bbox(_BBOX)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_baseline(names, actual, base, title):\n",
    "    fig = _fig()\n",
    "    ax  = fig.add_subplot(111)\n",
    "    x   = np.arange(len(names))\n",
    "    bars = ax.bar(x, actual, color=C_POS, alpha=.92, edgecolor=\"none\",\n",
    "                  width=.65, label=\"Actual (EN vs translations)\")\n",
    "    ax.axhline(base.mean(), color=C_NEG, ls=\"--\", lw=3.5,\n",
    "               label=f\"Shuffle baseline mean={base.mean():.3f}\")\n",
    "    ax.axhspan(base.mean() - base.std(), base.mean() + base.std(),\n",
    "               alpha=.14, color=C_NEG, label=\"Baseline +/- 1 sigma\")\n",
    "    _ann_bars(ax, bars, actual, vmax=1.0)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(names, rotation=22, ha=\"right\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"Jaccard overlap\", labelpad=14)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _legend(ax.legend(framealpha=.97))\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_pn_bar(pl, pv, nl, nv, title):\n",
    "    al     = pl + nl\n",
    "    av     = np.concatenate([pv, nv])\n",
    "    colors = [C_POS] * len(pl) + [C_NEG] * len(nl)\n",
    "    vm     = max(float(av.max()) * 1.28, 1e-3)\n",
    "    fig    = _fig()\n",
    "    ax     = fig.add_subplot(111)\n",
    "    bars   = ax.bar(np.arange(len(al)), av, color=colors,\n",
    "                    alpha=.92, edgecolor=\"none\", width=.72)\n",
    "    _ann_bars(ax, bars, av, vmax=vm)\n",
    "    ax.axvline(len(pl) - .5, color=\"#8aaae0\", lw=3.0, alpha=.8, ls=\"--\")\n",
    "    ax.set_xticks(np.arange(len(al)))\n",
    "    ax.set_xticklabels(al, rotation=20, ha=\"right\")\n",
    "    ax.set_ylim(0, vm)\n",
    "    ax.set_ylabel(\"Mean activation\", labelpad=14)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _legend(ax.legend(\n",
    "        handles=[mpatches.Patch(color=C_POS, label=\"POS\"),\n",
    "                 mpatches.Patch(color=C_NEG, label=\"NEG\")],\n",
    "        framealpha=.97))\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_pn_box(pv, nv, title):\n",
    "    fig = _fig(18, 12)\n",
    "    ax  = fig.add_subplot(111)\n",
    "    bp  = ax.boxplot(\n",
    "        [pv, nv], labels=[\"POS\", \"NEG\"], showmeans=True, patch_artist=True,\n",
    "        medianprops=dict(color=\"white\", lw=4.0),\n",
    "        meanprops=dict(marker=\"D\", markerfacecolor=\"#ffe070\",\n",
    "                       markeredgecolor=\"none\", markersize=14),\n",
    "        flierprops=dict(marker=\"o\", markerfacecolor=\"#c4dcff\",\n",
    "                        markersize=9, alpha=.7)\n",
    "    )\n",
    "    bp[\"boxes\"][0].set_facecolor(C_POS); bp[\"boxes\"][0].set_alpha(.55)\n",
    "    bp[\"boxes\"][1].set_facecolor(C_NEG); bp[\"boxes\"][1].set_alpha(.55)\n",
    "    for w in bp[\"whiskers\"] + bp[\"caps\"]:\n",
    "        w.set_color(\"#a8c0e8\")\n",
    "        w.set_lw(3.0)\n",
    "    ax.set_ylabel(\"Mean activation\", labelpad=14)\n",
    "    ax.tick_params(axis=\"x\", labelsize=_KS + 5)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# COMPARISON PLOT FUNCTIONS  —  Tab 4\n",
    "# ==============================================================\n",
    "\n",
    "def plot_sparsity_comparison(bdh_token_mat, tfm_token_mat, title, tfm_thresh):\n",
    "    \"\"\"\n",
    "    Side-by-side sorted activation profiles showing per-token sparsity.\n",
    "\n",
    "    BDH panel: ReLU exact-zero boundary marked in yellow.\n",
    "    Transformer panel: |GELU| < tfm_thresh boundary marked.\n",
    "\n",
    "    The two sparsity metrics are NOT the same scale:\n",
    "      - BDH uses strict threshold=0 (ReLU zeros are exact by design).\n",
    "      - Transformer uses |activation| > tfm_thresh because GELU produces\n",
    "        small-magnitude negative values that are functionally suppressed.\n",
    "\n",
    "    Returns  (fig, pct_bdh_active, pct_tfm_active).\n",
    "    Lower percentage = sparser = better monosemanticity.\n",
    "    \"\"\"\n",
    "    _apply_theme()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(34, 13))\n",
    "\n",
    "    # ---- BDH panel (exact ReLU zeros) ----\n",
    "    bdh_mean = bdh_token_mat.mean(0)\n",
    "    pct_bdh  = bdh_sparsity_pct(bdh_token_mat)\n",
    "    y_bdh    = np.sort(bdh_mean)[::-1][:500]\n",
    "    ax1.bar(np.arange(len(y_bdh)), y_bdh, color=C_BDH,\n",
    "            alpha=.88, edgecolor=\"none\", width=1.0)\n",
    "    nonzero_count = int(np.sum(bdh_mean > 0))\n",
    "    ax1.axvline(min(nonzero_count, 500), color=\"#ffe070\", lw=3.5, ls=\"--\",\n",
    "                label=\"ReLU zero boundary (exact)\")\n",
    "    ax1.set_title(\n",
    "        f\"BDH  —  {pct_bdh:.1f}% neurons active per token  (ReLU > 0)\",\n",
    "        color=C_BDH, fontsize=_TS, pad=20\n",
    "    )\n",
    "    ax1.set_xlabel(\"Neuron rank (by mean activation across tokens)\", labelpad=12)\n",
    "    ax1.set_ylabel(\"Mean activation\", labelpad=12)\n",
    "    ax1.annotate(\n",
    "        f\"SPARSE\\n{pct_bdh:.1f}% active\\nReLU exact zeros\",\n",
    "        xy=(len(y_bdh) * .58, float(y_bdh.max()) * .55),\n",
    "        fontsize=_AS + 4, color=_TC, fontfamily=\"monospace\",\n",
    "        fontweight=\"bold\", ha=\"center\",\n",
    "        bbox=dict(facecolor=\"#ffffff\", alpha=.93,\n",
    "                  edgecolor=C_BDH, pad=8, boxstyle=\"round,pad=0.4\")\n",
    "    )\n",
    "    _style(ax1); _tick_boxes(ax1)\n",
    "    _legend(ax1.legend(framealpha=.97))\n",
    "\n",
    "    # ---- Transformer panel (|GELU| > threshold) ----\n",
    "    tfm_abs  = np.abs(tfm_token_mat)\n",
    "    tfm_mean = tfm_abs.mean(0)\n",
    "    pct_tfm  = tfm_sparsity_pct(tfm_token_mat, thresh=tfm_thresh)\n",
    "    y_tfm    = np.sort(tfm_mean)[::-1][:500]\n",
    "    ax2.bar(np.arange(len(y_tfm)), y_tfm, color=C_TFM,\n",
    "            alpha=.80, edgecolor=\"none\", width=1.0)\n",
    "    # Mark the effective \"inactive\" threshold level on the y-axis\n",
    "    ax2.axhline(tfm_thresh, color=\"#ffe070\", lw=3.5, ls=\"--\",\n",
    "                label=f\"|GELU| = {tfm_thresh:.3f} inactive threshold\")\n",
    "    # Shade the region below threshold\n",
    "    ax2.fill_between(np.arange(len(y_tfm)),\n",
    "                     0, min(tfm_thresh, float(y_tfm.min()) + tfm_thresh),\n",
    "                     alpha=.20, color=\"#ffe070\",\n",
    "                     label=\"Near-zero region (inactive)\")\n",
    "    ax2.set_title(\n",
    "        f\"Transformer (distilGPT-2)  —  {pct_tfm:.1f}% neurons active per token\"\n",
    "        f\"  (|GELU| > {tfm_thresh:.3f})\",\n",
    "        color=C_TFM, fontsize=_TS, pad=20\n",
    "    )\n",
    "    ax2.set_xlabel(\"Neuron rank (by mean |activation| across tokens)\", labelpad=12)\n",
    "    ax2.set_ylabel(\"Mean |activation|\", labelpad=12)\n",
    "    ax2.annotate(\n",
    "        f\"DENSE\\n{pct_tfm:.1f}% active\\nGELU near-universal\",\n",
    "        xy=(len(y_tfm) * .58, float(y_tfm.max()) * .55),\n",
    "        fontsize=_AS + 4, color=_TC, fontfamily=\"monospace\",\n",
    "        fontweight=\"bold\", ha=\"center\",\n",
    "        bbox=dict(facecolor=\"#ffffff\", alpha=.93,\n",
    "                  edgecolor=C_TFM, pad=8, boxstyle=\"round,pad=0.4\")\n",
    "    )\n",
    "    _style(ax2); _tick_boxes(ax2)\n",
    "    _legend(ax2.legend(framealpha=.97))\n",
    "\n",
    "    fig.suptitle(title, fontsize=_TS + 3, color=C_TITLE, y=1.01,\n",
    "                 fontfamily=\"monospace\", fontweight=\"bold\",\n",
    "                 bbox=dict(facecolor=\"#04070f\", alpha=.6,\n",
    "                           edgecolor=\"none\", pad=10))\n",
    "    fig.tight_layout(pad=3.5)\n",
    "    return fig, pct_bdh, pct_tfm\n",
    "\n",
    "\n",
    "def plot_entropy_comparison(concepts, bdh_entropies, tfm_entropies, title):\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing Shannon activation entropy per concept.\n",
    "    Lower entropy = monosemantic. BDH expected to have lower entropy.\n",
    "    \"\"\"\n",
    "    _apply_theme()\n",
    "    fig, ax = plt.subplots(figsize=(32, 13))\n",
    "    x = np.arange(len(concepts))\n",
    "    w = 0.38\n",
    "\n",
    "    bars_b = ax.bar(x - w / 2, bdh_entropies, w, color=C_BDH,\n",
    "                    alpha=.88, edgecolor=\"none\",\n",
    "                    label=\"BDH (low entropy = concentrated = monosemantic)\")\n",
    "    bars_t = ax.bar(x + w / 2, tfm_entropies, w, color=C_TFM,\n",
    "                    alpha=.80, edgecolor=\"none\",\n",
    "                    label=\"Transformer (high entropy = diffuse = polysemantic)\")\n",
    "\n",
    "    vm = max(max(bdh_entropies), max(tfm_entropies)) * 1.22\n",
    "    _ann_bars(ax, bars_b, bdh_entropies, vmax=vm)\n",
    "    _ann_bars(ax, bars_t, tfm_entropies, vmax=vm)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(concepts, rotation=20, ha=\"right\")\n",
    "    ax.set_ylabel(\"Activation entropy (bits)\\n\"\n",
    "                  \"  lower = more selective / monosemantic\", labelpad=14)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _legend(ax.legend(framealpha=.97))\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_crosslingual_consistency(concepts, bdh_jacc, tfm_jacc, title):\n",
    "    \"\"\"\n",
    "    Grouped bar chart of mean Jaccard overlap EN <-> translations.\n",
    "    Higher = same neurons respond regardless of language.\n",
    "    \"\"\"\n",
    "    _apply_theme()\n",
    "    fig, ax = plt.subplots(figsize=(32, 13))\n",
    "    x = np.arange(len(concepts))\n",
    "    w = 0.38\n",
    "\n",
    "    bars_b = ax.bar(x - w / 2, bdh_jacc, w, color=C_BDH,\n",
    "                    alpha=.88, edgecolor=\"none\",\n",
    "                    label=\"BDH — cross-lingual neuron overlap\")\n",
    "    bars_t = ax.bar(x + w / 2, tfm_jacc, w, color=C_TFM,\n",
    "                    alpha=.80, edgecolor=\"none\",\n",
    "                    label=\"Transformer — cross-lingual overlap\")\n",
    "\n",
    "    vm = max(float(np.concatenate([bdh_jacc, tfm_jacc]).max()) * 1.28, .1)\n",
    "    _ann_bars(ax, bars_b, bdh_jacc, vmax=vm)\n",
    "    _ann_bars(ax, bars_t, tfm_jacc, vmax=vm)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(concepts, rotation=20, ha=\"right\")\n",
    "    ax.set_ylim(0, vm)\n",
    "    ax.set_ylabel(\"Mean Jaccard overlap (EN <-> translations)\", labelpad=14)\n",
    "    ax.set_title(title)\n",
    "    _style(ax)\n",
    "    _legend(ax.legend(framealpha=.97))\n",
    "    _tight(fig, ax)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_activation_heatmap_compare(words, bdh_vecs, tfm_vecs, n_show=60):\n",
    "    \"\"\"\n",
    "    Side-by-side heatmaps using middle token only to preserve ReLU zeros.\n",
    "\n",
    "    KEY FIX: For BDH we show (n_show//2) TOP active neurons AND (n_show//2)\n",
    "    ZERO/INACTIVE neurons side-by-side.  Selecting only top neurons hides the\n",
    "    sparsity because every selected neuron is by definition active.  Including\n",
    "    the zero neurons makes the dark background and bright spots clearly visible.\n",
    "\n",
    "    Transformer: top neurons only — they are all active (GELU near-universal).\n",
    "    \"\"\"\n",
    "    _apply_theme()\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        1, 2, figsize=(36, max(9, 3 + 2.0 * len(words)))\n",
    "    )\n",
    "\n",
    "    def _heat(ax, mat, title, cmap, color, xlabel=\"Neurons (active | inactive)\"):\n",
    "        im = ax.imshow(mat, aspect=\"auto\", cmap=cmap,\n",
    "                       interpolation=\"nearest\", vmin=0, vmax=1)\n",
    "        ax.set_yticks(np.arange(len(words)))\n",
    "        ax.set_yticklabels(words, fontsize=_KS + 1)\n",
    "        ax.set_xlabel(xlabel, labelpad=12)\n",
    "        ax.set_title(title, color=color, fontsize=_TS, pad=18)\n",
    "        fig.colorbar(im, ax=ax, fraction=.025, pad=.02)\n",
    "        _style(ax)\n",
    "        ax.figure.canvas.draw()\n",
    "        for lab in ax.get_yticklabels():\n",
    "            lab.set_color(_TC)\n",
    "            lab.set_fontweight(_TW)\n",
    "            lab.set_bbox(dict(facecolor=\"#fff\", alpha=.90,\n",
    "                              edgecolor=\"#9ab0e0\", pad=3,\n",
    "                              boxstyle=\"round,pad=0.22\"))\n",
    "\n",
    "    # ---- BDH: interleave top-active + zero neurons ----\n",
    "    n_active_show = n_show // 2\n",
    "    n_zero_show   = n_show // 2\n",
    "    total_bdh     = bdh_vecs.shape[1]\n",
    "\n",
    "    sorted_by_max = np.argsort(-bdh_vecs.max(0))          # highest → lowest\n",
    "    top_active_idx = sorted_by_max[:n_active_show]\n",
    "\n",
    "    # Zero neurons: where ALL concepts have activation = 0 (true ReLU zeros)\n",
    "    zero_mask = (bdh_vecs.max(0) == 0)\n",
    "    zero_indices = np.where(zero_mask)[0]\n",
    "    if len(zero_indices) >= n_zero_show:\n",
    "        rng = np.random.default_rng(42)\n",
    "        zero_sample = rng.choice(zero_indices, n_zero_show, replace=False)\n",
    "    else:\n",
    "        # Fall back to the least-active neurons if not enough true zeros\n",
    "        zero_sample = sorted_by_max[total_bdh - n_zero_show:]\n",
    "\n",
    "    # Concatenate: left = active (bright), right = inactive (dark)\n",
    "    top_idx_b = np.concatenate([top_active_idx, zero_sample])\n",
    "    B          = bdh_vecs[:, top_idx_b]\n",
    "    B_norm     = B / (B.max() + EPS)\n",
    "\n",
    "    # Draw a vertical divider between active and zero columns\n",
    "    _heat(ax1, B_norm,\n",
    "          f\"BDH  —  Sparse Activation Heatmap  (middle token)\\n\"\n",
    "          f\"Left {n_active_show} cols: TOP active neurons  |  \"\n",
    "          f\"Right {n_zero_show} cols: ZERO / inactive neurons\",\n",
    "          \"viridis\", C_BDH,\n",
    "          xlabel=f\"← {n_active_show} top active  |  {n_zero_show} zero inactive →\")\n",
    "    # Divider line between active and zero halves\n",
    "    ax1.axvline(n_active_show - 0.5, color=\"#ffe070\", lw=3.0, ls=\"--\", alpha=.9)\n",
    "\n",
    "    # ---- Transformer: top neurons only — all active ----\n",
    "    top_idx_t = np.argsort(-np.abs(tfm_vecs).max(0))[:n_show]\n",
    "    T          = np.abs(tfm_vecs[:, top_idx_t])\n",
    "    T_norm     = T / (T.max() + EPS)\n",
    "    _heat(ax2, T_norm,\n",
    "          f\"Transformer  —  Dense Activation Heatmap  (middle token)\\n\"\n",
    "          f\"Top {n_show} neurons — ALL uniformly active (GELU never zero)\",\n",
    "          \"plasma\", C_TFM,\n",
    "          xlabel=f\"Top {n_show} neurons ranked by |activation|\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Activation Heatmap: BDH (sparse, left=active / right=zero) \"\n",
    "        \"vs Transformer (dense, all active)  —  middle token only\",\n",
    "        fontsize=_TS + 1, color=C_TITLE, y=1.01,\n",
    "        fontfamily=\"monospace\", fontweight=\"bold\"\n",
    "    )\n",
    "    fig.tight_layout(pad=3.5)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _bar_html(pct, color, max_pct=100):\n",
    "    \"\"\"Render a small inline progress bar as HTML.\"\"\"\n",
    "    w = min(100, max(2, pct / max_pct * 100))\n",
    "    return (f'<div class=\"score-bar-wrap\">'\n",
    "            f'<div class=\"score-bar-fill\" style=\"width:{w:.1f}%;'\n",
    "            f'background:{color};\"></div></div>')\n",
    "\n",
    "\n",
    "def render_summary_table(summary, TOPK_SET_BDH, TOPK_SET_TFM, LANGS, tfm_thresh):\n",
    "    \"\"\"\n",
    "    Build the full summary scorecard HTML with winner badges, value bars,\n",
    "    interpretation notes, and a final verdict strip.\n",
    "    \"\"\"\n",
    "    bdh_sp  = summary.get(\"bdh_sparsity\",  float(\"nan\"))\n",
    "    tfm_sp  = summary.get(\"tfm_sparsity\",  float(\"nan\"))\n",
    "    bdh_ent = summary.get(\"bdh_entropy\",   float(\"nan\"))\n",
    "    tfm_ent = summary.get(\"tfm_entropy\",   float(\"nan\"))\n",
    "    bdh_jac = summary.get(\"bdh_jaccard\",   float(\"nan\"))\n",
    "    tfm_jac = summary.get(\"tfm_jaccard\",   float(\"nan\"))\n",
    "\n",
    "    # Determine per-metric winners\n",
    "    sp_bdh_wins  = bdh_sp  < tfm_sp   if not (np.isnan(bdh_sp)  or np.isnan(tfm_sp))  else None\n",
    "    ent_bdh_wins = bdh_ent < tfm_ent  if not (np.isnan(bdh_ent) or np.isnan(tfm_ent)) else None\n",
    "    jac_bdh_wins = bdh_jac > tfm_jac  if not (np.isnan(bdh_jac) or np.isnan(tfm_jac)) else None\n",
    "\n",
    "    def _badge(bdh_wins):\n",
    "        if bdh_wins is None: return \"\"\n",
    "        if bdh_wins:\n",
    "            return ('<span class=\"score-badge-win\">BDH WINS</span>'\n",
    "                    '&nbsp;<span class=\"score-badge-lose\">TFM</span>')\n",
    "        return ('<span class=\"score-badge-win\">TFM WINS</span>'\n",
    "                '&nbsp;<span class=\"score-badge-lose\">BDH</span>')\n",
    "\n",
    "    def _vc(v, is_win):\n",
    "        cls = \"score-val-win\" if is_win else \"score-val-lose\"\n",
    "        return cls\n",
    "\n",
    "    # Sparsity: lower active% = better\n",
    "    sp_max = max(bdh_sp, tfm_sp) + 1 if not (np.isnan(bdh_sp) or np.isnan(tfm_sp)) else 100\n",
    "    sp_b_bar = _bar_html(bdh_sp, C_BDH, sp_max)\n",
    "    sp_t_bar = _bar_html(tfm_sp, C_TFM, sp_max)\n",
    "    sp_b_cls  = _vc(bdh_sp, sp_bdh_wins)\n",
    "    sp_t_cls  = _vc(tfm_sp, not sp_bdh_wins if sp_bdh_wins is not None else False)\n",
    "\n",
    "    # Entropy: lower = better\n",
    "    ent_max = max(bdh_ent, tfm_ent) + 0.1 if not (np.isnan(bdh_ent) or np.isnan(tfm_ent)) else 14\n",
    "    ent_b_bar = _bar_html(bdh_ent, C_BDH, ent_max)\n",
    "    ent_t_bar = _bar_html(tfm_ent, C_TFM, ent_max)\n",
    "    ent_b_cls  = _vc(bdh_ent, ent_bdh_wins)\n",
    "    ent_t_cls  = _vc(tfm_ent, not ent_bdh_wins if ent_bdh_wins is not None else False)\n",
    "\n",
    "    # Jaccard: higher = better\n",
    "    jac_max = max(bdh_jac, tfm_jac) + 0.05 if not (np.isnan(bdh_jac) or np.isnan(tfm_jac)) else 1\n",
    "    jac_b_bar = _bar_html(bdh_jac * 100, C_BDH, jac_max * 100)\n",
    "    jac_t_bar = _bar_html(tfm_jac * 100, C_TFM, jac_max * 100)\n",
    "    jac_b_cls  = _vc(bdh_jac, jac_bdh_wins)\n",
    "    jac_t_cls  = _vc(tfm_jac, not jac_bdh_wins if jac_bdh_wins is not None else False)\n",
    "\n",
    "    lang_str = \", \".join(LANG_TAGS[l] for l in LANGS) if LANGS else \"—\"\n",
    "\n",
    "    # Count wins (among measured metrics only)\n",
    "    measured_wins = [w for w in [sp_bdh_wins, ent_bdh_wins, jac_bdh_wins]\n",
    "                     if w is not None]\n",
    "    bdh_wins   = sum(measured_wins)\n",
    "    total_meas = len(measured_wins)\n",
    "\n",
    "    if bdh_wins == total_meas and total_meas == 3:\n",
    "        v_color  = \"#38e090\"\n",
    "        v_border = \"#38e090\"\n",
    "        verdict  = f\"BDH wins all {total_meas}/{total_meas} measured metrics — Architectural monosemanticity confirmed.\"\n",
    "    elif bdh_wins >= 2:\n",
    "        v_color  = \"#38e090\"\n",
    "        v_border = \"#38e090\"\n",
    "        verdict  = f\"BDH leads {bdh_wins}/{total_meas} measured metrics — Strong evidence of architectural monosemanticity.\"\n",
    "    elif bdh_wins == 1:\n",
    "        v_color  = \"#ffc040\"\n",
    "        v_border = \"#ffc040\"\n",
    "        verdict  = f\"BDH leads {bdh_wins}/{total_meas} metrics — Try a different layer or increase TOPK.\"\n",
    "    else:\n",
    "        v_color  = \"#ff5a72\"\n",
    "        v_border = \"#ff5a72\"\n",
    "        verdict  = f\"BDH leads {bdh_wins}/{total_meas} metrics — Check layer selection and checkpoint.\"\n",
    "\n",
    "    sp_note    = \"lower % active = sparser = better for BDH\"\n",
    "    ent_note   = \"lower bits = energy concentrated = monosemantic\"\n",
    "    jac_note   = f\"higher overlap = same neurons across DE/FR/ES/IT\"\n",
    "\n",
    "    html = f\"\"\"\n",
    "<div class=\"score-card\">\n",
    "  <!-- Column headers -->\n",
    "  <div class=\"score-hdr\" style=\"display:grid;grid-template-columns:30% 30% 30% 10%;\n",
    "      border-bottom:1px solid rgba(55,90,200,.30);padding-bottom:.6rem;margin-bottom:.4rem;\">\n",
    "    <div>METRIC</div>\n",
    "    <div style=\"color:#38e090;\">BDH (measured)</div>\n",
    "    <div style=\"color:#ff5a72;\">Transformer (measured)</div>\n",
    "    <div>WINNER</div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Row 1: Sparsity -->\n",
    "  <div class=\"score-metric-row\">\n",
    "    <div>\n",
    "      <div class=\"score-metric-name\">Activation Sparsity</div>\n",
    "      <div class=\"score-metric-sub\">% (token, neuron) pairs active<br>\n",
    "        BDH: ReLU &gt; 0 &nbsp;|&nbsp; TFM: |GELU| &gt; {tfm_thresh:.3f}</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38a0ff;margin-top:3px;\">{sp_note}</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"{sp_b_cls}\">{bdh_sp:.1f}%</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38e090;\">ReLU — exact zeros</div>\n",
    "      {sp_b_bar}\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"{sp_t_cls}\">{tfm_sp:.1f}%</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#ff5a72;\">GELU — near-universal</div>\n",
    "      {sp_t_bar}\n",
    "    </div>\n",
    "    <div>{_badge(sp_bdh_wins)}</div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Row 2: Entropy -->\n",
    "  <div class=\"score-metric-row\">\n",
    "    <div>\n",
    "      <div class=\"score-metric-name\">Activation Entropy</div>\n",
    "      <div class=\"score-metric-sub\">Shannon H = -sum(p log2 p) bits<br>PRIMARY monosemanticity metric</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38a0ff;margin-top:3px;\">{ent_note}</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"{ent_b_cls}\">{bdh_ent:.2f} bits</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38e090;\">concentrated</div>\n",
    "      {ent_b_bar}\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"{ent_t_cls}\">{tfm_ent:.2f} bits</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#ff5a72;\">diffuse</div>\n",
    "      {ent_t_bar}\n",
    "    </div>\n",
    "    <div>{_badge(ent_bdh_wins)}</div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Row 3: Jaccard -->\n",
    "  <div class=\"score-metric-row\">\n",
    "    <div>\n",
    "      <div class=\"score-metric-name\">Cross-lingual Jaccard</div>\n",
    "      <div class=\"score-metric-sub\">EN ↔ {lang_str} top-K overlap<br>\n",
    "        BDH TopK={TOPK_SET_BDH} &nbsp;|&nbsp; TFM TopK={TOPK_SET_TFM}</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38a0ff;margin-top:3px;\">{jac_note}</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"{jac_b_cls}\">{bdh_jac:.3f}</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38e090;\">same neurons across langs</div>\n",
    "      {jac_b_bar}\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"{jac_t_cls}\">{tfm_jac:.3f}</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#ff5a72;\">different neurons per lang</div>\n",
    "      {jac_t_bar}\n",
    "    </div>\n",
    "    <div>{_badge(jac_bdh_wins)}</div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Row 4: Interpretability -->\n",
    "  <div class=\"score-metric-row\">\n",
    "    <div>\n",
    "      <div class=\"score-metric-name\">Interpretability Source</div>\n",
    "      <div class=\"score-metric-sub\">How readable are neurons?</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"score-val-win\">Architectural</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38e090;\">ReLU + Hebbian — by design</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"score-val-lose\">Post-hoc</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#ff5a72;\">requires SAE decomposition</div>\n",
    "    </div>\n",
    "    <div><span class=\"score-badge-win\">BDH</span></div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Row 5: Memory -->\n",
    "  <div class=\"score-metric-row\">\n",
    "    <div>\n",
    "      <div class=\"score-metric-name\">Memory Scaling</div>\n",
    "      <div class=\"score-metric-sub\">With sequence length T</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"score-val-win\">O(n &times; d)</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#38e090;\">constant — Hebbian table</div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <div class=\"score-val-lose\">O(T&sup2;)</div>\n",
    "      <div class=\"score-metric-sub\" style=\"color:#ff5a72;\">KV-cache grows with T</div>\n",
    "    </div>\n",
    "    <div><span class=\"score-badge-win\">BDH</span></div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Verdict -->\n",
    "  <div class=\"verdict-strip\" style=\"border:2px solid {v_border};\n",
    "      background:rgba(8,16,48,.90);margin-top:1.4rem;\">\n",
    "    <span style=\"color:{v_color};\">\n",
    "      BDH wins {bdh_wins}/{total_meas} measured metrics  &mdash;&nbsp;{verdict}\n",
    "    </span>\n",
    "  </div>\n",
    "</div>\"\"\"\n",
    "    return html\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SIDEBAR\n",
    "# ==============================================================\n",
    "with st.sidebar:\n",
    "    st.markdown(\"\"\"\n",
    "<div style=\"margin-bottom:.6rem;\">\n",
    "  <div style=\"font-family:'JetBrains Mono',monospace;font-weight:700;\n",
    "      font-size:.96rem;color:#c4dcff;\">BDH MONOSEMANTICITY</div>\n",
    "  <div style=\"font-size:.66rem;color:#304880;letter-spacing:.14em;margin-top:2px;\">\n",
    "      KRITI 2026 | PATH B | INTERPRETABILITY</div>\n",
    "</div>\"\"\", unsafe_allow_html=True)\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### Model Paths\")\n",
    "    BDH_SCRIPT_PATH = st.text_input(\n",
    "        \"BDH script path\", value=\"/content/bdh_europarl_train_probe.py\")\n",
    "    CKPT_PATH = st.text_input(\n",
    "        \"Checkpoint path\", value=\"checkpoints/bdh_europarl_bytes.pt\")\n",
    "    st.markdown(\"### Probe Parameters\")\n",
    "    layers_in  = st.text_input(\"Layers (comma-separated)\", value=\"4,5\")\n",
    "    TOPK_PRINT = st.number_input(\"TOPK_PRINT (Demo TopK)\",\n",
    "                                 min_value=1, max_value=50, value=5)\n",
    "    st.markdown(\"### TopK Settings\")\n",
    "    TOPK_SET_BDH = st.number_input(\n",
    "        \"TopK neurons — BDH\",\n",
    "        min_value=10, max_value=5000, value=200,\n",
    "        help=\"Number of top-K BDH neurons used for Jaccard overlap and candidate selection.\")\n",
    "    TOPK_SET_TFM = st.number_input(\n",
    "        \"TopK neurons — Transformer\",\n",
    "        min_value=10, max_value=3072, value=200,\n",
    "        help=\"Number of top-K Transformer neurons (out of 3072 MLP hidden dim).\")\n",
    "    MAX_CAND   = st.number_input(\"MAX_CAND\",\n",
    "                                 min_value=50, max_value=5000, value=800)\n",
    "    PASS_Z     = st.slider(\"PASS_Z threshold (Tab 3)\", 0.0, 5.0, 1.5, 0.1)\n",
    "    USE_AUG    = st.checkbox(\"POS sentence augmentation\", value=True)\n",
    "    st.markdown(\"### Sparsity Threshold\")\n",
    "    TFM_THRESH_INPUT = st.slider(\n",
    "        \"Transformer |GELU| inactive threshold\",\n",
    "        min_value=0.001, max_value=0.10, value=TFM_SPARSE_THRESH,\n",
    "        step=0.001, format=\"%.3f\",\n",
    "        help=(\n",
    "            \"GELU produces small-magnitude negative values for negative pre-activations. \"\n",
    "            \"Neurons with |activation| below this threshold are counted as inactive. \"\n",
    "            \"0.02 is a sensible default; increase to count fewer neurons as active.\"\n",
    "        )\n",
    "    )\n",
    "    st.markdown(\"### Languages\")\n",
    "    LANGS = st.multiselect(\"Target languages\", options=list(ALL_LANGS.keys()),\n",
    "                           default=list(ALL_LANGS.keys()))\n",
    "    st.markdown(\"### Comparison Settings\")\n",
    "    CMP_LAYER_BDH = st.number_input(\"BDH layer for comparison\",\n",
    "                                    min_value=0, max_value=5, value=4)\n",
    "    CMP_LAYER_TFM = st.number_input(\"TFM layer for comparison\",\n",
    "                                    min_value=0, max_value=5, value=3)\n",
    "\n",
    "LAYERS = [int(p.strip()) for p in layers_in.split(\",\") if p.strip().isdigit()]\n",
    "\n",
    "# Keep backward-compat alias for tabs 1/2/3 that use a single TOPK_SET\n",
    "TOPK_SET = TOPK_SET_BDH\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# HEADER\n",
    "# ==============================================================\n",
    "st.markdown(\"# BDH Monosemanticity Probe\")\n",
    "st.markdown(\n",
    "    '<div class=\"subtitle\">Kriti 2026 | Path B: Interpretability Showcases'\n",
    "    ' | BDH vs Transformer Comparative Study</div>',\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "st.markdown(\"\"\"<div class=\"info-strip\">\n",
    "This tool demonstrates that <strong>BDH synapses are monosemantic</strong> — each synapse\n",
    "responds to a single semantic concept consistently across languages — while Transformer neurons\n",
    "are <strong>polysemantic</strong> (encoding many unrelated concepts simultaneously).\n",
    "ReLU sparse activations produce exact zeros for inactive neurons.\n",
    "GELU activations are near-universal — almost every neuron fires on every token,\n",
    "making the Transformer uninterpretable without post-hoc SAE methods.\n",
    "</div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# LOAD MODELS\n",
    "# ==============================================================\n",
    "try:\n",
    "    device, cfg, model, N, ids_for_text, neuron_id = load_bdh(\n",
    "        BDH_SCRIPT_PATH, CKPT_PATH)\n",
    "    LAYERS = [L for L in LAYERS if 0 <= L < cfg.n_layer]\n",
    "    c1, c2, c3, c4, c5 = st.columns(5)\n",
    "    c1.metric(\"Device\",    device.upper())\n",
    "    c2.metric(\"BDH layers\", cfg.n_layer)\n",
    "    c3.metric(\"n_head\",    cfg.n_head)\n",
    "    c4.metric(\"n_embd\",    cfg.n_embd)\n",
    "    c5.metric(\"N/head\",    N)\n",
    "    if not LAYERS:\n",
    "        st.warning(\"No valid layers selected.\")\n",
    "        st.stop()\n",
    "except Exception as exc:\n",
    "    st.error(f\"BDH load failed: {exc}\")\n",
    "    st.info(\"Set the correct paths in the sidebar.\")\n",
    "    st.stop()\n",
    "\n",
    "with st.spinner(\"Loading distilGPT-2 for comparison...\"):\n",
    "    try:\n",
    "        tfm_mdl, tfm_tok = load_transformer(device)\n",
    "        st.success(\"distilGPT-2 loaded — Transformer comparison ready\")\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Transformer load failed: {e}. Comparison tab limited.\")\n",
    "        tfm_mdl, tfm_tok = None, None\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# TABS\n",
    "# ==============================================================\n",
    "tab_demo, tab_dataset, tab_neurons, tab_compare = st.tabs([\n",
    "    \"DEMO   Meaning-aligned Overlap\",\n",
    "    \"DATASET   Jaccard vs Baseline\",\n",
    "    \"NEURONS   POS vs NEG Selectivity\",\n",
    "    \"BDH vs TRANSFORMER   Comparative Interpretability\",\n",
    "])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# TAB 1 — DEMO\n",
    "# --------------------------------------------------------------\n",
    "with tab_demo:\n",
    "    st.markdown(\"## Demo: Meaning-aligned Sparse Feature Overlap\")\n",
    "    st.markdown(\"\"\"<div class=\"info-strip\">\n",
    "    Enter an English sentence. The tool translates it into selected languages, then checks\n",
    "    whether the <em>same</em> BDH neurons fire for semantically aligned tokens — proving\n",
    "    monosemantic cross-lingual encoding.\n",
    "    </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    demo_sent = st.text_input(\"English word or sentence\",\n",
    "                               value=\"This girl is beautiful.\", key=\"demo_input\")\n",
    "    run_demo  = st.button(\"Run Demo\", key=\"btn_demo\")\n",
    "\n",
    "    if run_demo:\n",
    "        if not LANGS:\n",
    "            st.warning(\"Select at least one language.\")\n",
    "            st.stop()\n",
    "\n",
    "        with st.spinner(\"Translating...\"):\n",
    "            translations = {l: translate(l, demo_sent, device) for l in LANGS}\n",
    "\n",
    "        st.markdown(\"### Full translations\")\n",
    "        tcols = st.columns(1 + len(LANGS))\n",
    "        tcols[0].markdown(f\"**EN**\\n\\n`{demo_sent}`\")\n",
    "        for i, (lang, txt) in enumerate(translations.items()):\n",
    "            tcols[i + 1].markdown(\n",
    "                f\"**{LANG_TAGS.get(lang, lang[:2].upper())}**\\n\\n`{txt}`\")\n",
    "\n",
    "        def best_match_span(lang_tokens, lang_sets, en_set,\n",
    "                            i_en, n_en, max_span=3):\n",
    "            if not lang_tokens:\n",
    "                return 0, 0, 0, []\n",
    "            n      = len(lang_tokens)\n",
    "            j0     = int(round(i_en * (n - 1) / (n_en - 1))) if n_en > 1 else 0\n",
    "            window = max(2, min(5, n // 2))\n",
    "            s_lo   = max(0, j0 - window)\n",
    "            s_hi   = min(n - 1, j0 + window)\n",
    "            best   = None\n",
    "            for s in range(s_lo, s_hi + 1):\n",
    "                U = set()\n",
    "                for span_len in range(1, max_span + 1):\n",
    "                    e = s + span_len\n",
    "                    if e > n:\n",
    "                        break\n",
    "                    U |= (lang_sets[e - 1] if lang_sets[e - 1] else set())\n",
    "                    ov     = len(en_set & U) if en_set else 0\n",
    "                    sh     = sorted(list(en_set & U))[:5]\n",
    "                    center = s + 0.5 * (span_len - 1)\n",
    "                    score  = 1000 * ov - 20 * abs(center - j0) + 2 * span_len\n",
    "                    if best is None or score > best[0]:\n",
    "                        best = (score, ov, span_len, s, sh)\n",
    "            _, ov, span_len, s, sh = best\n",
    "            return s, s + span_len, int(max(ov, 0)), sh\n",
    "\n",
    "        def compute_demo(en_text, trans_dict, layer_idx, topk):\n",
    "            en_words_disp  = wsplit(en_text)\n",
    "            en_words_clean = [clean_token(w) or w for w in en_words_disp]\n",
    "            n_en    = len(en_words_disp)\n",
    "            t2t     = {LANG_TAGS[lang]: txt for lang, txt in trans_dict.items()}\n",
    "            ltoks_disp  = {tag: wsplit(txt) for tag, txt in t2t.items()}\n",
    "            ltoks_clean = {tag: [clean_token(w) or w for w in toks]\n",
    "                           for tag, toks in ltoks_disp.items()}\n",
    "            tags    = list(ltoks_disp.keys())\n",
    "            en_lists = [topk_list(wc, layer_idx, topk, device, cfg, model,\n",
    "                                  N, ids_for_text, neuron_id) if wc else []\n",
    "                        for wc in en_words_clean]\n",
    "            en_sets  = [set(l) for l in en_lists]\n",
    "            lsets    = {tag: [topk_set(tc, layer_idx, topk, device, cfg,\n",
    "                                       model, N, ids_for_text, neuron_id)\n",
    "                               if tc else set()\n",
    "                               for tc in ltoks_clean[tag]]\n",
    "                        for tag in tags}\n",
    "            plot_rows = []\n",
    "            per_token = {}\n",
    "            for i, (en_disp, en_clean) in enumerate(\n",
    "                    zip(en_words_disp, en_words_clean)):\n",
    "                enS  = en_sets[i]\n",
    "                en5  = \", \".join(map(str, en_lists[i][:5])) if en_lists[i] else \"\"\n",
    "                prow = {\"EN_word\": en_disp}\n",
    "                trows = []\n",
    "                for tag in tags:\n",
    "                    toks_disp = ltoks_disp[tag]\n",
    "                    sets      = lsets[tag]\n",
    "                    if not toks_disp:\n",
    "                        prow[f\"{tag}_overlap\"] = 0\n",
    "                        trows.append({\"Language\": tag, \"Full sentence\": \"—\",\n",
    "                                      \"Matched phrase\": \"—\", \"Overlap\": 0,\n",
    "                                      \"Shared neurons (Top-5)\": \"—\"})\n",
    "                        continue\n",
    "                    s_idx, e_idx, bov, bsh = best_match_span(\n",
    "                        toks_disp, sets, enS, i_en=i, n_en=n_en)\n",
    "                    matched_phrase = \" \".join(toks_disp[s_idx:e_idx]).strip()\n",
    "                    prow[f\"{tag}_overlap\"] = int(bov)\n",
    "                    trows.append({\n",
    "                        \"Language\":             tag,\n",
    "                        \"Matched phrase\":       matched_phrase,\n",
    "                        \"Overlap\":              int(bov),\n",
    "                        \"Shared neurons (Top-5)\":\n",
    "                            \", \".join(map(str, bsh)) if bsh else \"—\",\n",
    "                        \"Full sentence\":        t2t[tag],\n",
    "                    })\n",
    "                plot_rows.append(prow)\n",
    "                per_token[en_disp] = {\"en5\": en5, \"rows\": trows}\n",
    "            df_plot = pd.DataFrame(plot_rows)\n",
    "            df_en   = pd.DataFrame([\n",
    "                {\"EN_idx\": i, \"EN_token\": w,\n",
    "                 \"EN_Top5_neurons\": per_token[w][\"en5\"]}\n",
    "                for i, w in enumerate(en_words_disp)\n",
    "            ])\n",
    "            return df_plot, tags, df_en, per_token, en_words_disp\n",
    "\n",
    "        for L in LAYERS:\n",
    "            st.markdown(f\"### Layer {L}\")\n",
    "            with st.spinner(f\"Layer {L}: computing overlaps...\"):\n",
    "                df_sum, tags, df_en, per_token, en_words = compute_demo(\n",
    "                    demo_sent, translations, L, int(TOPK_PRINT))\n",
    "            st.markdown(\"#### EN token — Top neurons\")\n",
    "            st.dataframe(df_en, use_container_width=True,\n",
    "                         height=min(280, 60 + 40 * len(df_en)))\n",
    "            f1 = plot_line(df_sum, tags,\n",
    "                           f\"Meaning-aligned token overlap  |  Layer {L}  |  TopK={TOPK_PRINT}\")\n",
    "            st.pyplot(f1, use_container_width=True); plt.close(f1)\n",
    "            f2 = plot_heat(df_sum, tags,\n",
    "                           f\"Overlap heatmap  |  Layer {L}  |  TopK={TOPK_PRINT}\")\n",
    "            st.pyplot(f2, use_container_width=True); plt.close(f2)\n",
    "            st.markdown(\"#### Per-token breakdown\")\n",
    "            for en_w in en_words:\n",
    "                d = per_token[en_w]\n",
    "                st.markdown(\n",
    "                    f'<div class=\"token-hdr\">EN token: <strong>{en_w}</strong>'\n",
    "                    f'&nbsp;|&nbsp;Top-5 EN neurons: {d[\"en5\"] or \"—\"}</div>',\n",
    "                    unsafe_allow_html=True)\n",
    "                df_rows = pd.DataFrame(d[\"rows\"])[\n",
    "                    [\"Language\", \"Matched phrase\", \"Overlap\",\n",
    "                     \"Shared neurons (Top-5)\", \"Full sentence\"]]\n",
    "                st.dataframe(df_rows, use_container_width=True,\n",
    "                             height=min(460, 60 + 55 * len(df_rows)))\n",
    "            st.markdown(\"---\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# TAB 2 — DATASET\n",
    "# --------------------------------------------------------------\n",
    "with tab_dataset:\n",
    "    st.markdown(\"## Dataset Mode: Jaccard Overlap vs Shuffle Baseline\")\n",
    "    st.markdown('<div class=\"info-strip\">Jaccard overlap between EN sparse feature sets '\n",
    "                'and translations vs a shuffle baseline proves that BDH neurons align '\n",
    "                'semantically, not randomly.</div>', unsafe_allow_html=True)\n",
    "\n",
    "    col_c, col_n = st.columns(2)\n",
    "    with col_c:\n",
    "        st.markdown(\"**Concept words — one per line (min 5)**\")\n",
    "        ct = st.text_area(\n",
    "            \"\", value=\"doctor\\nhospital\\nsurgery\\npatient\\nmedicine\",\n",
    "            height=190, key=\"cta\")\n",
    "    with col_n:\n",
    "        st.markdown(\"**NEG unrelated words — one per line (min 5)**\")\n",
    "        nt = st.text_area(\n",
    "            \"\", value=\"football\\nbanana\\nguitar\\nmountain\\npolitics\",\n",
    "            height=190, key=\"nta\")\n",
    "\n",
    "    n_shuf = st.number_input(\"Shuffle baseline trials\", 5, 200, 20, key=\"ns\")\n",
    "    run_ds = st.button(\"Run Dataset Analysis\", key=\"btn_ds\")\n",
    "\n",
    "    if run_ds:\n",
    "        concepts = [x.strip() for x in ct.splitlines() if x.strip()]\n",
    "        NEG      = [x.strip() for x in nt.splitlines() if x.strip()]\n",
    "        if len(concepts) < 5 or len(NEG) < 5:\n",
    "            st.warning(\"Provide at least 5 concept words and 5 NEG words.\")\n",
    "        elif not LANGS:\n",
    "            st.warning(\"Select at least one language.\")\n",
    "        else:\n",
    "            with st.spinner(\"Translating concept words...\"):\n",
    "                concept_trans = [\n",
    "                    {\"EN\": w, **{l: translate(l, w, device) for l in LANGS}}\n",
    "                    for w in concepts\n",
    "                ]\n",
    "            st.markdown(\"### Concept translation table\")\n",
    "            st.dataframe(pd.DataFrame(concept_trans)[[\"EN\"] + list(LANGS)],\n",
    "                         use_container_width=True, height=300)\n",
    "            st.session_state[\"concept_trans\"] = concept_trans\n",
    "            st.session_state[\"NEG\"]           = NEG\n",
    "\n",
    "            def compute_actual(L):\n",
    "                names, actual = [], []\n",
    "                for ct2 in concept_trans:\n",
    "                    names.append(ct2[\"EN\"])\n",
    "                    Sen = topk_set(ct2[\"EN\"], L, int(TOPK_SET), device,\n",
    "                                   cfg, model, N, ids_for_text, neuron_id)\n",
    "                    actual.append(float(np.mean([\n",
    "                        jaccard(Sen, topk_set(ct2[l], L, int(TOPK_SET), device,\n",
    "                                             cfg, model, N, ids_for_text, neuron_id))\n",
    "                        for l in LANGS\n",
    "                    ])))\n",
    "                return names, np.array(actual)\n",
    "\n",
    "            def compute_shuffle(L, ns):\n",
    "                n    = len(concept_trans)\n",
    "                base = []\n",
    "                for _ in range(ns):\n",
    "                    perm = np.random.permutation(n)\n",
    "                    vals = []\n",
    "                    for i in range(n):\n",
    "                        Sen = topk_set(\n",
    "                            concept_trans[i][\"EN\"], L, int(TOPK_SET),\n",
    "                            device, cfg, model, N, ids_for_text, neuron_id)\n",
    "                        vals.append(float(np.mean([\n",
    "                            jaccard(Sen, topk_set(\n",
    "                                concept_trans[perm[i]][l], L, int(TOPK_SET),\n",
    "                                device, cfg, model, N, ids_for_text, neuron_id))\n",
    "                            for l in LANGS\n",
    "                        ])))\n",
    "                    base.append(np.mean(vals))\n",
    "                return np.array(base)\n",
    "\n",
    "            results = {}\n",
    "            for L in LAYERS:\n",
    "                with st.spinner(f\"Layer {L}: computing...\"):\n",
    "                    names, actual_arr = compute_actual(L)\n",
    "                    base_dist         = compute_shuffle(L, int(n_shuf))\n",
    "                    results[L]        = (names, actual_arr, base_dist)\n",
    "                gap = actual_arr.mean() - base_dist.mean()\n",
    "                a, b, c_ = st.columns(3)\n",
    "                a.metric(f\"L{L} actual mean\",   f\"{actual_arr.mean():.3f}\")\n",
    "                b.metric(f\"L{L} baseline mean\", f\"{base_dist.mean():.3f}\")\n",
    "                c_.metric(f\"L{L} gap\",          f\"{gap:+.3f}\")\n",
    "                fb = plot_baseline(\n",
    "                    names, actual_arr, base_dist,\n",
    "                    f\"Layer {L}: Jaccard vs Shuffle Baseline  |  \"\n",
    "                    f\"TopK={TOPK_SET}  |  shuffles={int(n_shuf)}\")\n",
    "                st.pyplot(fb, use_container_width=True); plt.close(fb)\n",
    "\n",
    "            st.session_state[\"ds_results\"] = results\n",
    "            st.session_state[\"ds_layers\"]  = LAYERS\n",
    "            st.success(\"Dataset analysis complete. Proceed to NEURONS tab.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# TAB 3 — NEURONS\n",
    "# --------------------------------------------------------------\n",
    "with tab_neurons:\n",
    "    st.markdown(\"## Neuron Selectivity: POS vs NEG Activations\")\n",
    "    st.markdown('<div class=\"info-strip\">Finds the single best neuron per concept '\n",
    "                '(z = (pos_mean - neg_mean) / neg_std). PASS if z >= PASS_Z.</div>',\n",
    "                unsafe_allow_html=True)\n",
    "\n",
    "    if \"concept_trans\" not in st.session_state:\n",
    "        st.info(\"Run Dataset Mode first to build the concept list.\")\n",
    "    else:\n",
    "        concept_trans = st.session_state[\"concept_trans\"]\n",
    "        NEG           = st.session_state[\"NEG\"]\n",
    "        results       = st.session_state.get(\"ds_results\", {})\n",
    "        ds_layers     = st.session_state.get(\"ds_layers\", LAYERS)\n",
    "        run_n         = st.button(\"Find Best Neurons and Plot\", key=\"btn_neu\")\n",
    "\n",
    "        if run_n:\n",
    "            TOP_SHOW = min(3, len(concept_trans))\n",
    "\n",
    "            def best_neuron(i, L):\n",
    "                trans = concept_trans[i]\n",
    "                POS   = [trans[\"EN\"]] + [trans[l] for l in LANGS]\n",
    "                if USE_AUG:\n",
    "                    POS += pos_wrap(trans[\"EN\"])\n",
    "                NEG_S = [f\"The text is about {n}.\" for n in NEG]\n",
    "                inter = None\n",
    "                for t in POS:\n",
    "                    S     = topk_set(t, L, int(TOPK_SET), device, cfg,\n",
    "                                     model, N, ids_for_text, neuron_id)\n",
    "                    inter = S if inter is None else inter & S\n",
    "                Sen  = topk_set(trans[\"EN\"], L, int(TOPK_SET), device, cfg,\n",
    "                                model, N, ids_for_text, neuron_id)\n",
    "                cand = list((set(inter or set()) | set(Sen)))[:int(MAX_CAND)]\n",
    "                if not cand:\n",
    "                    return None\n",
    "                best = None\n",
    "                for gid in cand:\n",
    "                    pv  = np.array([act_mean(t, L, gid, device, cfg, model,\n",
    "                                            N, ids_for_text) for t in POS])\n",
    "                    nv  = np.array([act_mean(t, L, gid, device, cfg, model,\n",
    "                                            N, ids_for_text) for t in NEG_S])\n",
    "                    pm, nm = pv.mean(), nv.mean()\n",
    "                    ns_ = nv.std() + 1e-6\n",
    "                    sel = pm - nm\n",
    "                    z   = sel / ns_\n",
    "                    if best is None or (z, sel) > best[\"score\"]:\n",
    "                        best = dict(gid=gid, pos_vals=pv, neg_vals=nv,\n",
    "                                    pos_mean=float(pm), neg_mean=float(nm),\n",
    "                                    neg_std=float(ns_), sel=float(sel),\n",
    "                                    z=float(z), score=(z, sel))\n",
    "                return best\n",
    "\n",
    "            for L in ds_layers:\n",
    "                if L not in results:\n",
    "                    st.write(f\"Layer {L}: no data.\")\n",
    "                    continue\n",
    "                names, actual_arr, base_dist = results[L]\n",
    "                order = np.argsort(-(actual_arr - base_dist.mean()))[:TOP_SHOW]\n",
    "                st.markdown(f\"### Layer {L} — top {TOP_SHOW} concepts by margin\")\n",
    "                for idx in order:\n",
    "                    concept = concept_trans[idx][\"EN\"]\n",
    "                    with st.spinner(f\"Layer {L}: '{concept}'...\"):\n",
    "                        best = best_neuron(idx, L)\n",
    "                    if best is None:\n",
    "                        st.warning(f\"'{concept}': no candidate neurons.\")\n",
    "                        continue\n",
    "                    gid     = best[\"gid\"]\n",
    "                    verdict = \"PASS\" if best[\"z\"] >= PASS_Z else \"WEAK\"\n",
    "                    badge   = ('<span class=\"badge-pass\">PASS</span>'\n",
    "                               if verdict == \"PASS\"\n",
    "                               else '<span class=\"badge-weak\">WEAK</span>')\n",
    "                    st.markdown(\n",
    "                        f'<span class=\"concept-row\">{concept}</span>'\n",
    "                        f'<span class=\"neuron-tag\">neuron {gid}</span>'\n",
    "                        f'<span class=\"neuron-tag\">decode {decode_gid(gid, cfg, N)}</span>'\n",
    "                        f'&nbsp;&nbsp;{badge}',\n",
    "                        unsafe_allow_html=True)\n",
    "                    m1, m2, m3, m4 = st.columns(4)\n",
    "                    m1.metric(\"pos_mean\",    f\"{best['pos_mean']:.4f}\")\n",
    "                    m2.metric(\"neg_mean\",    f\"{best['neg_mean']:.4f}\")\n",
    "                    m3.metric(\"selectivity\", f\"{best['sel']:.4f}\")\n",
    "                    m4.metric(\"z-score\",     f\"{best['z']:.2f}\")\n",
    "                    pl = ([\"EN\"] + [LANG_TAGS[l] for l in LANGS]\n",
    "                          + ([\"P1\", \"P2\", \"P3\"] if USE_AUG else []))\n",
    "                    nl = [f\"NEG:{w}\" for w in NEG]\n",
    "                    fb = plot_pn_bar(\n",
    "                        pl, best[\"pos_vals\"], nl, best[\"neg_vals\"],\n",
    "                        f\"POS vs NEG  |  '{concept}'  |  layer {L}  \"\n",
    "                        f\"|  neuron {gid}  |  {verdict}\")\n",
    "                    st.pyplot(fb, use_container_width=True); plt.close(fb)\n",
    "                    fx = plot_pn_box(\n",
    "                        best[\"pos_vals\"], best[\"neg_vals\"],\n",
    "                        f\"Activation distribution  |  '{concept}'  \"\n",
    "                        f\"|  layer {L}  |  neuron {gid}  |  {verdict}\")\n",
    "                    st.pyplot(fx, use_container_width=True); plt.close(fx)\n",
    "                    st.markdown(\"---\")\n",
    "            st.success(\"Neuron analysis complete.\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# TAB 4 — BDH vs TRANSFORMER\n",
    "# ==============================================================\n",
    "with tab_compare:\n",
    "    st.markdown(\"## BDH vs Transformer: Comparative Interpretability Study\")\n",
    "    st.markdown(\"\"\"<div class=\"info-strip\">\n",
    "    <strong>A — Sparsity:</strong>\n",
    "      BDH uses ReLU — exact zeros.\n",
    "      Transformer uses GELU — neurons are near-universally active.\n",
    "      <em>Sparsity is measured correctly for each architecture</em>:\n",
    "      BDH threshold = 0 (exact); Transformer threshold = |GELU| &gt; threshold\n",
    "      (adjust in sidebar).<br>\n",
    "    <strong>B — Activation Entropy:</strong>\n",
    "      Shannon entropy of activation magnitudes. Lower = concentrated = monosemantic.\n",
    "      This is the primary monosemanticity metric.<br>\n",
    "    <strong>C — Cross-lingual Jaccard:</strong>\n",
    "      Same concept, four languages. BDH retains same neurons; Transformer scatters.<br>\n",
    "    <strong>D — Heatmap:</strong> Middle token only — preserves true ReLU zeros.<br>\n",
    "    <strong>E — Summary Scorecard:</strong>\n",
    "      All metrics with winner badges and comparison bars.\n",
    "    </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    # Expected-results box\n",
    "    st.markdown(f\"\"\"<div class=\"expected-box\">\n",
    "    <strong>Expected results with a well-trained BDH model:</strong><br>\n",
    "    &bull; Sparsity &mdash; BDH: <span class=\"win\">~5-30% active</span> &nbsp;|&nbsp;\n",
    "    Transformer: <span class=\"lose\">~80-99% active</span>\n",
    "    (using |GELU| &gt; {TFM_THRESH_INPUT:.3f}) &nbsp;\n",
    "    <em>Adjust the threshold slider in the sidebar if the Transformer appears too sparse.</em><br>\n",
    "    &bull; Entropy &mdash; BDH: <span class=\"win\">~3-8 bits</span> &nbsp;|&nbsp;\n",
    "    Transformer: <span class=\"lose\">~9-12 bits</span><br>\n",
    "    &bull; Jaccard &mdash; BDH: <span class=\"win\">0.70-0.90</span> &nbsp;|&nbsp;\n",
    "    Transformer: <span class=\"lose\">0.05-0.30</span><br>\n",
    "    &bull; Heatmap &mdash; BDH: <span class=\"win\">dark background, bright isolated spots</span> &nbsp;|&nbsp;\n",
    "    Transformer: <span class=\"lose\">uniformly lit, no clear structure</span>\n",
    "    </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    if tfm_mdl is None:\n",
    "        st.error(\"Transformer model not loaded. Check your connection and re-run.\")\n",
    "        st.stop()\n",
    "\n",
    "    col_a, col_b = st.columns(2)\n",
    "    with col_a:\n",
    "        st.markdown(\"**Concept words** (5-10 for best results)\")\n",
    "        cmp_concepts_raw = st.text_area(\n",
    "            \"\", value=\"doctor\\nhospital\\nsurgery\\npatient\\nmedicine\\ndiagnosis\",\n",
    "            height=190, key=\"cmp_c\")\n",
    "    with col_b:\n",
    "        st.markdown(\"**NEG (unrelated) words**\")\n",
    "        cmp_neg_raw = st.text_area(\n",
    "            \"\", value=\"football\\nbanana\\nguitar\\nmountain\\npolitics\\nocean\",\n",
    "            height=190, key=\"cmp_n\")\n",
    "\n",
    "    st.markdown(\"**Section A — Sparsity sentence** (type any sentence you want to analyse)\")\n",
    "    sparsity_sentence = st.text_input(\n",
    "        \"\",\n",
    "        value=\"The doctor treated the patient carefully.\",\n",
    "        key=\"sparsity_sent\",\n",
    "        help=\"This exact sentence is passed to both BDH and Transformer for sparsity measurement. Type anything you like — try domain-specific sentences, short phrases, or random text to see how sparsity changes.\"\n",
    "    )\n",
    "\n",
    "    run_cmp = st.button(\"Run Full Comparison\", key=\"btn_cmp\")\n",
    "\n",
    "    if run_cmp:\n",
    "        cmp_concepts = [x.strip() for x in cmp_concepts_raw.splitlines() if x.strip()]\n",
    "        cmp_neg      = [x.strip() for x in cmp_neg_raw.splitlines() if x.strip()]\n",
    "        if len(cmp_concepts) < 3:\n",
    "            st.warning(\"Enter at least 3 concept words.\")\n",
    "            st.stop()\n",
    "\n",
    "        summary = {}\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # SECTION A — SPARSITY (architecture-aware measurement)\n",
    "        # ----------------------------------------------------------\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"### A  —  Sparsity (Architecture-Aware Per-Token Measurement)\")\n",
    "        st.markdown(f\"\"\"<div class=\"compare-box\">\n",
    "        <span class=\"bdh-label\">BDH</span>: ReLU produces exact zeros.\n",
    "        A neuron is inactive iff activation = 0 exactly.\n",
    "        Sparsity = fraction of (token, neuron) pairs with activation &gt; 0.<br>\n",
    "        <span class=\"tfm-label\">Transformer</span>: GELU(x) can be negative for negative x,\n",
    "        not zero. Near-universal activation is the norm.\n",
    "        A neuron is inactive iff |GELU| &le; {TFM_THRESH_INPUT:.3f}\n",
    "        (adjust in sidebar &rarr; Sparsity Threshold).\n",
    "        </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        sample_text = sparsity_sentence.strip() or f\"The {cmp_concepts[0]} treated the patient carefully.\"\n",
    "        with st.spinner(f\"Computing per-token activations for: \\\"{sample_text[:60]}\\\"...\"):\n",
    "            bdh_tok_mat = bdh_token_acts(\n",
    "                sample_text, int(CMP_LAYER_BDH), device, model, ids_for_text)\n",
    "            tfm_tok_mat = tfm_token_acts(\n",
    "                tfm_mdl, tfm_tok, sample_text,\n",
    "                min(int(CMP_LAYER_TFM), 5), device)\n",
    "\n",
    "        fig_sp, pct_b, pct_t = plot_sparsity_comparison(\n",
    "            bdh_tok_mat, tfm_tok_mat,\n",
    "            f\"Sparsity  |  \\\"{sample_text[:55]}...\\\"  \"\n",
    "            f\"|  BDH layer {CMP_LAYER_BDH}  vs  Transformer layer {CMP_LAYER_TFM}\",\n",
    "            tfm_thresh=TFM_THRESH_INPUT)\n",
    "        st.pyplot(fig_sp, use_container_width=True); plt.close(fig_sp)\n",
    "\n",
    "        sc1, sc2, sc3 = st.columns(3)\n",
    "        sc1.metric(\"BDH active per token (ReLU > 0)\",\n",
    "                   f\"{pct_b:.1f}%\")\n",
    "        sc2.metric(f\"Transformer active per token (|GELU| > {TFM_THRESH_INPUT:.3f})\",\n",
    "                   f\"{pct_t:.1f}%\")\n",
    "        winner_sp = \"BDH sparser\" if pct_b < pct_t else \"TFM sparser\"\n",
    "        sc3.metric(\"Density ratio (TFM / BDH)\",\n",
    "                   f\"{pct_t / (pct_b + 0.1):.1f}x\",\n",
    "                   delta=winner_sp)\n",
    "        summary[\"bdh_sparsity\"] = pct_b\n",
    "        summary[\"tfm_sparsity\"] = pct_t\n",
    "\n",
    "        # Warn if sparsity direction seems wrong\n",
    "        if pct_b >= pct_t:\n",
    "            st.warning(\n",
    "                f\"BDH appears denser than Transformer ({pct_b:.1f}% vs {pct_t:.1f}%). \"\n",
    "                f\"Try: (1) increasing the Transformer threshold slider above \"\n",
    "                f\"{TFM_THRESH_INPUT:.3f}, (2) selecting a different BDH layer, \"\n",
    "                f\"or (3) verifying the checkpoint is fully trained.\"\n",
    "            )\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # SECTION B — ACTIVATION ENTROPY\n",
    "        # ----------------------------------------------------------\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"### B  —  Activation Entropy (Primary Monosemanticity Metric)\")\n",
    "        st.markdown(\"\"\"<div class=\"compare-box\">\n",
    "        <strong>H = -sum(p log2 p)</strong> over the normalised activation magnitude\n",
    "        distribution for each concept (mean across tokens, then entropy of vector).\n",
    "        Monosemantic neurons concentrate energy in very few positions: low entropy.\n",
    "        Polysemantic neurons spread energy everywhere: high entropy.\n",
    "        </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        with st.spinner(\"Computing activation entropy for all concepts...\"):\n",
    "            bdh_ents = []\n",
    "            tfm_ents = []\n",
    "            for c in cmp_concepts:\n",
    "                bv = bdh_token_acts(\n",
    "                    c, int(CMP_LAYER_BDH), device, model, ids_for_text).mean(0)\n",
    "                tv = tfm_token_acts(\n",
    "                    tfm_mdl, tfm_tok, c,\n",
    "                    min(int(CMP_LAYER_TFM), 5), device).mean(0)\n",
    "                bdh_ents.append(activation_entropy(bv))\n",
    "                tfm_ents.append(activation_entropy(tv))\n",
    "\n",
    "        fig_ent = plot_entropy_comparison(\n",
    "            cmp_concepts, bdh_ents, tfm_ents,\n",
    "            f\"Activation Entropy  (lower = monosemantic)  \"\n",
    "            f\"|  BDH layer {CMP_LAYER_BDH}  vs  Transformer layer {CMP_LAYER_TFM}\")\n",
    "        st.pyplot(fig_ent, use_container_width=True); plt.close(fig_ent)\n",
    "\n",
    "        e1, e2, e3 = st.columns(3)\n",
    "        e1.metric(\"BDH mean entropy\",         f\"{np.mean(bdh_ents):.2f} bits\")\n",
    "        e2.metric(\"Transformer mean entropy\",  f\"{np.mean(tfm_ents):.2f} bits\")\n",
    "        e3.metric(\"Entropy advantage (BDH lower by)\",\n",
    "                  f\"{np.mean(tfm_ents) - np.mean(bdh_ents):+.2f} bits\")\n",
    "\n",
    "        st.dataframe(pd.DataFrame({\n",
    "            \"Concept\":            cmp_concepts,\n",
    "            \"BDH entropy (bits)\": [f\"{e:.2f}\" for e in bdh_ents],\n",
    "            \"TFM entropy (bits)\": [f\"{e:.2f}\" for e in tfm_ents],\n",
    "            \"Winner\":             [\"BDH\" if b < t else \"TFM\"\n",
    "                                   for b, t in zip(bdh_ents, tfm_ents)],\n",
    "        }), use_container_width=True)\n",
    "        summary[\"bdh_entropy\"] = np.mean(bdh_ents)\n",
    "        summary[\"tfm_entropy\"] = np.mean(tfm_ents)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # SECTION C — CROSS-LINGUAL JACCARD\n",
    "        # ----------------------------------------------------------\n",
    "        if LANGS:\n",
    "            st.markdown(\"---\")\n",
    "            st.markdown(\"### C  —  Cross-lingual Consistency\")\n",
    "            st.markdown(f\"\"\"<div class=\"compare-box\">\n",
    "            Each concept is translated into all selected languages.\n",
    "            Jaccard overlap between EN top-K and translation top-K is computed\n",
    "            and averaged across languages.<br>\n",
    "            BDH TopK = {TOPK_SET_BDH} &nbsp;|&nbsp;\n",
    "            Transformer TopK = {TOPK_SET_TFM} (out of 3072 MLP neurons).<br>\n",
    "            <span class=\"bdh-label\">BDH</span>: encodes meaning, not surface form\n",
    "            &rarr; same neurons across languages.<br>\n",
    "            <span class=\"tfm-label\">Transformer</span>: encodes token statistics\n",
    "            &rarr; different neurons per language.\n",
    "            </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "            with st.spinner(\"Computing cross-lingual Jaccard scores...\"):\n",
    "                bdh_j_rows = []\n",
    "                tfm_j_rows = []\n",
    "                trans_table = []   # store for display\n",
    "                for c in cmp_concepts:\n",
    "                    lang_bdh_j = []\n",
    "                    lang_tfm_j = []\n",
    "                    row = {\"EN (concept)\": c}\n",
    "                    en_set_bdh = topk_set(\n",
    "                        c, int(CMP_LAYER_BDH), int(TOPK_SET_BDH),\n",
    "                        device, cfg, model, N, ids_for_text, neuron_id)\n",
    "                    en_vec_tfm = tfm_token_acts(\n",
    "                        tfm_mdl, tfm_tok, c,\n",
    "                        min(int(CMP_LAYER_TFM), 5), device).mean(0)\n",
    "                    en_topk_tfm = set(\n",
    "                        np.argsort(-np.abs(en_vec_tfm))[:int(TOPK_SET_TFM)])\n",
    "                    for lang in LANGS:\n",
    "                        tr = translate(lang, c, device)\n",
    "                        tag = LANG_TAGS[lang]\n",
    "                        row[f\"{tag} translation\"] = tr\n",
    "                        tr_set = topk_set(\n",
    "                            tr, int(CMP_LAYER_BDH), int(TOPK_SET_BDH),\n",
    "                            device, cfg, model, N, ids_for_text, neuron_id)\n",
    "                        j_bdh = jaccard(en_set_bdh, tr_set)\n",
    "                        lang_bdh_j.append(j_bdh)\n",
    "                        row[f\"{tag} BDH Jacc\"] = f\"{j_bdh:.3f}\"\n",
    "                        tr_vec = tfm_token_acts(\n",
    "                            tfm_mdl, tfm_tok, tr,\n",
    "                            min(int(CMP_LAYER_TFM), 5), device).mean(0)\n",
    "                        tr_topk = set(\n",
    "                            np.argsort(-np.abs(tr_vec))[:int(TOPK_SET_TFM)])\n",
    "                        j_tfm = jaccard(en_topk_tfm, tr_topk)\n",
    "                        lang_tfm_j.append(j_tfm)\n",
    "                        row[f\"{tag} TFM Jacc\"] = f\"{j_tfm:.3f}\"\n",
    "                    bdh_j_rows.append(np.mean(lang_bdh_j))\n",
    "                    tfm_j_rows.append(np.mean(lang_tfm_j))\n",
    "                    row[\"BDH mean Jacc\"] = f\"{np.mean(lang_bdh_j):.3f}\"\n",
    "                    row[\"TFM mean Jacc\"] = f\"{np.mean(lang_tfm_j):.3f}\"\n",
    "                    row[\"Winner\"] = \"BDH\" if np.mean(lang_bdh_j) > np.mean(lang_tfm_j) else \"TFM\"\n",
    "                    trans_table.append(row)\n",
    "\n",
    "            # Show translation table FIRST so user can see what was translated\n",
    "            st.markdown(\"#### Translations used + per-language Jaccard scores\")\n",
    "            st.dataframe(pd.DataFrame(trans_table), use_container_width=True,\n",
    "                         height=min(500, 60 + 50 * len(trans_table)))\n",
    "\n",
    "            fig_xl = plot_crosslingual_consistency(\n",
    "                cmp_concepts, np.array(bdh_j_rows), np.array(tfm_j_rows),\n",
    "                f\"Cross-lingual Consistency  \"\n",
    "                f\"|  BDH TopK={TOPK_SET_BDH}  vs  TFM TopK={TOPK_SET_TFM}\"\n",
    "                f\"  |  layers {CMP_LAYER_BDH} / {CMP_LAYER_TFM}\")\n",
    "            st.pyplot(fig_xl, use_container_width=True); plt.close(fig_xl)\n",
    "\n",
    "            cl1, cl2, cl3 = st.columns(3)\n",
    "            cl1.metric(\"BDH mean Jaccard\",        f\"{np.mean(bdh_j_rows):.3f}\")\n",
    "            cl2.metric(\"Transformer mean Jaccard\", f\"{np.mean(tfm_j_rows):.3f}\")\n",
    "            cl3.metric(\"BDH advantage\",\n",
    "                       f\"{np.mean(bdh_j_rows) - np.mean(tfm_j_rows):+.3f}\")\n",
    "            summary[\"bdh_jaccard\"] = np.mean(bdh_j_rows)\n",
    "            summary[\"tfm_jaccard\"] = np.mean(tfm_j_rows)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # SECTION D — HEATMAP\n",
    "        # ----------------------------------------------------------\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"### D  —  Activation Heatmap (Middle Token)\")\n",
    "        st.markdown(\"\"\"<div class=\"compare-box\">\n",
    "        Middle token of each concept phrase — not averaged — preserves ReLU exact zeros.<br>\n",
    "        <span class=\"bdh-label\">BDH</span>: Left half = top active neurons (bright spots).\n",
    "        Right half = zero/inactive neurons (dark). The yellow dashed line is the boundary.\n",
    "        True sparsity is only visible when inactive neurons are included alongside active ones.<br>\n",
    "        <span class=\"tfm-label\">Transformer</span>: All top neurons shown — uniformly lit\n",
    "        because GELU never produces exact zeros.\n",
    "        </div>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        with st.spinner(\"Building activation matrices (middle token)...\"):\n",
    "            bdh_vecs_mid = []\n",
    "            tfm_vecs_mid = []\n",
    "            for c in cmp_concepts:\n",
    "                bm = bdh_token_acts(\n",
    "                    c, int(CMP_LAYER_BDH), device, model, ids_for_text)\n",
    "                tm = tfm_token_acts(\n",
    "                    tfm_mdl, tfm_tok, c, min(int(CMP_LAYER_TFM), 5), device)\n",
    "                bdh_vecs_mid.append(bm[bm.shape[0] // 2])\n",
    "                tfm_vecs_mid.append(tm[tm.shape[0] // 2])\n",
    "\n",
    "        fig_hm = plot_activation_heatmap_compare(\n",
    "            cmp_concepts,\n",
    "            np.stack(bdh_vecs_mid),\n",
    "            np.stack(tfm_vecs_mid),\n",
    "            n_show=80)\n",
    "        st.pyplot(fig_hm, use_container_width=True); plt.close(fig_hm)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # SECTION E — SUMMARY SCORECARD\n",
    "        # ----------------------------------------------------------\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"### E  —  Summary Scorecard\")\n",
    "\n",
    "        scorecard_html = render_summary_table(\n",
    "            summary,\n",
    "            TOPK_SET_BDH=int(TOPK_SET_BDH),\n",
    "            TOPK_SET_TFM=int(TOPK_SET_TFM),\n",
    "            LANGS=LANGS,\n",
    "            tfm_thresh=TFM_THRESH_INPUT,\n",
    "        )\n",
    "        st.markdown(scorecard_html, unsafe_allow_html=True)\n",
    "        st.success(\"Comparison complete. All metrics measured from live model activations.\")\n",
    "'''\n",
    "\n",
    "import os\n",
    "with open(\"/content/app.py\", \"w\") as fh:\n",
    "    fh.write(APP_CODE)\n",
    "\n",
    "size   = os.path.getsize(\"/content/app.py\")\n",
    "source = open(\"/content/app.py\").read()\n",
    "\n",
    "checks = {\n",
    "    \"four tabs present\":                   \"tab_compare\" in source,\n",
    "    \"separate TOPK_SET_BDH\":               \"TOPK_SET_BDH\" in source,\n",
    "    \"separate TOPK_SET_TFM\":               \"TOPK_SET_TFM\" in source,\n",
    "    \"tfm sparsity uses abs()\":             \"np.abs(token_acts_matrix) > thresh\" in source,\n",
    "    \"bdh sparsity exact zero\":             \"bdh_sparsity_pct\" in source,\n",
    "    \"tfm threshold slider in sidebar\":     \"TFM_THRESH_INPUT\" in source,\n",
    "    \"entropy function defined\":            \"activation_entropy\" in source,\n",
    "    \"entropy in Tab 4\":                    \"bdh_ents\" in source,\n",
    "    \"z-score removed from Tab 4\":          \"bdh_best_z_fair\" not in source,\n",
    "    \"render_summary_table function\":       \"render_summary_table\" in source,\n",
    "    \"winner badges in scorecard\":          \"score-badge-win\" in source,\n",
    "    \"value bars in scorecard\":             \"score-bar-fill\" in source,\n",
    "    \"expected-results box\":                \"expected-box\" in source,\n",
    "    \"single-token heatmap\":                \"bm.shape[0] // 2\" in source,\n",
    "    \"sparsity direction warning\":          \"BDH appears denser\" in source,\n",
    "    \"no emojis in logic\":                  all(ch not in source\n",
    "                                               for ch in [\"\\U0001F7E2\", \"\\U0001F534\"]),\n",
    "}\n",
    "\n",
    "print(f\"app.py written  —  {size:,} bytes\")\n",
    "for label, passed in checks.items():\n",
    "    status = \"OK  \" if passed else \"FAIL\"\n",
    "    print(f\"  {status}  {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e89c2c-7d8e-4c6b-ab4b-0396628ba5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyngrok -q\n",
    "\n",
    "from google.colab import userdata\n",
    "from pyngrok import ngrok\n",
    "\n",
    "NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "print('ngrok ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af322a02-dd39-4bff-859a-a28bf4007012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Install Python dependencies\n",
    "# Pinned versions avoid API-breaking changes between runs.\n",
    "# --------------------------------------------------------------\n",
    "os.system(\n",
    "    \"pip install -q \"\n",
    "    \"pyngrok \"\n",
    "    \"streamlit \"\n",
    "    \"imageio \"\n",
    "    \"transformers \"\n",
    "    \"sentencepiece \"\n",
    "    \"accelerate \"\n",
    "    \"torch \"\n",
    "    \"numpy \"\n",
    "    \"pandas \"\n",
    "    \"matplotlib \"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Kill any previous Streamlit process that may still be running\n",
    "# from an earlier cell execution, to avoid port conflicts.\n",
    "# --------------------------------------------------------------\n",
    "os.system(\"pkill -f streamlit 2>/dev/null || true\")\n",
    "time.sleep(2)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Verify that app.py was written by Cell 1 before proceeding.\n",
    "# --------------------------------------------------------------\n",
    "APP_PATH = \"/content/app.py\"\n",
    "if not os.path.exists(APP_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        \"app.py not found at /content/app.py. \"\n",
    "        \"Please run Cell 1 first to write the application file.\"\n",
    "    )\n",
    "\n",
    "print(f\"app.py confirmed — {os.path.getsize(APP_PATH):,} bytes\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Start Streamlit as a background subprocess.\n",
    "# --server.headless disables the browser-open prompt.\n",
    "# --server.enableCORS false is required for ngrok tunnelling.\n",
    "# --server.enableXsrfProtection false avoids CSRF header issues\n",
    "#   in the Colab iframe environment.\n",
    "# --------------------------------------------------------------\n",
    "server_process = subprocess.Popen([\n",
    "    \"streamlit\", \"run\", APP_PATH,\n",
    "    \"--server.port\",                \"8501\",\n",
    "    \"--server.headless\",            \"true\",\n",
    "    \"--server.address\",             \"0.0.0.0\",\n",
    "    \"--server.enableCORS\",          \"false\",\n",
    "    \"--server.enableXsrfProtection\",\"false\",\n",
    "])\n",
    "\n",
    "# Allow Streamlit time to initialise before creating the tunnel\n",
    "print(\"Waiting for Streamlit to start...\")\n",
    "time.sleep(8)\n",
    "\n",
    "# Check that the subprocess is still alive\n",
    "if server_process.poll() is not None:\n",
    "    raise RuntimeError(\n",
    "        \"Streamlit process exited unexpectedly. \"\n",
    "        \"Check for import errors in app.py.\"\n",
    "    )\n",
    "\n",
    "print(\"Streamlit server running on port 8501\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Create an ngrok HTTPS tunnel so the app is reachable from\n",
    "# outside the Colab VM.\n",
    "# Replace the token below with your own from https://ngrok.com\n",
    "# if the current one has expired.\n",
    "# --------------------------------------------------------------\n",
    "from pyngrok import ngrok\n",
    "\n",
    "try:\n",
    "    ngrok.kill()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "tunnel = ngrok.connect(8501, bind_tls=True, inspect=False)\n",
    "public_url = tunnel.public_url\n",
    "\n",
    "print(f\"\\nApp is running at: {public_url}\")\n",
    "print(\"Open the link above in a new browser tab, or view the inline frame below.\\n\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Display the app inline inside the Colab output cell.\n",
    "# A wider frame (1200 px) avoids horizontal scrollbars on most\n",
    "# laptop screens.\n",
    "# --------------------------------------------------------------\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "display(IFrame(public_url, width=1200, height=820))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
