{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8a31b-4d41-4e43-a3d2-20ffd002b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# and `inputs` is symbolic_inputs.pt already loaded\n",
    "\n",
    "all_hidden_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in inputs:\n",
    "        idx = idx.unsqueeze(0)  # (1, T)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(idx, output_hidden_states=True)\n",
    "\n",
    "        # outputs.hidden_states: tuple (num_layers, 1, T, D)\n",
    "        hidden = torch.stack(outputs.hidden_states, dim=0)\n",
    "        hidden = hidden[:, 0]  # (num_layers, T, D)\n",
    "\n",
    "        all_hidden_states.append(hidden.cpu().numpy())\n",
    "\n",
    "# Shape: (num_inputs, num_layers, T, D)\n",
    "acts = np.stack(all_hidden_states)\n",
    "\n",
    "np.save(\"transformer_hidden_states.npy\", acts)\n",
    "print(\"Saved transformer_hidden_states.npy\")\n",
    "print(\"Shape:\", acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07877fc-abff-4a2b-bee4-92c4cac21776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create dummy symbolic inputs\n",
    "# Shape: (num_inputs, T)\n",
    "num_inputs = 200\n",
    "T = 16\n",
    "\n",
    "inputs = torch.randint(\n",
    "    low=0,\n",
    "    high=30522,   # valid BERT vocab range\n",
    "    size=(num_inputs, T),\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "torch.save(inputs, \"symbolic_inputs.pt\")\n",
    "\n",
    "print(\"symbolic_inputs.pt created\")\n",
    "print(\"Shape:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c62927-7ad9-4d8f-9537-2a82c76f2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"transformer_hidden_states.npy\" in os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3df97-c647-4e13-a36e-49ee747d83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load transformer hidden states\n",
    "acts = np.load(\"transformer_hidden_states.npy\")\n",
    "num_inputs, num_layers, T, D = acts.shape\n",
    "\n",
    "# Use last layer\n",
    "acts = acts[:, -1]  # (num_inputs, T, D)\n",
    "\n",
    "# Build neuron firing vectors (hidden dims = neurons)\n",
    "FIRE_THRESHOLD = 1e-3\n",
    "neuron_vectors = []\n",
    "\n",
    "for d in range(D):\n",
    "    values = acts[:, :, d].reshape(-1)\n",
    "    firing = (np.abs(values) > FIRE_THRESHOLD).astype(np.float32)\n",
    "    neuron_vectors.append(firing)\n",
    "\n",
    "neuron_vectors = np.stack(neuron_vectors)\n",
    "\n",
    "# Normalize\n",
    "neuron_vectors = StandardScaler().fit_transform(neuron_vectors)\n",
    "\n",
    "# Cluster neurons\n",
    "kmeans = KMeans(n_clusters=8, random_state=0, n_init=10)\n",
    "clusters = kmeans.fit_predict(neuron_vectors)\n",
    "\n",
    "# Save clusters\n",
    "np.save(\"transformer_neuron_clusters.npy\", clusters)\n",
    "print(\"Saved transformer_neuron_clusters.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decba878-f5ba-49b4-9315-3731ad603cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Segment 1: Transformer Cluster Inspection (FINAL)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "CLUSTER_ID = 6          # which neuron cluster to inspect\n",
    "LAYER_ID = -1           # last transformer layer\n",
    "TOP_K = 10              # top activating inputs\n",
    "FIRE_THRESHOLD = 1e-3   # dense firing threshold\n",
    "\n",
    "# ============================================================\n",
    "# SAFE TOKEN MAP (symbolic + fallback)\n",
    "# ============================================================\n",
    "REVERSE_TOKEN_MAP = {\n",
    "    0: \"x\",\n",
    "    1: \"y\",\n",
    "    2: \"n\",\n",
    "    3: \"+\",\n",
    "    4: \"-\",\n",
    "    5: \"*\",\n",
    "    6: \"/\",\n",
    "    7: \"=\",\n",
    "    8: \"1\",\n",
    "    9: \"2\",\n",
    "    10: \"3\",\n",
    "    11: \"4\",\n",
    "    12: \"5\",\n",
    "    13: \"6\",\n",
    "    14: \"7\",\n",
    "    15: \"8\",\n",
    "    16: \"9\",\n",
    "}\n",
    "\n",
    "def decode_equation(token_tensor):\n",
    "    \"\"\"\n",
    "    Safe decoder:\n",
    "    - Known symbolic tokens → readable\n",
    "    - Unknown BERT tokens → <token_id>\n",
    "    \"\"\"\n",
    "    tokens = token_tensor.tolist()\n",
    "    symbols = [\n",
    "        REVERSE_TOKEN_MAP[t] if t in REVERSE_TOKEN_MAP else f\"<{t}>\"\n",
    "        for t in tokens\n",
    "    ]\n",
    "    return \" \".join(symbols)\n",
    "\n",
    "# ============================================================\n",
    "# SAFETY CHECKS\n",
    "# ============================================================\n",
    "required_files = [\n",
    "    \"symbolic_inputs.pt\",\n",
    "    \"transformer_hidden_states.npy\",\n",
    "    \"transformer_neuron_clusters.npy\"\n",
    "]\n",
    "\n",
    "for f in required_files:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Required file '{f}' not found. \"\n",
    "            f\"Run the previous segments first.\"\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "inputs = torch.load(\"symbolic_inputs.pt\")  # (num_inputs, T)\n",
    "acts = np.load(\"transformer_hidden_states.npy\")  # (num_inputs, num_layers, T, D)\n",
    "clusters = np.load(\"transformer_neuron_clusters.npy\")  # (D,)\n",
    "\n",
    "num_inputs, num_layers, T, D = acts.shape\n",
    "print(\"Loaded inputs shape :\", inputs.shape)\n",
    "print(\"Loaded acts shape   :\", acts.shape)\n",
    "print(\"Loaded clusters     :\", clusters.shape)\n",
    "\n",
    "# Select layer\n",
    "acts = acts[:, LAYER_ID]  # (num_inputs, T, D)\n",
    "\n",
    "# ============================================================\n",
    "# IDENTIFY NEURONS IN CLUSTER\n",
    "# ============================================================\n",
    "cluster_dims = np.where(clusters == CLUSTER_ID)[0]\n",
    "\n",
    "print(f\"\\nCluster {CLUSTER_ID} contains {len(cluster_dims)} neurons\")\n",
    "\n",
    "if len(cluster_dims) == 0:\n",
    "    raise ValueError(\"Selected cluster has no neurons.\")\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE CLUSTER ACTIVATION SCORE PER INPUT\n",
    "# ============================================================\n",
    "cluster_scores = []\n",
    "\n",
    "for i in range(num_inputs):\n",
    "    h = acts[i]  # (T, D)\n",
    "\n",
    "    # Dense firing mask\n",
    "    firing = np.abs(h) > FIRE_THRESHOLD\n",
    "\n",
    "    # Restrict to cluster subspace\n",
    "    cluster_firing = firing[:, cluster_dims]  # (T, |cluster|)\n",
    "\n",
    "    # Score = how active this cluster is\n",
    "    score = cluster_firing.sum()\n",
    "    cluster_scores.append(score)\n",
    "\n",
    "cluster_scores = np.array(cluster_scores)\n",
    "\n",
    "# ============================================================\n",
    "# SHOW TOP ACTIVATING INPUTS\n",
    "# ============================================================\n",
    "top_indices = np.argsort(cluster_scores)[-TOP_K:][::-1]\n",
    "\n",
    "print(\"\\nTop activating inputs for Transformer cluster:\\n\")\n",
    "\n",
    "for idx in top_indices:\n",
    "    print(f\"Input {idx}, score = {cluster_scores[idx]:.2f}\")\n",
    "    print(decode_equation(inputs[idx]))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d2b45-e25a-41fb-b104-e0a7b88fcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "FIRE_THRESHOLD = 1e-3   # small → dense firing (Transformer property)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD HIDDEN STATES\n",
    "# ============================================================\n",
    "acts = np.load(\"transformer_hidden_states.npy\")\n",
    "num_inputs, num_layers, T, D = acts.shape\n",
    "\n",
    "print(\"Loaded hidden states:\", acts.shape)\n",
    "\n",
    "# Use last layer\n",
    "acts = acts[:, -1]   # (num_inputs, T, D)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD NEURON VECTORS\n",
    "# ============================================================\n",
    "# Each hidden dimension = one neuron\n",
    "# Shape: (D, num_inputs * T)\n",
    "\n",
    "neuron_vectors = []\n",
    "\n",
    "for d in range(D):\n",
    "    values = acts[:, :, d].reshape(-1)\n",
    "    firing = (np.abs(values) > FIRE_THRESHOLD).astype(np.float32)\n",
    "    neuron_vectors.append(firing)\n",
    "\n",
    "neuron_vectors = np.stack(neuron_vectors)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE\n",
    "# ============================================================\n",
    "np.save(\"transformer_neuron_vectors.npy\", neuron_vectors)\n",
    "\n",
    "print(\"Saved transformer_neuron_vectors.npy\")\n",
    "print(\"Shape:\", neuron_vectors.shape)\n",
    "print(\"Firing ratio:\", neuron_vectors.mean())\n",
    "print(\"Sparsity:\", 1.0 - neuron_vectors.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860e0c0-5014-4731-a57f-c25e956cf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Segment 2: Build Transformer Activation Atlas (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "UMAP_NEIGHBORS = 25        # higher → smoother, more overlap\n",
    "UMAP_MIN_DIST = 0.4        # higher → dense clusters\n",
    "RANDOM_STATE = 42\n",
    "POINT_SIZE = 6\n",
    "\n",
    "# ============================================================\n",
    "# SAFETY CHECKS\n",
    "# ============================================================\n",
    "required_files = [\n",
    "    \"transformer_neuron_vectors.npy\",\n",
    "    \"transformer_neuron_clusters.npy\"\n",
    "]\n",
    "\n",
    "for f in required_files:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Required file '{f}' not found. \"\n",
    "            f\"Run the neuron-vector & clustering step first.\"\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "neuron_vectors = np.load(\"transformer_neuron_vectors.npy\")\n",
    "clusters = np.load(\"transformer_neuron_clusters.npy\")\n",
    "\n",
    "print(\"Neuron vectors shape:\", neuron_vectors.shape)\n",
    "print(\"Clusters shape      :\", clusters.shape)\n",
    "\n",
    "assert neuron_vectors.shape[0] == clusters.shape[0], \\\n",
    "    \"Mismatch: neurons != cluster labels\"\n",
    "\n",
    "# ============================================================\n",
    "# NORMALIZATION\n",
    "# ============================================================\n",
    "scaler = StandardScaler()\n",
    "neuron_vectors = scaler.fit_transform(neuron_vectors)\n",
    "\n",
    "# ============================================================\n",
    "# UMAP PROJECTION\n",
    "# ============================================================\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=UMAP_NEIGHBORS,\n",
    "    min_dist=UMAP_MIN_DIST,\n",
    "    n_components=2,\n",
    "    metric=\"cosine\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "coords = reducer.fit_transform(neuron_vectors)\n",
    "np.save(\"transformer_atlas_coords.npy\", coords)\n",
    "\n",
    "print(\"Saved transformer_atlas_coords.npy\")\n",
    "# ============================================================\n",
    "# Transformer Overlapping Cluster Atlas (Paper-Style)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "coords = np.load(\"transformer_atlas_coords.npy\")      # (D, 2)\n",
    "clusters = np.load(\"transformer_neuron_clusters.npy\") # (D,)\n",
    "\n",
    "unique_clusters = np.unique(clusters)\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "print(\"Clusters:\", unique_clusters)\n",
    "\n",
    "# ============================================================\n",
    "# COLORS (paper-like)\n",
    "# ============================================================\n",
    "colors = [\n",
    "    \"#1f77b4\",  # blue\n",
    "    \"#ff7f0e\",  # orange\n",
    "    \"#2ca02c\",  # green\n",
    "    \"#d62728\",  # red\n",
    "    \"#9467bd\",  # purple\n",
    "    \"#8c564b\",  # brown\n",
    "    \"#e377c2\",  # pink\n",
    "    \"#7f7f7f\",  # gray\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# FIGURE\n",
    "# ============================================================\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "# plot small clusters first → big ones on top\n",
    "for i, cid in enumerate(unique_clusters):\n",
    "    mask = clusters == cid\n",
    "    pts = coords[mask]\n",
    "\n",
    "    # ---- jitter to reveal overlap (KEY STEP) ----\n",
    "    jitter = np.random.normal(scale=0.015, size=pts.shape)\n",
    "    pts_j = pts + jitter\n",
    "\n",
    "    # scatter\n",
    "    plt.scatter(\n",
    "        pts_j[:, 0],\n",
    "        pts_j[:, 1],\n",
    "        s=22,\n",
    "        alpha=0.45,\n",
    "        color=colors[i % len(colors)],\n",
    "        label=f\"Cluster {cid}\"\n",
    "    )\n",
    "\n",
    "    # ---- convex hull (paper look) ----\n",
    "    if pts.shape[0] > 20:\n",
    "        hull = ConvexHull(pts)\n",
    "        hull_pts = pts[hull.vertices]\n",
    "        poly = Polygon(\n",
    "            hull_pts,\n",
    "            facecolor=colors[i % len(colors)],\n",
    "            alpha=0.08,\n",
    "            edgecolor=None\n",
    "        )\n",
    "        plt.gca().add_patch(poly)\n",
    "\n",
    "# ============================================================\n",
    "# LABELS\n",
    "# ============================================================\n",
    "plt.title(\n",
    "    \"Transformer Neuron Atlas\\nDense, Overlapping & Entangled Representations\",\n",
    "    fontsize=14\n",
    ")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "\n",
    "plt.legend(\n",
    "    fontsize=9,\n",
    "    frameon=False,\n",
    "    markerscale=1.3,\n",
    "    loc=\"best\"\n",
    ")\n",
    "\n",
    "plt.grid(alpha=0.15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1ccbf-88f2-43d2-9486-2ee42f6e0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Segment 3: Transformer Neuron Clustering (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "LAYER_ID = -1              # analyze last transformer layer\n",
    "FIRE_THRESHOLD = 1e-3      # small → dense firing\n",
    "N_CLUSTERS = 8\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# ============================================================\n",
    "# LOAD ACTIVATIONS\n",
    "# ============================================================\n",
    "if not os.path.exists(\"transformer_hidden_states.npy\"):\n",
    "    raise FileNotFoundError(\n",
    "        \"transformer_hidden_states.npy not found. \"\n",
    "        \"Run the hidden-state logging step first.\"\n",
    "    )\n",
    "\n",
    "acts = np.load(\"transformer_hidden_states.npy\")\n",
    "\n",
    "num_inputs, num_layers, T, D = acts.shape\n",
    "print(\"Loaded transformer activations:\", acts.shape)\n",
    "\n",
    "# Select layer\n",
    "acts = acts[:, LAYER_ID]    # (num_inputs, T, D)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD NEURON FIRING VECTORS\n",
    "# ============================================================\n",
    "# Each hidden dimension = one neuron\n",
    "# Shape: (D, num_inputs * T)\n",
    "\n",
    "neuron_vectors = []\n",
    "\n",
    "for d in range(D):\n",
    "    values = acts[:, :, d].reshape(-1)\n",
    "    firing = (np.abs(values) > FIRE_THRESHOLD).astype(np.float32)\n",
    "    neuron_vectors.append(firing)\n",
    "\n",
    "neuron_vectors = np.stack(neuron_vectors)\n",
    "\n",
    "# ============================================================\n",
    "# SPARSITY CHECK\n",
    "# ============================================================\n",
    "firing_ratio = neuron_vectors.mean()\n",
    "sparsity = 1.0 - firing_ratio\n",
    "\n",
    "print(f\"Transformer firing ratio  : {firing_ratio:.3f}\")\n",
    "print(f\"Transformer sparsity      : {sparsity:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# NORMALIZE FOR CLUSTERING\n",
    "# ============================================================\n",
    "scaler = StandardScaler()\n",
    "neuron_vectors_norm = scaler.fit_transform(neuron_vectors)\n",
    "\n",
    "# ============================================================\n",
    "# CLUSTER NEURONS\n",
    "# ============================================================\n",
    "kmeans = KMeans(\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "clusters = kmeans.fit_predict(neuron_vectors_norm)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE OUTPUTS (REQUIRED BY SEGMENT 1 & 2)\n",
    "# ============================================================\n",
    "np.save(\"transformer_neuron_vectors.npy\", neuron_vectors)\n",
    "np.save(\"transformer_neuron_clusters.npy\", clusters)\n",
    "\n",
    "print(\"Saved transformer_neuron_vectors.npy\")\n",
    "print(\"Saved transformer_neuron_clusters.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521aeaa-3bfd-4d28-8923-d50b04352de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Segment 4: Transformer Neuron Sparsity Analysis (FIXED)\n",
    "# Path B – Interpretability\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "LAYER_ID = -1              # analyze last transformer layer\n",
    "FIRE_THRESHOLD = 1e-3      # small → dense firing (Transformer property)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRANSFORMER ACTIVATIONS\n",
    "# Expected shape: (num_inputs, num_layers, T, D)\n",
    "# ============================================================\n",
    "if not os.path.exists(\"transformer_hidden_states.npy\"):\n",
    "    raise FileNotFoundError(\n",
    "        \"transformer_hidden_states.npy not found. \"\n",
    "        \"Run the transformer activation logging step first.\"\n",
    "    )\n",
    "\n",
    "acts = np.load(\"transformer_hidden_states.npy\")\n",
    "\n",
    "num_inputs, num_layers, T, D = acts.shape\n",
    "print(\"Loaded transformer activations:\", acts.shape)\n",
    "\n",
    "# Select layer to analyze\n",
    "acts = acts[:, LAYER_ID]    # (num_inputs, T, D)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD NEURON FIRING VECTORS\n",
    "# Each hidden dimension = one neuron\n",
    "# Output shape: (D, num_inputs * T)\n",
    "# ============================================================\n",
    "neuron_vectors = np.zeros((D, num_inputs * T), dtype=np.float32)\n",
    "\n",
    "for d in range(D):\n",
    "    values = acts[:, :, d].reshape(-1)\n",
    "    neuron_vectors[d] = (np.abs(values) > FIRE_THRESHOLD).astype(np.float32)\n",
    "\n",
    "# ============================================================\n",
    "# SPARSITY / DENSITY CHECK\n",
    "# ============================================================\n",
    "firing_ratio = neuron_vectors.mean()\n",
    "sparsity = 1.0 - firing_ratio\n",
    "\n",
    "print(\"\\n=== Transformer Neuron Sparsity ===\")\n",
    "print(f\"Firing ratio  : {firing_ratio:.3f}\")\n",
    "print(f\"Sparsity      : {sparsity:.3f}\")\n",
    "\n",
    "if firing_ratio > 0.6:\n",
    "    print(\"✅ Dense firing confirmed (>60%) — entangled representation\")\n",
    "else:\n",
    "    print(\"⚠️ Firing ratio below expectation; try lowering FIRE_THRESHOLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c853e-0b29-464b-8a12-870f528eb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Segment 5: Log Transformer Hidden States (FINAL FIXED)\n",
    "# Path B – Interpretability\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD PRETRAINED TRANSFORMER (NO CUSTOM MODULE)\n",
    "# ============================================================\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    output_hidden_states=True\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ============================================================\n",
    "# LOAD INPUT DATA (SAFE)\n",
    "# ============================================================\n",
    "if not os.path.exists(\"symbolic_inputs.pt\"):\n",
    "    raise FileNotFoundError(\n",
    "        \"symbolic_inputs.pt not found. \"\n",
    "        \"Run the data generation step first.\"\n",
    "    )\n",
    "\n",
    "inputs = torch.load(\"symbolic_inputs.pt\")  # shape: (num_inputs, T)\n",
    "print(\"Loaded inputs:\", inputs.shape)\n",
    "\n",
    "# ============================================================\n",
    "# LOG TRANSFORMER HIDDEN STATES\n",
    "# ============================================================\n",
    "def log_transformer_hidden_states(model, inputs):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        activations: np.ndarray\n",
    "                     shape = (num_inputs, num_layers, T, D)\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in inputs:\n",
    "            idx = idx.unsqueeze(0).to(device)  # (1, T)\n",
    "\n",
    "            outputs = model(idx)\n",
    "\n",
    "            # outputs.hidden_states:\n",
    "            # tuple of length (num_layers + 1)\n",
    "            # each tensor shape: (1, T, D)\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            # Skip embedding layer → take only transformer layers\n",
    "            layer_states = torch.stack(\n",
    "                [h[0] for h in hidden_states[1:]], dim=0\n",
    "            )  # (num_layers, T, D)\n",
    "\n",
    "            activations.append(layer_states.cpu().numpy())\n",
    "\n",
    "    return np.stack(activations)\n",
    "\n",
    "# ============================================================\n",
    "# RUN LOGGING\n",
    "# ============================================================\n",
    "acts = log_transformer_hidden_states(model, inputs)\n",
    "\n",
    "np.save(\"transformer_hidden_states.npy\", acts)\n",
    "\n",
    "print(\"\\nSaved transformer_hidden_states.npy\")\n",
    "print(\"Hidden states shape:\", acts.shape)\n",
    "\n",
    "# ============================================================\n",
    "# DENSE FIRING CHECK (IMPORTANT FOR YOUR CLAIM)\n",
    "# ============================================================\n",
    "firing_ratio = (np.abs(acts) > 1e-3).mean()\n",
    "\n",
    "print(\"\\n=== Dense Firing Check ===\")\n",
    "print(\"Firing ratio:\", firing_ratio)\n",
    "\n",
    "if firing_ratio > 0.6:\n",
    "    print(\"✅ Dense firing confirmed (>60%) — Transformer neurons are entangled\")\n",
    "else:\n",
    "    print(\"⚠️ Firing ratio lower than expected — consider lowering threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1f5d1-0b77-448b-bba8-1a014aa21627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
