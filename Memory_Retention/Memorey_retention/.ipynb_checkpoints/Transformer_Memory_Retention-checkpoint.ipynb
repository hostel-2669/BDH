{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948848f1-1da3-44ee-9279-d1b5059fa357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dataclasses\n",
    "import math\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETUP THE TRANSFORMER (Increased Limit)\n",
    "# =============================================================================\n",
    "CONTEXT_LIMIT = 512  # <--- INCREASED LIMIT (Holds a long paragraph)\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = CONTEXT_LIMIT\n",
    "    vocab_size: int = 128\n",
    "    n_layer: int = 2\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        if t > self.config.block_size:\n",
    "            idx = idx[:, -self.config.block_size:]\n",
    "            t = self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        x = self.transformer.wte(idx) + self.transformer.wpe(pos)\n",
    "        for block in self.transformer.h: x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=20):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# =============================================================================\n",
    "# 2. INTERACTIVE DEMO (LONG PARAGRAPH VERSION)\n",
    "# =============================================================================\n",
    "def run_long_context_demo():\n",
    "    # Helper: Encode/Decode\n",
    "    chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!?,.:; \"\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode = lambda s: [stoi.get(c, 0) for c in s]\n",
    "    decode = lambda l: ''.join([itos.get(i, '') for i in l])\n",
    "\n",
    "    # Initialize Model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = GPT(GPTConfig())\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      TRANSFORMER MEMORY DEMO (Long Context)\")\n",
    "    print(f\"      Memory Limit: {CONTEXT_LIMIT} characters\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- INPUT 1: THE SECRET ---\n",
    "    secret = input(\"\\n[1] Enter the SECRET (e.g., 'Kriti is the winner'): \")\n",
    "    if not secret: secret = \"Kriti is the winner.\"\n",
    "\n",
    "    # --- INPUT 2: THE DISTRACTION (LONG) ---\n",
    "    print(f\"\\n[2] Enter a VERY LONG PARAGRAPH to push the secret out.\")\n",
    "    print(f\"    (Must be > {CONTEXT_LIMIT - len(secret)} chars to trigger forgetting)\")\n",
    "    print(\"    Tip: Copy/Paste a long text block here.\")\n",
    "    distraction = input(\"    Paste here: \")\n",
    "\n",
    "    # Combine History\n",
    "    history = secret + \" \" + distraction\n",
    "    print(f\"\\n--> Total Text Length: {len(history)} chars\")\n",
    "    print(f\"--> Memory Limit:      {CONTEXT_LIMIT} chars\")\n",
    "\n",
    "    # --- INPUT 3: THE QUESTION ---\n",
    "    prompt = input(\"\\n[3] Enter your QUESTION (e.g., 'Who is the winner?'): \")\n",
    "    full_input_str = history + \" \" + prompt\n",
    "\n",
    "    # Convert to Tensor\n",
    "    input_ids = torch.tensor(encode(full_input_str), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    # --- SHOW WHAT HAPPENS INTERNALLY ---\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"INTERNAL MEMORY SCAN:\")\n",
    "\n",
    "    # Visualize the buffer state\n",
    "    if input_ids.size(1) > CONTEXT_LIMIT:\n",
    "        overflow_amount = input_ids.size(1) - CONTEXT_LIMIT\n",
    "        print(f\"[!] MEMORY OVERFLOW by {overflow_amount} characters.\")\n",
    "        print(\"[!] The beginning (Secret) has been DELETED.\")\n",
    "        actual_input = input_ids[:, -CONTEXT_LIMIT:]\n",
    "        secret_status = \"GONE\"\n",
    "    else:\n",
    "        print(\"[OK] Memory is NOT full yet.\")\n",
    "        print(\"     The model can still see the secret (No Forgetting).\")\n",
    "        actual_input = input_ids\n",
    "        secret_status = \"VISIBLE\"\n",
    "\n",
    "    # Decode what the model actually sees\n",
    "    visible_text = decode(actual_input[0].tolist())\n",
    "\n",
    "    # Show only the start and end of memory to keep output clean\n",
    "    preview = visible_text if len(visible_text) < 100 else f\"...{visible_text[-100:]}\"\n",
    "    print(f\"\\n[WHAT THE MODEL SEES NOW (Last 100 chars)]:\\n'{preview}'\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Prediction\n",
    "    if secret_status == \"GONE\":\n",
    "        print(\"\\n[PREDICTION]: The model will FAIL (Secret deleted).\")\n",
    "    else:\n",
    "        print(\"\\n[PREDICTION]: The model MIGHT work (Secret still in memory).\")\n",
    "\n",
    "    # --- GENERATE ANSWER ---\n",
    "    print(\"\\n--> Model is answering...\")\n",
    "    output_ids = model.generate(actual_input, max_new_tokens=15)\n",
    "    new_tokens = output_ids[0].tolist()[len(actual_input[0]):]\n",
    "    reply = decode(new_tokens)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"MODEL REPLY: {reply}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Start the demo\n",
    "run_long_context_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
